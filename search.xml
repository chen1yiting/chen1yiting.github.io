<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>PyTorch-EnvSetup&amp;BasicConcept</title>
    <url>/2021/09/26/PyTorch-EnvSetup-BasicConcept/</url>
    <content><![CDATA[<h2 id="一-PyTorch基础概念"><a href="#一-PyTorch基础概念" class="headerlink" title="一. PyTorch基础概念"></a>一. PyTorch基础概念</h2><h3 id="A-环境搭建"><a href="#A-环境搭建" class="headerlink" title="A. 环境搭建"></a>A. 环境搭建</h3><h4 id="1-系统显卡驱动，CUDA和cudnn配置"><a href="#1-系统显卡驱动，CUDA和cudnn配置" class="headerlink" title="1. 系统显卡驱动，CUDA和cudnn配置"></a>1. 系统显卡驱动，CUDA和cudnn配置</h4><p>本机配置：Win10，Geforce 3070 140w 8GB显存，Intel 11th i7 八核十六线程</p>
<p>当前Nvidia显卡驱动版本462.62，最高支持cuda版本11.2（安装cuda版本11.1）</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921151203907.png" alt="image-20210921151203907"></p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921151508628.png" alt="image-20210921151508628"></p>
<p>本机目前安装CUDA版本11.1，可通过nvcc -V查看</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921152447031.png" alt="image-20210921152447031"></p>
<p>对应cudnn版本为8.1.1</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921155417638.png" alt="image-20210921155417638"></p>
<p>通过运行CUDA自带的deviceQuery.exe可知，</p>
<p>当前cuda driver version为11.2，当前cuda runtime version为11.1。两者允许不一致，但是runtime version必须 &lt;= driver version</p>
<h4 id="2-PyTorch安装"><a href="#2-PyTorch安装" class="headerlink" title="2. PyTorch安装"></a>2. PyTorch安装</h4><p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921155822943.png" alt="image-20210921155822943"></p>
<p>CUDA版本应该与runtime version相对应。</p>
<p>可以下载对应的torch，torchvision和torchaudio手动安装：</p>
<p><a href="https://download.pytorch.org/whl/torch_stable.html">https://download.pytorch.org/whl/torch_stable.html</a></p>
<p>直接到conda 环境下运行辣个command/或者pip install 各个.whl文件。</p>
<h3 id="B-张量简介与创建"><a href="#B-张量简介与创建" class="headerlink" title="B. 张量简介与创建"></a>B. 张量简介与创建</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>张量<strong>tensor</strong>就是<strong>多维数组</strong>，是标量、向量、矩阵的高维拓展。具备以下属性。</p>
<ul>
<li>卷积之类的处理之后，数据类型默认为32-bit floating point, torch.float/float32 ；</li>
<li>图像的标签则默认为64-bit integer (signed) torch.int/torch.int64</li>
</ul>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921163051863.png" alt="image-20210921163051863"></p>
<h4 id="1-直接创建"><a href="#1-直接创建" class="headerlink" title="1. 直接创建"></a>1. 直接创建</h4><h5 id="torch-tensor"><a href="#torch-tensor" class="headerlink" title="torch.tensor()"></a>torch.tensor()</h5><p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921163600998.png" alt="image-20210921163600998"></p>
<ul>
<li>data：数据，可以是list，numpy</li>
<li>dtype：数据类型，默认与data的一致</li>
<li>device：所在设备，cuda/cpu</li>
<li>requires_grad：是否需要梯度</li>
<li>pin_memory：是否存于锁页内存（目前不懂作用，默认为false即可）</li>
</ul>
<h5 id="torch-from-numpy"><a href="#torch-from-numpy" class="headerlink" title="torch.from_numpy()"></a>torch.from_numpy()</h5><p>从ndarray创建</p>
<p><em><strong><u><code>torch.from_numpy(ndarray)</code></u></strong></em></p>
<p>从numpy创建tensor，<strong>注意：两者之间共享内存，改动其中一个另一个也会被更改。</strong></p>
<h4 id="2-依据数值创建"><a href="#2-依据数值创建" class="headerlink" title="2. 依据数值创建"></a>2. 依据数值创建</h4><h5 id="torch-xxx-like-xxx"><a href="#torch-xxx-like-xxx" class="headerlink" title="torch.xxx_like()/xxx()"></a>torch.xxx_like()/xxx()</h5><p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921164613808.png" alt="image-20210921164613808"></p>
<ul>
<li>out: 输出的张量（给生成的数据打上新标签）</li>
<li>layout: 内存中的布局形式，有strided, sparse_coo等（目前不懂作用，不过默认strided就好）</li>
</ul>
<p>根据input形状创建全零或全一张量</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921164936568.png" alt="image-20210921164936568"></p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921165012519.png" alt="image-20210921165012519"></p>
<p><em><strong><u>torch.full( )</u><em><strong>和</strong></em><u>torch.full_like( )</u></strong></em></p>
<p>依据input形状创建全（fill_value）值的向量</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921165151658.png" alt="image-20210921165151658"></p>
<ul>
<li>size：张量的形状，如（3，3）</li>
<li>fill_value：张量的值</li>
</ul>
<h5 id="torch-arange"><a href="#torch-arange" class="headerlink" title="torch.arange( )"></a>torch.arange( )</h5><p>创建等差的1维张量，数值区间为[start, end)</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921165611232.png" alt="image-20210921165611232"></p>
<ul>
<li>start: 初始数值</li>
<li>end: 结束数值</li>
<li>step：数列公差</li>
</ul>
<h5 id="torch-linspace"><a href="#torch-linspace" class="headerlink" title="torch.linspace( )"></a>torch.linspace( )</h5><p>创建均分的1维张量，数值区间[start, end]</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921165758777.png" alt="image-20210921165758777"></p>
<ul>
<li>steps: 数列长度</li>
</ul>
<h5 id="torch-logspace"><a href="#torch-logspace" class="headerlink" title="torch.logspace( )"></a>torch.logspace( )</h5><p>创建对数均分的一维张量，长度为<em>steps</em>，底为<em>base</em>（默认为10）</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921170039498.png" alt="image-20210921170039498"></p>
<h5 id="torch-eyes"><a href="#torch-eyes" class="headerlink" title="torch.eyes( )"></a>torch.eyes( )</h5><p>创建单位对角矩阵（2维），默认为方阵，n行数（方阵只需设置行数），m列数</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921170217035.png" alt="image-20210921170217035"></p>
<h4 id="3-依概率分布创建"><a href="#3-依概率分布创建" class="headerlink" title="3.依概率分布创建"></a>3.依概率分布创建</h4><h5 id="torch-normal"><a href="#torch-normal" class="headerlink" title="torch.normal( )"></a>torch.normal( )</h5><p>生成正态分布（Gaussian）</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921170751238.png" alt="image-20210921170751238"></p>
<ul>
<li><p>mean：均值</p>
</li>
<li><p>std：标准差</p>
</li>
<li><p>四种模式：</p>
<ul>
<li><p>mean为<strong>标</strong>量，std为<strong>标</strong>量，例：torch.normal(0., 1., size=(4, ))；</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921171403037.png" alt="image-20210921171403037"></p>
</li>
<li><p>mean为<strong>标</strong>量，std为张量, 同下，</p>
</li>
<li><p>mean为张量，std为<strong>标</strong>量, 例如mean = torch.arrange(1, 5, dtype=torch.float), std = 1</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921171600585.png" alt="image-20210921171600585"></p>
</li>
<li><p>mean为张量，std为张量,例: mean = torch.arrange(1, 5, dypte=torch.float) , std = torch.arrange(1, 5, dypte=torch.float) <img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921171237469.png" alt="image-20210921171237469"></p>
</li>
</ul>
</li>
</ul>
<h5 id="torch-randn-randn-like"><a href="#torch-randn-randn-like" class="headerlink" title="torch.randn()/randn_like()"></a>torch.randn()/randn_like()</h5><p>生成<strong>标准正态分布</strong></p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921171733876.png" alt="image-20210921171733876"></p>
<ul>
<li>size：张量的形状</li>
</ul>
<h5 id="torch-rand-rand-like"><a href="#torch-rand-rand-like" class="headerlink" title="torch.rand()/rand_like()"></a>torch.rand()/rand_like()</h5><p>在区间**[0,1)**上，生成**均匀分布**<img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921171938510.png" alt="image-20210921171938510"></p>
<h5 id="torch-randint-randint-like"><a href="#torch-randint-randint-like" class="headerlink" title="torch.randint()/randint_like()"></a>torch.randint()/randint_like()</h5><p>在区间[low, high)生成<strong>整数均匀分布</strong></p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921172147889.png" alt="image-20210921172147889"></p>
<h5 id="torch-randperm"><a href="#torch-randperm" class="headerlink" title="torch.randperm()"></a>torch.randperm()</h5><p>生成从0到n-1的随机排列</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921172235710.png" alt="image-20210921172235710"></p>
<h5 id="torch-bernoulli"><a href="#torch-bernoulli" class="headerlink" title="torch.bernoulli()"></a>torch.bernoulli()</h5><p>以input为概率，生成伯努利分布（0-1分布，两点分布），input为概率值</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921172334929.png" alt="image-20210921172334929"></p>
<h3 id="C-张量的操作"><a href="#C-张量的操作" class="headerlink" title="C. 张量的操作"></a>C. 张量的操作</h3><h4 id="1-张量拼接与切分"><a href="#1-张量拼接与切分" class="headerlink" title="1.张量拼接与切分"></a>1.张量拼接与切分</h4><h5 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat( )"></a>torch.cat( )</h5><p>将张量按维度dim进行拼接，**<u>cat()操作并不会拓展张量的维度</u>**</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921193450329.png" alt="image-20210921193450329"></p>
<ul>
<li>tensor：张量序列</li>
<li>dim：要拼接的维度</li>
</ul>
<h5 id="torch-stack"><a href="#torch-stack" class="headerlink" title="torch.stack()"></a>torch.stack()</h5><p>在**<u>新创建</u>**的维度dim上进行拼接</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921193619709.png" alt="image-20210921193619709"></p>
<ul>
<li>tensor：张量序列</li>
<li>dim：要拼接的维度</li>
</ul>
<h5 id="torch-chunk"><a href="#torch-chunk" class="headerlink" title="torch.chunk()"></a>torch.chunk()</h5><p>将张量按维度dim进行平均切分，返回张量列表。若不能整除，最后一份张量最小。</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921194449301.png" alt="image-20210921194449301"></p>
<ul>
<li>input：要切分的张量</li>
<li>chunks：要切分的份数</li>
<li>dim：要切分的维度</li>
</ul>
<p>例：</p>
<p><code>a = torch.ones((2, 7))</code> </p>
<p><code>list_of_tensors = torch.chunk(a, dim=1, chunk = 3)</code></p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921195940743.png" alt="image-20210921195940743"></p>
<h5 id="torch-split"><a href="#torch-split" class="headerlink" title="torch.split()"></a>torch.split()</h5><p>将张量按维度dim进行切分，返回值是张量列表</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921200047221.png" alt="image-20210921200047221"></p>
<ul>
<li>tensor：要切分的张量</li>
<li>split_size_or_sections：为int时，表示每一份的长度；为list时，按list元素切分</li>
<li>dim：要切分的维度</li>
</ul>
<h4 id="2-张量索引"><a href="#2-张量索引" class="headerlink" title="2. 张量索引"></a>2. 张量索引</h4><h5 id="torch-index-select"><a href="#torch-index-select" class="headerlink" title="torch.index_select()"></a>torch.index_select()</h5><p>在维度dim上，按index索引数据，返回值为依index索引数据拼接而成的张量。注意，此函数中，index类型必须为torch.long。</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921200540590.png" alt="image-20210921200540590"></p>
<ul>
<li>input：要索引的张量</li>
<li>dim：要索引的维度</li>
<li>index：要索引数据的序号</li>
</ul>
<p>例：</p>
<p><code>t = torch.randint(0, 9, size=(3,3))</code></p>
<p><code>idx = torch.tensor([0, 2], dtype=torch.long)</code></p>
<p><code>t_select = torch.index_select(t, dim=0, index=idx)</code></p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921200914186.png" alt="image-20210921200914186"></p>
<h5 id="torch-masked-select"><a href="#torch-masked-select" class="headerlink" title="torch.masked_select()"></a>torch.masked_select()</h5><p>按mask中的True进行索引，返回值为一维张量。</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921201241414.png" alt="image-20210921201241414"></p>
<ul>
<li>input：要索引的张量</li>
<li>mask：与input同形状的布尔类型张量</li>
</ul>
<p><code>热知识：t = torch.randint(0, 9, size=(3,3))</code></p>
<p>​                <code>mask = t.ge(5) # ge means greater or equal</code>    </p>
<p>​        <code>#        mask = t.gt(5) # ge means greater</code>    </p>
<p>​        <code>#        mask = t.le(5) # ge means less or equal</code>    </p>
<p>​        <code>#        mask = t.lt(5) # ge means less</code></p>
<p>​        <code>t_select = torch.masked_select(t, mask)</code></p>
<p>Output:</p>
<p>​        <img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921201821510.png" alt="image-20210921201821510"></p>
<h4 id="3-张量的变换"><a href="#3-张量的变换" class="headerlink" title="3. 张量的变换"></a>3. 张量的变换</h4><h5 id="torch-reshape"><a href="#torch-reshape" class="headerlink" title="torch.reshape()"></a>torch.reshape()</h5><p>变换张量的形状，<strong>注意</strong>：当张量在内存中是连续时，新张量与input<strong>共享数据内存</strong>。</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921202011330.png" alt="image-20210921202011330"></p>
<p>新张量形状大小应该与input相匹配。若为某一维度长度由其他维度计算得知，<strong>可设置为-1</strong>。</p>
<h5 id="torch-transpose-torch-t"><a href="#torch-transpose-torch-t" class="headerlink" title="torch.transpose()/torch.t()"></a>torch.transpose()/torch.t()</h5><p>交换张量的两个维度。**(torch.t()适用于二维张量，等于torch.transpose(input, 0, 1))**</p>
<p><strong>图像处理时</strong>经常会用到，将channels * h * w变成h * w * channels。</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921202318278.png" alt="image-20210921202318278"></p>
<ul>
<li>input：要变换的张量</li>
<li>dim0：要交换的维度</li>
<li>dim1：要交换的维度</li>
</ul>
<h5 id="torch-squeeze-unsqueeze"><a href="#torch-squeeze-unsqueeze" class="headerlink" title="torch.squeeze()/unsqueeze()"></a>torch.squeeze()/unsqueeze()</h5><p><strong>squeeze()</strong>: 压缩长度为1的维度（轴）</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921202819633.png" alt="image-20210921202819633"></p>
<p>dim：若为None，移除所有长度为1的轴；若指定维度，当且仅当该轴长度为1时，可以被移除。</p>
<p>**unsqueeze()**：依据dim扩展维度</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921202948318.png" alt="image-20210921202948318"></p>
<p>dim: 扩展的维度</p>
<h3 id="D-张量数学运算"><a href="#D-张量数学运算" class="headerlink" title="D. 张量数学运算"></a>D. 张量数学运算</h3><p>分为三类：</p>
<ol>
<li>加减乘除</li>
<li>对数，指数，幂函数</li>
<li>三角函数</li>
</ol>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921203343226.png" alt="image-20210921203343226"></p>
<h4 id="两个pythonic的方法"><a href="#两个pythonic的方法" class="headerlink" title="两个pythonic的方法"></a>两个pythonic的方法</h4><p>两个特别pythonic的方法：</p>
<p>torch.addcdiv()</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921203603682.png" alt="image-20210921203603682"></p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921203550310.png" alt="image-20210921203550310"></p>
<p>torch.addcmul()</p>
<p>理解方式同上</p>
<h3 id="E-计算图与动态图机制"><a href="#E-计算图与动态图机制" class="headerlink" title="E. 计算图与动态图机制"></a>E. 计算图与动态图机制</h3><h4 id="1-线性回归（Linear-Regression）"><a href="#1-线性回归（Linear-Regression）" class="headerlink" title="1. 线性回归（Linear Regression）"></a>1. 线性回归（Linear Regression）</h4><p><strong>线性回归</strong>是分析一个<strong>变量</strong>与另一个（多个）<strong>变量</strong>之间<strong>关系</strong>的方法。</p>
<p>因变量：y</p>
<p>自变量：x</p>
<p>关系：y = w*x + b</p>
<p>分析：求解w，b</p>
<h5 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h5><p>Model：y = w*x + b</p>
<h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><p>MSE： <img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921204404418.png" alt="image-20210921204404418"></p>
<h5 id="求解梯度并更新参数"><a href="#求解梯度并更新参数" class="headerlink" title="求解梯度并更新参数"></a>求解梯度并更新参数</h5><p>w = w - LR * w.grad</p>
<p>b = w - LR * b.grad</p>
<h4 id="2-计算图（Computational-Graph）"><a href="#2-计算图（Computational-Graph）" class="headerlink" title="2. 计算图（Computational Graph）"></a>2. 计算图（Computational Graph）</h4><h5 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h5><p>计算图是用来<strong>描述运算</strong>的<strong>有向无环图</strong>。</p>
<p>计算图的两个主要元素：<strong>结点</strong>（Node）和<strong>边</strong>（Edge）</p>
<p>结点表示<strong>数据</strong>，如向量，矩阵，张量</p>
<p>边表示<strong>运算</strong>，如加减乘除卷积等<br>$$<br>y = (x + w) * (w + 1)<br>$$<br>a = x + w<br>b = w + 1<br>y = a * b</p>
<img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921212833057.png" alt="image-20210921212833057" style="zoom: 25%;">

<h5 id="计算图与求导机制"><a href="#计算图与求导机制" class="headerlink" title="计算图与求导机制"></a>计算图与求导机制</h5><p>当我们对w进行求导</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921213436761.png" alt="image-20210921213436761"></p>
<img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921213452525.png" alt="image-20210921213452525" style="zoom:25%;">

<h5 id="is-leaf-叶子结点"><a href="#is-leaf-叶子结点" class="headerlink" title="is_leaf(叶子结点)"></a>is_leaf(叶子结点)</h5><p>torch.Tensor的is_leaf属性。用户创建的结点为叶子结点。</p>
<p>在反向传播（BP）中，所有的计算都要依赖于叶子结点，反向传播之后，所有的非叶子结点的梯度都会被释放掉。</p>
<p>is_leaf：张量是否为叶子结点（结点代表数据，边代表运算）。</p>
<p><strong>如果想使用非叶子结点的梯度，则需要用torch.Tensor.retain_grad()</strong></p>
<h5 id="grad-fn-方法"><a href="#grad-fn-方法" class="headerlink" title="grad_fn(方法)"></a>grad_fn(方法)</h5><p>记录创建该张量时所用的方法（函数）。</p>
<p><strong>叶子结点的gradient_function 为None。</strong></p>
<p>例：</p>
<p>y.grad_fn = &lt; MulBackward0 &gt;</p>
<p>a.grad_fn = &lt; AddBackward0 &gt;</p>
<p>b.grad_fn = &lt; AddBackward0 &gt;</p>
<h4 id="3-动态图vs静态图"><a href="#3-动态图vs静态图" class="headerlink" title="3. 动态图vs静态图"></a>3. 动态图vs静态图</h4><p>(PyTorch) 动态图：运算与搭建<strong>同时</strong>进行（自驾游出行）。</p>
<p>(Tensorflow1.0) 静态图：<strong>先</strong>搭建图，<strong>后</strong>运算（跟旅游团出行）。</p>
<h3 id="F-autograd与逻辑回归"><a href="#F-autograd与逻辑回归" class="headerlink" title="F. autograd与逻辑回归"></a>F. autograd与逻辑回归</h3><p>深度学习模型训练就是不断的更新权值。</p>
<p>我们不需要手动计算梯度，只需要搭好前向计算图。</p>
<h4 id="1-torch-autograd"><a href="#1-torch-autograd" class="headerlink" title="1. torch.autograd"></a>1. torch.autograd</h4><h5 id="torch-autograd-backward"><a href="#torch-autograd-backward" class="headerlink" title="torch.autograd.backward"></a>torch.autograd.backward</h5><p>用于自动求取梯度 </p>
<p>一般常见的loss.backward()就是调用的这个方法。</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210922005408519.png" alt="image-20210922005408519"></p>
<ul>
<li>tensors: 用于求导的张量，如loss</li>
<li>retain_graph：保存计算图</li>
<li>create_graph：创建导数计算图，用于高阶求导</li>
<li>grad_tensors：多梯度权重（用于多个梯度之间权重的设置）</li>
</ul>
<p><strong>注意</strong>：在retain_graph为false的情况，无法连续执行两次反向传播，会提示”the buffers have already been freed“。</p>
<h5 id="torch-autograd-grad"><a href="#torch-autograd-grad" class="headerlink" title="torch.autograd.grad"></a>torch.autograd.grad</h5><p>求取梯度，返回值是我们想要的那一个张量的梯度。</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210922010704023.png" alt="image-20210922010704023"></p>
<ul>
<li>outputs：用于求导的张量，如loss</li>
<li>inputs：需要梯度的张量</li>
<li>create_graph：创建导数计算图（用于高阶求导时要为True，例如对导数再次求导）</li>
<li>retain_graph：保存计算图</li>
<li>grad_outputs：多梯度权重</li>
</ul>
<h5 id="注意"><a href="#注意" class="headerlink" title="注意"></a><strong><u><em>注意</em></u></strong></h5><ol>
<li><p><strong>梯度不自动清零</strong></p>
<p>所以一般在执行backward( )操作之后要进行grad.zero_( )</p>
<p>_下划线表示原地，原位操作（in-place）</p>
</li>
<li><p><strong>依赖于叶子结点的结点，required_grad默认为True</strong></p>
</li>
<li><p><strong>叶子结点不可执行in-place操作</strong></p>
<p>反向传播时根据地址寻找数据，并且用到了叶子结点该地址当中的数据。如果在反向传播前改变了该地址的数据，就会引起反向传播出错。</p>
</li>
</ol>
<h4 id="2-逻辑回归（Logisti-Regress）"><a href="#2-逻辑回归（Logisti-Regress）" class="headerlink" title="2. 逻辑回归（Logisti Regress）"></a>2. 逻辑回归（Logisti Regress）</h4><p>逻辑回归是线性的二分类模型</p>
<p>模型表达式:</p>
<img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210922012147317.png" alt="image-20210922012147317" style="zoom:25%;">

<img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210922012334999.png" alt="image-20210922012334999" style="zoom: 50%;">

<p>线性回归是分析自变量x与因变量y（标量）之间关系的方法</p>
<p>逻辑回归是分析自变量x与因变量y（概率）之间关系的方法</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210922012729014.png" alt="image-20210922012729014"></p>
]]></content>
      <categories>
        <category>PyTorch框架学习</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PythonLibraries</title>
    <url>/2021/09/27/PythonLabraries/</url>
    <content><![CDATA[<h1 id="一些好用且常用的库"><a href="#一些好用且常用的库" class="headerlink" title="一些好用且常用的库"></a>一些好用且常用的库</h1><h2 id="collections"><a href="#collections" class="headerlink" title="collections"></a>collections</h2><h3 id="namedtuple"><a href="#namedtuple" class="headerlink" title="namedtuple()"></a>namedtuple()</h3><p>collections模块的namedtuple子类不仅可以使用索引 (index) 访问item，还可以通过类似字典的方式（key-value）进行访问。在仿真引擎中对机械臂进行控制建模时，需要对同一个关节有多维度的设定，此时namedtuple用起来就很方便。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 适用于仿真中处理描述机器人运动状态和设定的各种参数</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line">author = namedtuple(<span class="string">&#x27;author&#x27;</span>, [<span class="string">&#x27;firstname&#x27;</span>, <span class="string">&#x27;lastname&#x27;</span>])</span><br><span class="line">me = author(Yiting, Chen)</span><br><span class="line"><span class="built_in">print</span> me.firstname, me.lastname</span><br><span class="line"><span class="built_in">print</span> me[<span class="number">0</span>], me[<span class="number">1</span>]</span><br><span class="line">me = author._make([Eating, Chen])</span><br><span class="line"><span class="built_in">print</span> me.firstname, me.lastname</span><br><span class="line">me = me._replace(firstname = Eating716)</span><br><span class="line"><span class="built_in">print</span> me.firstname, me.lastname</span><br><span class="line"></span><br><span class="line"><span class="comment"># results</span></span><br><span class="line"><span class="comment"># Yiting Chen</span></span><br><span class="line"><span class="comment"># Yiting Chen</span></span><br><span class="line"><span class="comment"># Eating Chen</span></span><br><span class="line"><span class="comment"># Eating716 Chen</span></span><br><span class="line"> </span><br><span class="line">websites = [</span><br><span class="line">    (<span class="string">&#x27;Sohu&#x27;</span>, <span class="string">&#x27;http://www.sohu.com/&#x27;</span>, <span class="string">u&#x27;张朝阳&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;Baidu&#x27;</span>, <span class="string">&#x27;http://www.baidu.com.cn/&#x27;</span>, <span class="string">u&#x27;李彦宏&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;163&#x27;</span>, <span class="string">&#x27;http://www.163.com/&#x27;</span>, <span class="string">u&#x27;丁磊&#x27;</span>)</span><br><span class="line">]</span><br><span class="line"> </span><br><span class="line">Website = namedtuple(<span class="string">&#x27;Website&#x27;</span>, [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;url&#x27;</span>, <span class="string">&#x27;founder&#x27;</span>])</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> website <span class="keyword">in</span> websites:</span><br><span class="line">    website = Website._make(website)</span><br><span class="line">    <span class="built_in">print</span> website</span><br><span class="line">    </span><br><span class="line"><span class="comment"># results:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Website(name=&#x27;Sohu&#x27;, url=&#x27;http://www.sohu.com/&#x27;, founder=u&#x27;\u5f20\u671d\u9633&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Website(name=&#x27;Baidu&#x27;, url=&#x27;http://www.baidu.com.cn/&#x27;, founder=u&#x27;\u738b\u5fd7\u4e1c&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Website(name=&#x27;163&#x27;, url=&#x27;http://www.163.com/&#x27;, founder=u&#x27;\u4e01\u78ca&#x27;)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>PythonCoding</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Prologue💬</title>
    <url>/2021/09/26/SayHi/</url>
    <content><![CDATA[<p><strong><font size="10">Hi, this is Yiting Chen🚬. </font></strong></p>
<p><strong><font size="6">I’m an amateur in Robotics &amp; ML🤖. </font></strong></p>
<p><strong><font size="4">Wishing to make some progress in this area📈 </font></strong></p>
<p><strong><font size="6">This blog is for recording my daily study📝.</font></strong></p>
<p><strong><font size="4">Please let me 📬(<a href="mailto:&#99;&#104;&#101;&#x6e;&#x79;&#105;&#116;&#x69;&#110;&#103;&#x40;&#119;&#104;&#117;&#x2e;&#x65;&#100;&#x75;&#x2e;&#x63;&#x6e;">&#99;&#104;&#101;&#x6e;&#x79;&#105;&#116;&#x69;&#110;&#103;&#x40;&#119;&#104;&#117;&#x2e;&#x65;&#100;&#x75;&#x2e;&#x63;&#x6e;</a>) know if there is any question. </font></strong></p>
]]></content>
  </entry>
  <entry>
    <title>面向对象编程</title>
    <url>/2021/09/27/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="面向对象编程"><a href="#面向对象编程" class="headerlink" title="面向对象编程"></a>面向对象编程</h2><h3 id="面向对象的思想"><a href="#面向对象的思想" class="headerlink" title="面向对象的思想"></a>面向对象的思想</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我们现在先整个类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">human</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, nation</span>):</span></span><br><span class="line">        self.planet = earth</span><br><span class="line">        self.nation = nation</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">whereufrom</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;I come from %s&quot;</span> % self.nation)</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Yiting</span>(<span class="params">human</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, age, surname, nation</span>):</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">super</span>(<span class="params">Yiting, self</span>).<span class="title">__init__</span>(<span class="params">nation</span>)</span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">age</span> = <span class="title">age</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">surname</span> = <span class="title">surname</span></span></span><br><span class="line"><span class="function">    	<span class="title">self</span>.<span class="title">firstname</span> = &#x27;<span class="title">Yiting</span>&#x27;</span></span><br><span class="line"><span class="function">        </span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">eating</span>(<span class="params">self, food</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Yiting is eating&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">whoamI</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;My name is Yiting &#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.surname))</span><br><span class="line">        <span class="keyword">return</span> self.firstname + self.surname</span><br><span class="line">    </span><br><span class="line">me = Yiting(<span class="number">21</span>, Chen, China)</span><br></pre></td></tr></table></figure>



<ul>
<li><p><strong>类(Class):</strong> 用来描述具有相同的属性和方法的对象的集合。它定义了该集合中每个对象所共有的属性和方法。对象是类的实例。<strong>实例化</strong>创建一个类的实例，类的具体对象。例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">me = Yiting(<span class="number">21</span>, Chen)</span><br></pre></td></tr></table></figure></li>
<li><p><strong>类变量：</strong>类变量在整个实例化的对象中是公用的。类变量定义在类中且在函数体之外。类变量通常不作为实例变量使用。可以使用点号 <strong>.</strong> 来访问对象的属性。使用如下类的名称访问类变量:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Yiting.age, Yiting.surname</span><br><span class="line"><span class="comment"># 21, Chen</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>方法重写：</strong>如果<strong>从父类继承的方法不能满足子类的需求</strong>，可以对其进行改写，这个过程叫方法的覆盖（override），也称为方法的重写。</p>
</li>
<li><p><strong>局部变量：</strong>定义在方法中的变量，<strong>只作用于当前实例</strong>的类。</p>
</li>
</ul>
<ul>
<li><strong>实例变量：</strong>在类的声明中，属性是用变量来表示的。这种变量就称为实例变量，是在类声明的内部但是在类的其他成员方法之外声明的。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Yiting.age, Yiting.surname, etc.</span><br></pre></td></tr></table></figure>



<ul>
<li><strong>继承：</strong>即一个派生类（derived class）继承基类（base class）的字段和方法。继承也允许把一个派生类的对象作为一个基类对象对待。例如，有这样一个设计：一个Dog类型的对象派生自Animal类，这是模拟”是一个（is-a）”关系（例如Dog是一个Animal）。Python 总是<strong>首先查找对应类型的方法，如果它不能在派生类中找到对应的方法，它才开始到基类中逐个查找。</strong>（先在本类中查找调用的方法，找不到才去基类中找）。</li>
</ul>
<ul>
<li><p><strong>方法：</strong>类中定义的函数。__ init __()方法是一种特殊的方法，被称为类的构造函数或初始化方法，当创建了这个类的实例时就会调用该方法</p>
</li>
<li><p><strong>对象：</strong>通过类定义的数据结构实例。对象包括两个数据成员（类变量和实例变量）和方法。例如me就是对象。</p>
</li>
</ul>
<h3 id="类的私有（保护）属性和方法"><a href="#类的私有（保护）属性和方法" class="headerlink" title="类的私有（保护）属性和方法"></a>类的私有（保护）属性和方法</h3><p>__ private_attrs：两个下划线开头，声明该属性为私有，<strong>不能在类的外部被使用或直接访问。</strong>在类内部的方法中使用时：self.__private_attrs。</p>
<p>__ private_method：两个下划线开头，声明该方法为私有方法，<strong>不能在类的外部调用。</strong>在类的内部调用 self. __private_methods</p>
<p>_foo: 以单下划线开头的表示的是 protected 类型的变量，<strong>即保护类型只能允许其本身与子类进行访问</strong>，不能用于 <strong>from module import *</strong></p>
<h3 id="super-的使用方法"><a href="#super-的使用方法" class="headerlink" title="super()的使用方法"></a>super()的使用方法</h3><p><strong>super()</strong> 函数是用于调用父类(超类)的一个方法。</p>
<p>注意，python2.x和3.x有所不同：</p>
<ul>
<li>Python 3 可以使用直接使用 <strong>super().xxx</strong> 代替 <strong>super(Class, self).xxx</strong> </li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FooParent</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="comment"># 这个为父类</span></span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.parent = <span class="string">&#x27;I\&#x27;m the parent.&#x27;</span></span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&#x27;Parent&#x27;</span>)</span><br><span class="line">    <span class="comment"># 定义函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span>(<span class="params">self,message</span>):</span></span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;%s from Parent&quot;</span> % message)</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FooChild</span>(<span class="params">FooParent</span>):</span></span><br><span class="line">    <span class="comment"># 这个为子类</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># super(FooChild,self) 首先找到 FooChild 的父类（就是类 FooParent），然后把类 FooChild 的对象转换为类 FooParent 的对象</span></span><br><span class="line">        <span class="built_in">super</span>(FooChild,self).__init__()    </span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&#x27;Child&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span>(<span class="params">self,message</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(FooChild, self).bar(message)</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&#x27;Child bar fuction&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span> (self.parent)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    fooChild = FooChild()</span><br><span class="line">    fooChild.bar(<span class="string">&#x27;HelloWorld&#x27;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Parent</span></span><br><span class="line"><span class="comment"># Child</span></span><br><span class="line"><span class="comment"># HelloWorld from Parent</span></span><br><span class="line"><span class="comment"># Child bar fuction</span></span><br><span class="line"><span class="comment"># I&#x27;m the parent.</span></span><br></pre></td></tr></table></figure>

<p>三种情况：</p>
<p>情况一：<strong>子类需要自动调用父类的方法：</strong>子类不重写__ init __ ()方法，实例化子类后，会自动调用父类的 __ init __()的方法。</p>
<p>情况二：<strong>子类不需要自动调用父类的方法：</strong>子类重写 __ init __ ()方法，实例化子类后，将不会自动调用父类的__ init __()的方法。</p>
<p>情况三：<strong>子类重写__init__()方法又需要调用父类的方法：</strong>使用super关键词：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">super</span>(子类，self).__init__(参数<span class="number">1</span>，参数<span class="number">2</span>，....)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Son</span>(<span class="params">Father</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name</span>):</span>   </span><br><span class="line">    <span class="built_in">super</span>(Son, self).__init__(name)</span><br></pre></td></tr></table></figure>



<h2 id><a href="#" class="headerlink" title></a></h2>]]></content>
      <categories>
        <category>PythonCoding</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch-ModelsConstruction</title>
    <url>/2021/09/26/PyTorch-ModelsConstruction/</url>
    <content><![CDATA[<h2 id="三-PyTorch模型搭建（Models）"><a href="#三-PyTorch模型搭建（Models）" class="headerlink" title="三. PyTorch模型搭建（Models）"></a>三. PyTorch模型搭建（Models）</h2><h3 id="A-模型创建步骤与nn-Module"><a href="#A-模型创建步骤与nn-Module" class="headerlink" title="A. 模型创建步骤与nn.Module"></a>A. 模型创建步骤与nn.Module</h3><h4 id="1-网路模型创建步骤"><a href="#1-网路模型创建步骤" class="headerlink" title="1. 网路模型创建步骤"></a>1. 网路模型创建步骤</h4><p>模型（nn.Module）：</p>
<ul>
<li>模型创建：<ul>
<li>构建网络<ul>
<li>卷积层，池化层，激活函数层等等</li>
</ul>
</li>
<li>拼接网络<ul>
<li>LeNet，ResNet，AlexNet等等</li>
</ul>
</li>
</ul>
</li>
<li>权值初始化<ul>
<li>Xavier，Kaiming，正态分布，均匀分布等</li>
</ul>
</li>
</ul>
<p>例：</p>
<p>LeNet模型如下：</p>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923111900159-16326570841981.png" alt="image-20210923111900159"></p>
<p>LeNet计算图：</p>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923111953621-16326570841983.png" alt="image-20210923111953621"></p>
<p>从左向右便是前向传播。</p>
<p>构建模型的两个要素：</p>
<ul>
<li>构建子模块：conv，pool，fc等等：在__ init __()中实现</li>
<li>拼接子模块：按一定的顺序，一定的拓扑结构进行组装：在forward()当中实现</li>
</ul>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># 模型初始化，就是构建子模块</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LeNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, classes)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 拼接子模块，也就是前向传播的一个实现</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = F.relu(self.conv1(x))</span><br><span class="line">        out = F.max_pool2d(out, <span class="number">2</span>)</span><br><span class="line">        out = F.relu(self.conv2(out))</span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = F.relu(self.fc1(out))</span><br><span class="line">        out = F.relu(self.fc2(out))</span><br><span class="line">        out = self.fc3(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line">        </span><br></pre></td></tr></table></figure>



<h4 id="2-nn-Module"><a href="#2-nn-Module" class="headerlink" title="2. nn.Module"></a>2. nn.Module</h4><p><strong>torch.nn</strong>：</p>
<ul>
<li>nn.Parameter：张量子类，表示可学习的参数，如weight，bias</li>
<li>nn.Module：所有网络层的基类，管理网络的属性（例如LeNet要继承它）</li>
<li>nn.functional：函数的具体实现，如卷积，池化，激活函数等等</li>
<li>nn.init：提供了丰富的参数初始化的方法</li>
</ul>
<p><strong>nn.Module</strong>：</p>
<ul>
<li><strong>parameters（重要）</strong>：存储管理nn.Parameter类</li>
<li><strong>modules（重要）</strong>：存储管理nn.Module类（构建子模块）</li>
<li>buffers：存储管理缓冲属性，如BN层中的running_mean</li>
<li>***_hooks：存储管理钩子函数（共五个和hooks有关的字典）</li>
</ul>
<p><strong>在对创建的新module（例如LeNet）进行赋值时（__ init  <strong>()中的属性构建），会被一个</strong> setAttr __()函数进行拦截，判断其为parameter类还是module类。然后在存储进各自对应的字典当中。</strong></p>
<p><strong>总结</strong>：</p>
<ul>
<li>一个module可以包含多个子module</li>
<li>一个module相当于一个运算，必须实现forward()函数</li>
<li>每个module都有8个字典管理它的属性</li>
</ul>
<h3 id="B-模型容器与AlexNet构建"><a href="#B-模型容器与AlexNet构建" class="headerlink" title="B. 模型容器与AlexNet构建"></a>B. 模型容器与AlexNet构建</h3><h4 id="1-Containers"><a href="#1-Containers" class="headerlink" title="1. Containers"></a>1. Containers</h4><p><strong>Containers（容器）</strong>:</p>
<ul>
<li><strong>nn.Sequential</strong>：按顺序的将一组网络层包裹起来</li>
<li><strong>nn.ModuleList</strong>：像python的list一样包装多个网络层</li>
<li><strong>nn.ModuleDict</strong>：像python的dict一样包装多个网络层</li>
</ul>
<h5 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a>nn.Sequential</h5><p><strong>nn.Sequential</strong>是nn.Module的容器，<strong>按顺序</strong>包装一组网络层：</p>
<ul>
<li>顺序性：各网络层之间严格按照顺序构建</li>
<li>自带forward()：自带的forward里，通过for循环一次执行前向运算</li>
</ul>
<p>例1：用Sequential搭建网络</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNetSequential</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># 构建子模块</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LeNetSequential, self).__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">        	nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),)</span><br><span class="line">       	</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">        	nn.Linear(<span class="number">6</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, classes),)</span><br><span class="line">    <span class="comment"># 拼接子模块</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>通常以全连接层为界限，分为特征提取器和分类器两部分。</p>
<p>例2：对Sequential输入OrderDict</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNetSequentialOrderDict</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># 构建子模块</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LeNetSequentialOrderDict, self).__init__()</span><br><span class="line">        self.features = nn.Sequential(OrderDict(&#123;</span><br><span class="line">        	<span class="string">&#x27;conv1&#x27;</span> : nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>),</span><br><span class="line">            <span class="string">&#x27;relu1&#x27;</span> : nn.ReLU(),</span><br><span class="line">            <span class="string">&#x27;pool1&#x27;</span> : nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            </span><br><span class="line">            <span class="string">&#x27;conv2&#x27;</span> : nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            <span class="string">&#x27;relu2&#x27;</span> : nn.ReLU(),</span><br><span class="line">            <span class="string">&#x27;pool2&#x27;</span> : nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>),</span><br><span class="line">        &#125;))</span><br><span class="line">       	</span><br><span class="line">        self.classifier = nn.Sequential(OrderDict(&#123;</span><br><span class="line">        	<span class="string">&#x27;fc1&#x27;</span> : nn.Linear(<span class="number">6</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>),</span><br><span class="line">            <span class="string">&#x27;relu3&#x27;</span> : nn.ReLU(),</span><br><span class="line">            <span class="string">&#x27;fc2&#x27;</span> : nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            <span class="string">&#x27;relu4&#x27;</span> : nn.ReLU(),</span><br><span class="line">            <span class="string">&#x27;fc3&#x27;</span> : nn.Linear(<span class="number">84</span>, classes),</span><br><span class="line">        &#125;))</span><br><span class="line">    <span class="comment"># 拼接子模块</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h5 id="nn-ModuleList"><a href="#nn-ModuleList" class="headerlink" title="nn.ModuleList"></a>nn.ModuleList</h5><p>nn.ModuleList是nn.Module的容器，用于包装一组网络层，以迭代方式调用一组网络层。</p>
<p><strong>主要方法</strong>：</p>
<ul>
<li>append()：在ModuleList后面添加网络层</li>
<li>extend()：拼接两个ModuleList</li>
<li>insert()：指定在ModuleList中位置插入网络层</li>
</ul>
<p>例1：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModuleList</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ModuleList, self).__init__()</span><br><span class="line">        self.linears = nn.ModuleList([nn.Linear(<span class="number">10</span>, <span class="number">10</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>)])</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i, linear <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.linears):</span><br><span class="line">            x = linear(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br></pre></td></tr></table></figure>

<h5 id="nn-ModuleDict"><a href="#nn-ModuleDict" class="headerlink" title="nn.ModuleDict"></a>nn.ModuleDict</h5><p><strong>nn.ModuleDict</strong>是 nn.module的容器， 用于包装一组网络层，以<strong>索引</strong>的方式调用网络层。</p>
<ul>
<li>clear()：清空ModuleDict</li>
<li>items()：返回可迭代的键值对(key-value pairs)</li>
<li>keys()：返回字典的键(key)</li>
<li>values()：返回字典的值(value)</li>
<li>pop()：返回一对键值，并从字典中删除</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModuleDict</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ModuleDict, self).__init__()</span><br><span class="line">        self.choices = nn.ModuleDict(&#123;</span><br><span class="line">            <span class="string">&#x27;conv&#x27;</span> : nn.Conv2d(<span class="number">10</span>, <span class="number">10</span>, <span class="number">3</span>),</span><br><span class="line">            <span class="string">&#x27;pool&#x27;</span> : nn.MaxPool2d(<span class="number">3</span>)</span><br><span class="line">        &#125;)</span><br><span class="line">        </span><br><span class="line">        self.activations = nn.ModuleDict(&#123;</span><br><span class="line">            <span class="string">&#x27;relu&#x27;</span> : nn.ReLU(),</span><br><span class="line">            <span class="string">&#x27;prelu&#x27;</span> : nn.PReLU()</span><br><span class="line">        &#125;)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, choice, act</span>):</span></span><br><span class="line">        x = self.choices[choise](x)</span><br><span class="line">        x = self.activations[act](x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">output = net(fake_img, <span class="string">&#x27;conv&#x27;</span>, <span class="string">&#x27;prelu&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h5 id="容器总结："><a href="#容器总结：" class="headerlink" title="容器总结："></a>容器总结：</h5><ul>
<li>nn.Sequential：<strong>顺序性</strong>，个网络层之间严格按顺序执行，常用于block构建</li>
<li>nn.ModuleList：<strong>迭代性</strong>，常用于大量重复网络构建，通过for循环实现重复构建</li>
<li>nn.ModuleDict：<strong>索引性</strong>，常用于可选择的网络层</li>
</ul>
<h4 id="2-AlexNet"><a href="#2-AlexNet" class="headerlink" title="2.AlexNet"></a>2.AlexNet</h4><p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923150239978-16326570841982.png" alt="image-20210923150239978"></p>
<p>前面卷积池化的部分组装成<strong>features</strong>，后面的全连接部分组装成<strong>classifier</strong></p>
<h3 id="C-nn网络层-卷积层"><a href="#C-nn网络层-卷积层" class="headerlink" title="C. nn网络层-卷积层"></a>C. nn网络层-卷积层</h3><h4 id="1-1d-2d-3d卷积"><a href="#1-1d-2d-3d卷积" class="headerlink" title="1. 1d/2d/3d卷积"></a>1. 1d/2d/3d卷积</h4><p>卷积运算：卷积核在输入信号（图像）上滑动，相应位置上进行<strong>乘加</strong>。</p>
<p>卷积核：又称为滤波器，过滤器，可认为是某种模式，特征。</p>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923195219275-16326570841984.png" alt="image-20210923195219275"></p>
<p>卷积过程类似于用一个模板去图像上寻找与他类似的区域，与卷积核模式越相似，激活值越高，从而实现特征提取。</p>
<p>AlexNet卷积核可视化（如下图），发现卷积核学习到的是边缘，条纹，色彩着一些细节模式。<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923195435689-16326570841985.png" alt="image-20210923195435689" style="zoom:50%;"></p>
<p><strong>卷积维度</strong>：一般情况下，卷积核<strong>在几个维度下滑动，就是几维卷积</strong>。</p>
<p>一个卷积核在一个信号上的几维卷积。</p>
<p>一维卷积：</p>
<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923201126680-16326570841987.png" alt="image-20210923201126680" style="zoom:33%;">

<p>二维卷积：</p>
<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923201301917-16326570841986.png" alt="image-20210923201301917" style="zoom:33%;">

<p>三维卷积：                        <img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923201323469-16326570841988.png" alt="image-20210923201323469" style="zoom:33%;">、</p>
<p>nn.Conv2d</p>
<p>对多个二维信号进行二维卷积</p>
<p>主要参数：</p>
<ul>
<li>in_channels：输入通道数</li>
<li>out_channels：输出通道数，等价于卷积核个数</li>
<li>kernel_size：卷积核尺寸</li>
<li>stride：步长（每次滑动经过多少像素）</li>
<li>padding：填充个数（保持输入和输出尺寸匹配）</li>
<li>dilation：孔洞卷积大小（常用于图像分割任务，用于提升感受区域）</li>
<li>groups：分组卷积设置</li>
<li>bias：偏置</li>
</ul>
<p>图像尺寸变化：</p>
<p>（简化版）：outsize = （Insize - kernelsize）/ stride + 1</p>
<p>（完整版）：<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923202218467-16326570841989.png" alt="image-20210923202218467"></p>
<h4 id="2-卷积-nn-Conv2d"><a href="#2-卷积-nn-Conv2d" class="headerlink" title="2. 卷积-nn.Conv2d()"></a>2. 卷积-nn.Conv2d()</h4><p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923205449306-163265708419810.png" alt="image-20210923205449306"></p>
<h4 id="3-转置卷积-nn-ConvTranspose"><a href="#3-转置卷积-nn-ConvTranspose" class="headerlink" title="3.转置卷积-nn.ConvTranspose"></a>3.转置卷积-nn.ConvTranspose</h4><p>转置卷积又称为反卷积（Deconvolution)和部分跨越卷积（Fractionally strided Convolution），用于对图像进行上采样（UpSample）。</p>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923210001350-163265708419811.png" alt="image-20210923210001350"></p>
<p>nn.Transpose2d</p>
<p>转置卷积实现上采样</p>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923210527210-163265708419812.png" alt="image-20210923210527210"></p>
<p>主要参数：</p>
<ul>
<li>in_channels：输入通道数</li>
<li>out_channels：输出通道数，等价于卷积核个数</li>
<li>kernel_size：卷积核尺寸</li>
<li>stride：步长（每次滑动经过多少像素）</li>
<li>padding：填充个数（保持输入和输出尺寸匹配）</li>
<li>dilation：孔洞卷积大小（常用于图像分割任务，用于提升感受区域）</li>
<li>groups：分组卷积设置</li>
<li>bias：偏置</li>
</ul>
<p>转置卷积的尺寸计算：</p>
<p>（简化版）outsize = （insize - 1）*stride + kernelsize</p>
<p>（完整版）：<br><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923210734836-163265708419814.png" alt="image-20210923210734836"></p>
<h3 id="D-池化、线性、激活函数层"><a href="#D-池化、线性、激活函数层" class="headerlink" title="D. 池化、线性、激活函数层"></a>D. 池化、线性、激活函数层</h3><h4 id="1-池化层——Pooling-Layer"><a href="#1-池化层——Pooling-Layer" class="headerlink" title="1. 池化层——Pooling Layer"></a>1. 池化层——Pooling Layer</h4><p>池化运算：对信号进行 <strong>“收集”</strong> 并 <strong>“总结”</strong>，类似水池收集水资源，因而得名池化层。</p>
<p><strong>“收集”</strong>：多变少</p>
<p><strong>“总结”</strong>：最大值 / 平均值</p>
<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923211736432-163265708419813.png" alt="image-20210923211736432" style="zoom:50%;">

<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923211835083-163265708419815.png" alt="image-20210923211835083" style="zoom:50%;">

<h5 id="nn-MaxPool2d"><a href="#nn-MaxPool2d" class="headerlink" title="nn.MaxPool2d"></a>nn.MaxPool2d</h5><p>对二维信号（图像）进行最大值池化</p>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923212056580-163265708419916.png" alt="image-20210923212056580"></p>
<p>主要参数：</p>
<ul>
<li>kernel_size：池化核尺寸</li>
<li>stride：步长（步长通常与窗口大小一样）</li>
<li>padding：填充个数</li>
<li>dilation：池化核间隔大小</li>
<li>ceil_mode：尺寸向上取整</li>
<li>return_indices：记录池化像素索引（记录最大值在的位置，在反池化的时候把最大值放在对应的位置上）</li>
</ul>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923212830427-163265708419917.png" alt="image-20210923212830427"></p>
<h5 id="nn-AvgPool2d"><a href="#nn-AvgPool2d" class="headerlink" title="nn.AvgPool2d"></a>nn.AvgPool2d</h5><p>对二维信号（图像）进行平均值池化</p>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923213658337-163265708419918.png" alt="image-20210923213658337"></p>
<p>主要参数：</p>
<ul>
<li>kernel_size：池化核尺寸</li>
<li>stride：步长（步长通常与窗口大小一样）</li>
<li>padding：填充个数</li>
<li>ceil_mode：尺寸向上取整</li>
<li>count_include_pad：填充值用于计算</li>
<li>divisor_override：除法因子</li>
</ul>
<h5 id="nn-MaxUnpool2d"><a href="#nn-MaxUnpool2d" class="headerlink" title="nn.MaxUnpool2d"></a>nn.MaxUnpool2d</h5><p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923221525532-163265708419919.png" alt="image-20210923221525532"></p>
<p>对二维信号（图像）进行最大值池化上采样</p>
<p>主要参数：</p>
<ul>
<li>kernel_size：池化核尺寸</li>
<li>stride：步长</li>
<li>padding：填充个数</li>
</ul>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923212830427-163265708419917.png" alt="image-20210923212830427"></p>
<p>在forward( )的时候要传入索引层，就是indices。</p>
<h4 id="2-线性层——Linear-Layer"><a href="#2-线性层——Linear-Layer" class="headerlink" title="2. 线性层——Linear Layer"></a>2. 线性层——Linear Layer</h4><p>线性层又称全连接层，其<strong>每个神经元与上一层所有神经元相连</strong>。</p>
<p>实现对前一层的<strong>线性组合，线性变换</strong></p>
<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923222244138-163265708419920.png" alt="image-20210923222244138" style="zoom:25%;">

<h5 id="nn-Linear"><a href="#nn-Linear" class="headerlink" title="nn.Linear"></a>nn.Linear</h5><p>对一维信号（向量）进行线性组合</p>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923222813962-163265708419921.png" alt="image-20210923222813962"></p>
<p>主要参数：</p>
<ul>
<li>in_features：输入结点数</li>
<li>out_features：输出结点数</li>
<li>bias：是否需要偏置</li>
</ul>
<p>计算公式：<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923222749821-163265708419922.png" alt="image-20210923222749821" style="zoom:50%;"></p>
<h4 id="3-激活函数层——Activation-Layer"><a href="#3-激活函数层——Activation-Layer" class="headerlink" title="3. 激活函数层——Activation Layer"></a>3. 激活函数层——Activation Layer</h4><p>激活函数对特征进行<strong>非线性变换</strong>，赋予多层神经网络具有<strong>深度意义</strong>。</p>
<p>如果<strong>没有</strong>非线性变化，<strong>无论</strong>多少层线性层叠加都没有意义，一个矩阵就都能搞定。</p>
<h5 id="nn-Sigmoid"><a href="#nn-Sigmoid" class="headerlink" title="nn.Sigmoid"></a>nn.Sigmoid</h5><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923223344685-163265708419923.png" alt="image-20210923223344685" style="zoom:33%;">

<p>特性：</p>
<ul>
<li>输出值在（0，1），符合概率</li>
<li>导数范围是[0, 0.25]，容易梯度消失</li>
<li>输出为非0均值，破坏数据分布</li>
</ul>
<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923223502929-163265708419924.png" alt="image-20210923223502929" style="zoom: 25%;">

<h5 id="nn-tanh"><a href="#nn-tanh" class="headerlink" title="nn.tanh"></a>nn.tanh</h5><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923223628333-163265708419925.png" alt="image-20210923223628333" style="zoom: 50%;">

<p>特性：</p>
<ul>
<li>输出值在（-1，1），数据符合0均值</li>
<li>导数范围是（0，1），易导致梯度消失</li>
</ul>
<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923223736195-163265708419926.png" alt="image-20210923223736195" style="zoom: 25%;">

<h5 id="nn-ReLU"><a href="#nn-ReLU" class="headerlink" title="nn.ReLU"></a>nn.ReLU</h5><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923223932829-163265708419927.png" alt="image-20210923223932829" style="zoom: 33%;">

<p>特性：</p>
<ul>
<li>输出值均为正数，负半轴导致死神经元（有一部分神经元就死掉了）</li>
<li>导数是1（不改变梯度的尺度），缓解梯度消失，但易引发梯度爆炸</li>
</ul>
<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923224046062-163265708419928.png" alt="image-20210923224046062" style="zoom:25%;">

<h5 id="nn-ReLU变式"><a href="#nn-ReLU变式" class="headerlink" title="nn.ReLU变式"></a>nn.ReLU变式</h5><p>nn.LeakyReLU</p>
<ul>
<li>negative_slope：负半轴斜率</li>
</ul>
<p>nn.PReLU</p>
<ul>
<li>init：可学习斜率</li>
</ul>
<p>nn.RReLU</p>
<p>R代表random</p>
<ul>
<li>lower：均匀分布下限</li>
<li>upper：均匀分布上限</li>
</ul>
<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923224438706-163265708419929.png" alt="image-20210923224438706" style="zoom:25%;">

<p><strong>还有很多其他改进</strong>，日后补充</p>
]]></content>
      <categories>
        <category>PyTorch框架学习</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch-Loss&amp;Optimizer</title>
    <url>/2021/09/26/PyTorch-Loss-Optimizer/</url>
    <content><![CDATA[<h2 id="四-PyTorch损失优化（Loss-amp-Optimizer）"><a href="#四-PyTorch损失优化（Loss-amp-Optimizer）" class="headerlink" title="四. PyTorch损失优化（Loss&amp;Optimizer）"></a>四. PyTorch损失优化（Loss&amp;Optimizer）</h2><h3 id="A-权值初始化"><a href="#A-权值初始化" class="headerlink" title="A. 权值初始化"></a>A. 权值初始化</h3><h4 id="1-梯度消失与爆炸Gradient-Vanishing-and-Exploring"><a href="#1-梯度消失与爆炸Gradient-Vanishing-and-Exploring" class="headerlink" title="1. 梯度消失与爆炸Gradient Vanishing and Exploring"></a>1. 梯度消失与爆炸Gradient Vanishing and Exploring</h4><h5 id="方差一致性原则"><a href="#方差一致性原则" class="headerlink" title="方差一致性原则"></a>方差一致性原则</h5><p>$$</p>
<ol>
<li> E(X*Y) = E(X)*E(Y)\</li>
<li> D(X) = E(X^2) - [E(X)]^2\</li>
<li> D(X+Y) = D(X) + D(Y)<br>$$</li>
</ol>
<p>$$<br>1.2.3 \Rightarrow D(X<em>Y) = D(X)<em>D(Y) + D(X)</em>[E(Y)]^2 + D(Y)</em>[E(X)]^2\<br>若E(X)=0, E(Y)=0\<br>则D(X*Y) = D(X)*D(Y)<br>$$</p>
<p>$$<br>第一个隐藏层的第一个神经元, 输入层的每一个神经元✖权值求和：\<br>H_{11} = \sum^{n}<em>{i=0}X_i*W</em>{1i}\由公式 D(X<em>Y) = D(X)<em>D(Y)\<br>可知D(H_{11}) = \sum^{n}<em>{i=0}D(X_i)*D(W</em>{1i})\ = n*(1</em>1)n为神经元的个数，X_i和W_i的方差均为1\<br>=n\<br>std(H_{11}) = \sqrt{D(H_{11})} = \sqrt{n}\<br>意味着经过一层神经元的传播标准差就会扩大\sqrt{n}倍\<br>因此，想让方差D(H_1) = n</em>D(X)*D(W) = 1\<br>D(W)=\frac{1}{n}\Rightarrow std(W)=\sqrt{\frac{1}{n}}\<br>所以权值的标准差为\sqrt{\frac{1}{n}}时（n为每一层神经元个数），\才可以保证尺度维持在恰当范围。<br>$$</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210924001951770.png" alt="image-20210924001951770"></p>
<p>需要保持尺度不变</p>
<h4 id="2-Xavier方法和Kaiming方法"><a href="#2-Xavier方法和Kaiming方法" class="headerlink" title="2. Xavier方法和Kaiming方法"></a>2. Xavier方法和Kaiming方法</h4><p>Xavier初始化</p>
<p>方差一致性：保持数据尺度维持在恰当范围，通常方差为1。</p>
<p>适用激活函数：饱和函数，如sigmoid，Tanh</p>
<img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210924004244068.png" alt="image-20210924004244068" style="zoom: 50%;">

<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210924004442111.png" alt="image-20210924004442111"></p>
<p>Kaiming初始化</p>
<p>方差一致性：保持数据尺度维持在恰当范围，通常方差为1。</p>
<p>适用激活函数：ReLU及其变种</p>
<img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210924005023102.png" alt="image-20210924005023102" style="zoom: 33%;">





<h4 id="3-常用初始化方法"><a href="#3-常用初始化方法" class="headerlink" title="3. 常用初始化方法"></a>3. 常用初始化方法</h4><h5 id="PyTorch方法查询"><a href="#PyTorch方法查询" class="headerlink" title="PyTorch方法查询"></a>PyTorch方法查询</h5><p><a href="https://pytorch.org/docs/stable/nn.init.html">https://pytorch.org/docs/stable/nn.init.html</a></p>
<ul>
<li>Xavier均匀/正态分布</li>
<li>Kaiming均匀/正态分布</li>
<li>均匀分布 / 正态分布 / 常熟分布</li>
<li>正交矩阵初始化 / 单位矩阵初始化 / 稀疏矩阵初始化</li>
</ul>
<h5 id="nn-init-calculate-gain"><a href="#nn-init-calculate-gain" class="headerlink" title="nn.init.calculate_gain()"></a>nn.init.calculate_gain()</h5><p>nn.init.calculate_gain(nonlinearity, param=None)</p>
<p>计算激活函数的方差变换尺度</p>
<p>主要参数：</p>
<ul>
<li>nonlinearity：激活函数名称</li>
<li>param：激活函数的参数，如leakyReLU的negative_slop</li>
</ul>
<h3 id="B-损失函数"><a href="#B-损失函数" class="headerlink" title="B. 损失函数"></a>B. 损失函数</h3><h4 id="1-损失函数概念"><a href="#1-损失函数概念" class="headerlink" title="1. 损失函数概念"></a>1. 损失函数概念</h4><p><strong>损失函数</strong>：衡量模型输出与真实标签的<strong>差异</strong></p>
<img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210924105850749.png" alt="image-20210924105850749" style="zoom:50%;">

<p>损失函数（Loss Function）：</p>
<p>计算<strong>一个样本</strong><br>$$<br>Loss = f（y’，y)<br>$$<br>代价函数（Cost Function）：</p>
<p>计算<strong>训练集（样本集）</strong>中各个Loss的<strong>平均值</strong><br>$$<br>Cost = \frac{1}{N}\sum^{N}_{i}f(y’_i, y_i)<br>$$<br>目标函数（Objective Function）：<br>$$<br>Obj = Cost（代价函数） + Regularization<br>$$<br>Cost代价函数<strong>并不是越小越好</strong>，可能会有过拟合。约束项是Regularization（L1，L2，稀疏约束），和Cost一起构成目标函数。</p>
<h4 id="2-损失函数"><a href="#2-损失函数" class="headerlink" title="2. 损失函数"></a>2. 损失函数</h4><h5 id="nn-CrossEntropyLoss"><a href="#nn-CrossEntropyLoss" class="headerlink" title="nn.CrossEntropyLoss"></a>nn.CrossEntropyLoss</h5><p><strong>nn.LogSoftmax()与nn.NLLLoss()结合</strong>，进行交叉熵计算</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210924112655749.png" alt="image-20210924112655749"></p>
<p>主要参数：</p>
<ul>
<li>weight：各类别的loss设置权值</li>
<li>ignore_index：忽略某个类别</li>
<li>reduction：计算模式，可为none/sum/mean。‘none’不会造成维度上的衰减，‘sum’会将loss值相加，‘mean’会对所有loss求平均。  </li>
</ul>
<p>none- 逐个元素计算</p>
<p>sum- 所有元素求和，返回标量</p>
<p>mean- 加权平均，返回标量</p>
<p>交叉熵 = 信息熵 + 相对熵</p>
<p>熵（用来描述一件事情的不确定性，不确定性的概率越大，熵就越大）：</p>
<p>熵是自信息的一个期望，是描述整个概率分布：<br>$$<br>H(P)=E_{x\thicksim p}[I(x)] = -\sum^{N}<em>{i}P(x_i)logP(x_i)<br>$$<br>自信息的公式，对概率取一个负log：<br>$$<br>I(x) = -log[p(x)]<br>$$<br>相对熵，用来衡量两个分布之间的距离（差异）：<br>$$<br>D</em>{KL}(P, Q) = E_{x\thicksim p}[log\frac{P(x)}{Q(x)}]\<br>= E_{x\thicksim p}[logP(x) - logQ(x)]\<br>=\sum^{N}<em>{i=1}P(x_i)[logP(x_i)-logQ(x_i)]\<br>=\sum^{N}</em>{i=1}P(x_i)logP(x_i)-\sum^{N}_{i=1}P(x_i)logQ(x_i)\<br>=H(P,Q) - H(P)\<br>P是真实分布，Q是模型拟合的分布。我们要用Q去拟合（逼近）P<br>$$<br>weight参数的使用：</p>
<ul>
<li>向量的形式，有多少个类别，就要有多少个weight</li>
<li>使用之后，会给不同的类别赋予不同的权值</li>
</ul>
<h5 id="nn-NLLLoss"><a href="#nn-NLLLoss" class="headerlink" title="nn.NLLLoss"></a>nn.NLLLoss</h5><p>实现负对数似然函数中的负号功能（其实只是执行了加个负号的功能）</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210924163054623.png" alt="image-20210924163054623"></p>
<p>主要参数：</p>
<ul>
<li>weight：各类别的loss设置权值</li>
<li>ignore_index：忽略某个类别</li>
<li>reduction：计算模式，可为none/sum/mean</li>
</ul>
<p>none- 逐个元素计算</p>
<p>sum- 所有元素求和，返回标量</p>
<p>mean- 加权平均，返回标量</p>
<h5 id="nn-BCELoss"><a href="#nn-BCELoss" class="headerlink" title="nn.BCELoss"></a>nn.BCELoss</h5><p>二分类交叉熵</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925145330328.png" alt="image-20210925145330328"></p>
<p><strong>注意事项</strong>：输入值取值在[0, 1]，符合概率取值的区间</p>
<p>主要参数：</p>
<ul>
<li>weight：各类别的loss设置权值</li>
<li>ignore_index：忽略某个类别</li>
<li>reduction：计算模式，可为none/sum/mean</li>
</ul>
<h5 id="nn-BCEWithLogitsLoss"><a href="#nn-BCEWithLogitsLoss" class="headerlink" title="nn.BCEWithLogitsLoss"></a>nn.BCEWithLogitsLoss</h5><p>结合<strong>Sigmoid</strong>与<strong>二分类交叉熵</strong></p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925145853652.png" alt="image-20210925145853652"></p>
<p><strong>注意事项：网络后面不加sigmoid函数</strong></p>
<p>主要参数：</p>
<ul>
<li>pos_weight：正样本的权值</li>
<li>weight：各类别的loss设置权值</li>
<li>ignore_index：忽略某个类别</li>
<li>reduction：计算模式，可为none/sum/mean</li>
</ul>
<h5 id="nn-L1Loss"><a href="#nn-L1Loss" class="headerlink" title="nn.L1Loss"></a>nn.L1Loss</h5><p>计算inputs与target之差的绝对值</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925150840297.png" alt="image-20210925150840297"></p>
<h5 id="nn-MSELoss"><a href="#nn-MSELoss" class="headerlink" title="nn.MSELoss"></a>nn.MSELoss</h5><p>计算inputs与target之差的平方</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925150852293.png" alt="image-20210925150852293"></p>
<p>主要参数：</p>
<ul>
<li>reduction：计算模式，可为none/ sum/ mean</li>
</ul>
<h5 id="nn-SmoothL1Loss"><a href="#nn-SmoothL1Loss" class="headerlink" title="nn.SmoothL1Loss"></a>nn.SmoothL1Loss</h5><p>平滑的L1Loss。</p>
<p>Xi是模型输出，Yi是标签。</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925151138993.png" alt="image-20210925151138993"></p>
<p>主要参数：</p>
<ul>
<li>reduction：计算模式，可为none/ sum/ mean</li>
<li><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925151323568.png" alt="image-20210925151323568" style="zoom:33%;"></li>
</ul>
<h5 id="nn-PoissonNLLLoss"><a href="#nn-PoissonNLLLoss" class="headerlink" title="nn.PoissonNLLLoss"></a>nn.PoissonNLLLoss</h5><p>泊松分布的负对数似然损失函数</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925151752707.png" alt="image-20210925151752707"></p>
<p>主要参数：</p>
<ul>
<li><strong>log_input</strong>：输入是否为对数形式，决定计算公式</li>
<li><strong>full</strong>：计算所有loss，默认为False</li>
<li><strong>eps</strong>：修正项，避免log（input）为nan</li>
</ul>
<h5 id="nn-KLDivLoss"><a href="#nn-KLDivLoss" class="headerlink" title="nn.KLDivLoss"></a>nn.KLDivLoss</h5><p>计算<strong>KLD（divergence）</strong>，KL散度，<strong>相对熵</strong>。衡量两个分布之间的距离。</p>
<p><strong>注意事项：需提前将输入计算log-probabilities，如通过nn.logsoftmax()</strong></p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925152326201.png" alt="image-20210925152326201"></p>
<p>主要参数：</p>
<ul>
<li>reduction：none/ sum/ mean /batchmean</li>
</ul>
<p>batchmean- batchsize维度求平均值</p>
<h5 id="nn-MarginRankingLoss"><a href="#nn-MarginRankingLoss" class="headerlink" title="nn.MarginRankingLoss"></a>nn.MarginRankingLoss</h5><p>计算两个向量之间的相似度，<strong>用于排序任务</strong></p>
<p>y=1时，希望x1与x2大，当x1&gt;x2时，不产生loss</p>
<p>y=-1时，希望x2比x1大，当x2&gt;x1时，不产生loss</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925153835354.png" alt="image-20210925153835354"></p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925153849860.png" alt="image-20210925153849860"></p>
<p><strong>特别说明：该方法计算两组数据之间的差异，返回一个n*n的loss矩阵</strong></p>
<p>主要参数：</p>
<ul>
<li>margin：边界值，x1与x2之间的差异值</li>
<li>reduction：计算模式</li>
</ul>
<h5 id="nn-MultiLabelMarginLoss"><a href="#nn-MultiLabelMarginLoss" class="headerlink" title="nn.MultiLabelMarginLoss"></a>nn.MultiLabelMarginLoss</h5><p>多标签边界损失函数</p>
<p>举例：四分类任务，样本x属于0类或者3类</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925155812561.png" alt="image-20210925155812561"></p>
<p>标签：[0，3，-1，1]，不是[1，0，0，1]</p>
<h5 id="nn-SofrMarginLoss"><a href="#nn-SofrMarginLoss" class="headerlink" title="nn.SofrMarginLoss"></a>nn.SofrMarginLoss</h5><p>计算二分类的logistic损失</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925160607942.png" alt="image-20210925160607942"></p>
<h5 id="nn-MultiLabelSoftMarginLoss"><a href="#nn-MultiLabelSoftMarginLoss" class="headerlink" title="nn.MultiLabelSoftMarginLoss"></a>nn.MultiLabelSoftMarginLoss</h5><p>SoftMarginLoss多标签版本</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925160853772.png" alt="image-20210925160853772"></p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925160909202.png" alt="image-20210925160909202"></p>
<p>主要参数：</p>
<p><strong>weight</strong>：各类别的loss设置权值</p>
<p><strong>reduction</strong>：计算模式，可为none / sum / mean</p>
<h5 id="nn-MultiMarginLoss"><a href="#nn-MultiMarginLoss" class="headerlink" title="nn.MultiMarginLoss"></a>nn.MultiMarginLoss</h5><p>计算多分类的折页损失</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925161602192.png" alt="image-20210925161602192"></p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925161628228.png" alt="image-20210925161628228"></p>
<p>主要参数：</p>
<ul>
<li>p：可选1或2</li>
<li>weight：各类别的loss设置权值</li>
<li>margin：边界值</li>
<li>reduction：计算模式，可为none/sum/mean</li>
</ul>
<h5 id="nn-TripletMarginLoss"><a href="#nn-TripletMarginLoss" class="headerlink" title="nn.TripletMarginLoss"></a>nn.TripletMarginLoss</h5><p>计算三元组损失，人脸验证中常用</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925162115800.png" alt="image-20210925162115800"></p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925162211016.png" alt="image-20210925162211016"></p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925162237021.png" alt="image-20210925162237021"></p>
<p>主要参数：</p>
<ul>
<li>p：范数的阶，默认为2</li>
<li>margin：边界值</li>
<li>reduction：计算模式，可为none/sum/mean</li>
</ul>
<h5 id="nn-HingeEmbeddingLoss"><a href="#nn-HingeEmbeddingLoss" class="headerlink" title="nn.HingeEmbeddingLoss"></a>nn.HingeEmbeddingLoss</h5><p>计算两个输入的相似性，常用于非线性embedding和半监督学习</p>
<p><strong>特别注意：输入x应为两个输入只差的绝对值</strong></p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925162430193.png" alt="image-20210925162430193"></p>
<p>主要参数：</p>
<ul>
<li>margin：边界值</li>
<li>reduction：计算模式，可为none/sum/mean</li>
</ul>
<h5 id="nn-CosineEmbeddingLoss"><a href="#nn-CosineEmbeddingLoss" class="headerlink" title="nn.CosineEmbeddingLoss"></a>nn.CosineEmbeddingLoss</h5><p>采用余弦相似度计算两个输入的相似性，主要关注方向上的差异。</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925162632490.png" alt="image-20210925162632490"></p>
<p>主要参数：</p>
<p>margin：可取值[-1，1]，推荐为[0，0.5]</p>
<p>reduction：计算模式，可为none/sum/mean</p>
<h5 id="nn-CTCLoss"><a href="#nn-CTCLoss" class="headerlink" title="nn.CTCLoss"></a>nn.CTCLoss</h5><p>计算CTC损失，解决时序类数据的分类Connectionist Temporal Classification</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925163057884.png" alt="image-20210925163057884"></p>
<p>主要参数：</p>
<ul>
<li>blank：blank label</li>
<li>zero_infinity：无穷大的值或者梯度置0</li>
<li>reduction：计算模式，可为none/sum/mean</li>
</ul>
<h3 id="C-优化器-Optimizer"><a href="#C-优化器-Optimizer" class="headerlink" title="C. 优化器 Optimizer"></a>C. 优化器 Optimizer</h3><h4 id="1-什么是优化器"><a href="#1-什么是优化器" class="headerlink" title="1. 什么是优化器"></a>1. 什么是优化器</h4><p>pytorch的优化器：管理并更新模型中的可学习参数的值，使得模型输出更接近真实标签</p>
<ul>
<li><strong>导数</strong>：函数在指定坐标轴上的变化率</li>
<li><strong>方向导数</strong>：指定方向上的变化率</li>
<li><strong>梯度</strong>：一个向量，方向为方向导数取得最大值的方向</li>
</ul>
<img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925164501982.png" alt="image-20210925164501982" style="zoom:50%;">

<h4 id="2-optimizer的属性"><a href="#2-optimizer的属性" class="headerlink" title="2. optimizer的属性"></a>2. optimizer的属性</h4><p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925164937096.png" alt="image-20210925164937096"></p>
<p>基本属性：</p>
<ul>
<li>defaults：优化器超参数</li>
<li>state：参数的缓存，如momentum的缓存</li>
<li>param_groups：管理的参数组</li>
<li>_step_count：记录更新的次数，学习率调整中使用</li>
</ul>
<p>基本方法：</p>
<ul>
<li><p>zero_grad()：清空所管理参数的梯度<img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925165616845.png" alt="image-20210925165616845" style="zoom:50%;"></p>
</li>
<li><p>step()：执行一步更新，更新一次我们的权值参数</p>
</li>
<li><p>add_param_group()：添加参数组</p>
</li>
<li><p>state_dict()：获取优化器当前状态信息字典</p>
</li>
<li><p>load_state_dict()：加载状态信息字典（用于模型断点的续训练）</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925211631557.png" alt="image-20210925211631557"></p>
</li>
</ul>
<p><strong>pytorch特性：张量梯度不自动清零，会将每一次计算的梯度加在一起。所以需要使用完梯度，或者backward之前，要zero_grad()一下。</strong></p>
<h4 id="3-optimizer的方法"><a href="#3-optimizer的方法" class="headerlink" title="3. optimizer的方法"></a>3. optimizer的方法</h4><h4 id="4-learning-rate学习率"><a href="#4-learning-rate学习率" class="headerlink" title="4. learning rate学习率"></a>4. learning rate学习率</h4><p>梯度下降：<br>$$<br>W_{i+1} = W_{i} - g(W_{i})<br>$$<br>假设现在函数为<br>$$<br>y = f(x) = 4<em>x^2\<br>y’ = f’(x)=8</em>x<br>$$<br><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925215213471.png" alt="image-20210925215213471" style="zoom:33%;"><br>$$<br>x_0 =2,y_0 = 16,f’(x_0)=16\<br>x_1=x_0 - f’(x_0)=2-16=14\<br>x_1=-14,y_1 = 784, f’(x_1)=-112\<br>x_2 = x_1 - f’(x_1)=-14+112=98,y_2=38416\<br>……<br>$$<br>如果没有学习率的话，就会梯度爆炸。</p>
<img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925220758203.png" alt="image-20210925220758203" style="zoom: 50%;">

<p>学习率（learning rate）控制更新的步伐。<br>$$<br>W_{i+1} = W_{i} - LR*g(W_{i})<br>$$<br>此时我们将LR设置为0.2：</p>
<img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925221017651.png" alt="image-20210925221017651" style="zoom: 50%;">

<p>将LR设置为0.1时：</p>
<img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925221141585.png" alt="image-20210925221141585" style="zoom: 50%;">



<h4 id="5-momentum-动量"><a href="#5-momentum-动量" class="headerlink" title="5. momentum 动量"></a>5. momentum 动量</h4><p>Momentum（动量，冲量）：<strong>结合当前梯度与上一次更新信息</strong>，用于当前更新。</p>
<p>指数加权平均：<br>$$<br>v_t = \beta * v_{t-1} + (1-\beta)*\theta_t\<br>v_{100} = \beta * v_{99} + (1-\beta)*\theta_{100}\<br>= (1-\beta)<em>\theta_{100} + (1-\beta)<em>\beta</em>\theta_{99}+(\beta^2</em>v_{98})\<br>=\sum^N_i(1-\beta)<em>\beta^i</em>\theta_{N-i}<br>$$<br><strong>核心思想</strong>：距离当前时间越近，影响越大；距离当前时间越远，影响越小。</p>
<p>PyTorch中的更新公式：<br>$$<br>v_i = m<em>v_{i-1}+g(w_i)\<br>w_{i+1}=w_i - lr</em>v_i\<br>w_{w+1}：第i+1次更新的参数\<br>lr：学习率\<br>v_i = 更新量\<br>m：momentum系数\<br>g(w_i)：w_i的梯度<br>$$</p>
<h4 id="6-torch-optim-SGD"><a href="#6-torch-optim-SGD" class="headerlink" title="6. torch.optim.SGD"></a>6. torch.optim.SGD</h4><p>用的最多的优化器，绝大部分模型都可以用SGD获得一个不错的效果。</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925230235496.png" alt="image-20210925230235496"></p>
<p>主要参数：</p>
<ul>
<li>params：管理的参数组</li>
<li>lr：初始学习率</li>
<li>momentum：动量参数，贝塔</li>
<li>weight_decay：L2正则化系数</li>
<li>nesterov：是否采用NAG（通常为false，不会采用）</li>
</ul>
<h3 id="D-Pytorch的十种优化器"><a href="#D-Pytorch的十种优化器" class="headerlink" title="D. Pytorch的十种优化器"></a>D. Pytorch的十种优化器</h3><p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925230441311.png" alt="image-20210925230441311"></p>
<h4 id="TODO：详细使用方法以后补充"><a href="#TODO：详细使用方法以后补充" class="headerlink" title="TODO：详细使用方法以后补充"></a>TODO：详细使用方法以后补充</h4>]]></content>
      <categories>
        <category>PyTorch框架学习</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch-DataProcess&amp;Dataset</title>
    <url>/2021/09/26/PyTorch-DataProcess-Dataset/</url>
    <content><![CDATA[<h2 id="二-PyTorch数据处理（Dataset）"><a href="#二-PyTorch数据处理（Dataset）" class="headerlink" title="二. PyTorch数据处理（Dataset）"></a>二. PyTorch数据处理（Dataset）</h2><h3 id="A-数据读取机制Dataloader与Dataset"><a href="#A-数据读取机制Dataloader与Dataset" class="headerlink" title="A. 数据读取机制Dataloader与Dataset"></a>A. 数据读取机制Dataloader与Dataset</h3><p><strong>数据</strong>：</p>
<ul>
<li><strong>数据收集</strong>：Img &amp; Label</li>
<li><strong>数据划分</strong>：train valid test</li>
<li><strong>数据读取</strong>：Dataloader（Sampler生成索引 / Dataset根据索引读取数据）</li>
<li><strong>数据预处理</strong>：transform</li>
</ul>
<p><strong>Epoch</strong>: 所有训练样本都输入到模型当中一次，称为一个epoch</p>
<p><strong>Iteration</strong>：一批样本输入到模型中</p>
<p><strong>Batchsize</strong>：批大小，决定了一个Epoch进行多少次Iteration</p>
<p><strong>例</strong>：</p>
<ul>
<li><p>样本总数：80，Batchsize：8，1 Epoch = 10 Iteration</p>
</li>
<li><p>样本总数：87，Batchsize：8</p>
<p>drop_last = True/False，1 Epoch = 10 Iteration/11 Iteration</p>
</li>
</ul>
<p>数据读取机制：</p>
<ul>
<li>读哪些数据：在每一个iteration时，读一个batchsize大小，该读取哪几个数据。一般是sampler输出的index。</li>
<li>从哪读数据：在硬盘中从哪找数据。dataset中的data_dir。</li>
<li>怎么读数据： Dataset中的getitem</li>
</ul>
<p>训练时：</p>
<p>以Epoch为主循环，其中嵌入每次Iteration的循环。</p>
<h4 id="1-Dataloader和Dataset"><a href="#1-Dataloader和Dataset" class="headerlink" title="1. Dataloader和Dataset"></a>1. Dataloader和Dataset</h4><h5 id="torch-utils-data-Dataloader"><a href="#torch-utils-data-Dataloader" class="headerlink" title="torch.utils.data.Dataloader"></a>torch.utils.data.Dataloader</h5><p>构建可迭代的数据装载器</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922133310174.png" alt="image-20210922133310174"></p>
<ul>
<li>dataset：Dataset类，决定数据从哪读取及如何读取</li>
<li>batchsize：批大小</li>
<li>num_works：是否多进程读取数据</li>
<li>shuffle：每个epoch是否乱序</li>
<li>drop_last：当样本数不能被batchsize整除时，是否舍弃最后一批数据</li>
</ul>
<h5 id="torch-utils-data-Dataset"><a href="#torch-utils-data-Dataset" class="headerlink" title="torch.utils.data.Dataset"></a>torch.utils.data.Dataset</h5><p>Dataset抽象类，所有自定义的Dataset需要继承它，并且复写__ getitem __( )</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922155733171.png" alt="image-20210922155733171"></p>
<p>getitem: 接受一个索引，返回一个样本。</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922170343251.png" alt="image-20210922170343251"></p>
<h3 id="B-数据预处理模块"><a href="#B-数据预处理模块" class="headerlink" title="B. 数据预处理模块"></a>B. 数据预处理模块</h3><p><strong>torchvision</strong>简介：</p>
<p>计算机视觉工具包</p>
<ul>
<li>torchvision.transforms：常用的图像预处理方法</li>
<li>torchvision.dataset：常用数据集的dataset实现，MNIST，CIFAR-10，ImageNet</li>
<li>torchvision.model：常用的模型预训练，AlexNet，VGG，ResNet，GoogleLeNet</li>
</ul>
<h4 id="1-transform运行机制"><a href="#1-transform运行机制" class="headerlink" title="1.transform运行机制"></a>1.transform运行机制</h4><p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922171520294.png" alt="image-20210922171520294"></p>
<h5 id="transforms-Compose"><a href="#transforms-Compose" class="headerlink" title="transforms.Compose"></a>transforms.Compose</h5><p>将一系列方法进行有序的包装，之后安装顺序对数据进行处理。</p>
<h5 id="transforms-Normalize"><a href="#transforms-Normalize" class="headerlink" title="transforms.Normalize"></a>transforms.Normalize</h5><p>逐channel的对图像进行标准化</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922172839907.png" alt="image-20210922172839907"></p>
<p>output = (input - mean) / std</p>
<ul>
<li>mean：各通道的均值</li>
<li>std：各通道的标准差</li>
<li>inplace：是否原地操作</li>
</ul>
<h4 id="2-数据标准化"><a href="#2-数据标准化" class="headerlink" title="2.数据标准化"></a>2.数据标准化</h4><h3 id="C-数据增强"><a href="#C-数据增强" class="headerlink" title="C. 数据增强"></a>C. 数据增强</h3><h4 id="1-transforms——剪裁"><a href="#1-transforms——剪裁" class="headerlink" title="1. transforms——剪裁"></a>1. transforms——剪裁</h4><h5 id="transforms-CenterCrop"><a href="#transforms-CenterCrop" class="headerlink" title="transforms.CenterCrop"></a>transforms.CenterCrop</h5><p>从图像中心裁剪图片</p>
<ul>
<li>size：所需裁剪图片尺寸</li>
</ul>
<h5 id="transforms-RandomCrop"><a href="#transforms-RandomCrop" class="headerlink" title="transforms.RandomCrop"></a>transforms.RandomCrop</h5><p>从图片中随机裁剪尺寸为size的图片</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922210309872.png" alt="image-20210922210309872"></p>
<ul>
<li>size：所需裁剪图片尺寸</li>
<li>padding：设置填充大小。当为a时，上下左右均填充a个像素；当为（a，b）时，上下填充b个像素，左右填充a个。当为（a，b，c，d）时，左，上，右，下分别填充a，b，c，d个像素。</li>
<li>pad_is_need：若图像小于设定的size，则填充</li>
<li>padding_mode：四种填充模式<ol>
<li>constant：像素值由fill设定</li>
<li>edge：像素值由图像边缘像素决定</li>
<li>reflect：镜像填充，最后一个像素不镜像[1, 2, 3, 4]——&gt; [3,2,1,2,3,4,3,2]</li>
<li>symmetric：镜像填充，最后一个像素镜像[1, 2, 3, 4]——&gt;[2,1,1,2,3,4,4,3]</li>
</ol>
</li>
<li>fill：constant时，设置填充的像素值</li>
</ul>
<h5 id="transforms-RandomResizeCrop"><a href="#transforms-RandomResizeCrop" class="headerlink" title="transforms.RandomResizeCrop"></a>transforms.RandomResizeCrop</h5><p>随机大小，长宽比裁剪图片</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922211513888.png" alt="image-20210922211513888"></p>
<ul>
<li><p>size：所需裁剪图片尺寸</p>
</li>
<li><p>scale：随即裁剪面积比例，默认（0.08，1）</p>
</li>
<li><p>ratio：随机长宽比，默认（3/4, 4/3）</p>
</li>
<li><p>interpolation：插值方法 </p>
<p>PIL.Image.NEAREST</p>
<p>PIL.Image.BILINEAR</p>
<p>PIL.Image.BICUBIC</p>
</li>
</ul>
<h5 id="FiveCrop-TenCrop"><a href="#FiveCrop-TenCrop" class="headerlink" title="FiveCrop/TenCrop"></a>FiveCrop/TenCrop</h5><p>在图片的上下左右以及中心裁剪出尺寸为size的5张图片，TenCrop对这5张图片进行水平或者垂直镜像获得10张图片</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922212510007.png" alt="image-20210922212510007"></p>
<p>size：所需裁剪图片的尺寸</p>
<p>vertical_flip：是否垂直翻转</p>
<h4 id="2-transforms——翻转、旋转"><a href="#2-transforms——翻转、旋转" class="headerlink" title="2. transforms——翻转、旋转"></a>2. transforms——翻转、旋转</h4><h5 id="transforms-RandomHorizontalFlip-VerticalFlip"><a href="#transforms-RandomHorizontalFlip-VerticalFlip" class="headerlink" title="transforms.RandomHorizontalFlip/VerticalFlip"></a>transforms.RandomHorizontalFlip/VerticalFlip</h5><p>依概率水平（左右）或垂直（上下）翻转图片</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922213455329.png" alt="image-20210922213455329"></p>
<ul>
<li>p：翻转概率</li>
</ul>
<h5 id="transforms-RandomRotation"><a href="#transforms-RandomRotation" class="headerlink" title="transforms.RandomRotation"></a>transforms.RandomRotation</h5><p>随机旋转图片</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922213841911.png" alt="image-20210922213841911"></p>
<ul>
<li>degrees：旋转角度。当为a时，在（-a，a）之间选择旋转角度；当为（a，b）时，在（a，b）之间选择旋转角度</li>
<li>resample：重采样方法</li>
<li>expand：是否扩大图片，以保持原图信息</li>
</ul>
<h4 id="3-transfroms——图像变换"><a href="#3-transfroms——图像变换" class="headerlink" title="3. transfroms——图像变换"></a>3. transfroms——图像变换</h4><h5 id="transforms-Pad"><a href="#transforms-Pad" class="headerlink" title="transforms.Pad"></a>transforms.Pad</h5><p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922215241192.png" alt="image-20210922215241192"></p>
<ul>
<li><p>padding：设置填充大小。当为a时，上下左右均填充a个像素；当为（a，b）时，上下填充b个像素，左右填充a个。当为（a，b，c，d）时，左，上，右，下分别填充a，b，c，d个像素。</p>
</li>
<li><p>padding_mode：四种填充模式</p>
<ol>
<li>constant：像素值由fill设定</li>
<li>edge：像素值由图像边缘像素决定</li>
<li>reflect：镜像填充，最后一个像素不镜像[1, 2, 3, 4]——&gt; [3,2,1,2,3,4,3,2]</li>
<li>symmetric：镜像填充，最后一个像素镜像[1, 2, 3, 4]——&gt;[2,1,1,2,3,4,4,3]</li>
</ol>
</li>
<li><p>fill：constant时，设置填充的像素值</p>
</li>
</ul>
<h5 id="transforms-ColorJitter"><a href="#transforms-ColorJitter" class="headerlink" title="transforms.ColorJitter"></a>transforms.ColorJitter</h5><p>调整亮度，对比度，饱和度和色相</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922215447677.png" alt="image-20210922215447677"></p>
<ul>
<li>brightness：亮度调整参数。当为a时，从[max(0, 1-a), 1+a]中随机选择，当为（a，b）时，从[a, b]中选择</li>
<li>contrast：对比度参数，同上</li>
<li>saturation：饱和度参数，同上</li>
<li>hue：色相参数，当为a时，从[-a，a]中选择参数（0&lt;= a &lt;=0.5）当为（a，b）时，从[a, b]中选择参数，注：-0.5 &lt;= a &lt;= b &lt;= 0.5</li>
</ul>
<h5 id="transforms-Grayscale-RandomGrayscale"><a href="#transforms-Grayscale-RandomGrayscale" class="headerlink" title="transforms.Grayscale/RandomGrayscale"></a>transforms.Grayscale/RandomGrayscale</h5><p>依据概率将图片转换为灰度图</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922220031371.png" alt="image-20210922220031371"></p>
<ul>
<li>num_output_channels：输出通道数只能设置为1或3</li>
<li>p：概率值，图像被转换为灰度图的概率</li>
</ul>
<h5 id="transforms-RandomAffine"><a href="#transforms-RandomAffine" class="headerlink" title="transforms.RandomAffine"></a>transforms.RandomAffine</h5><p>对图像进行仿射变换，仿射变换是二维的线性变换，由五种基本原子变换构成，分别是<strong>旋转</strong>、<strong>平移</strong>、<strong>缩放</strong>、<strong>错切</strong>和<strong>翻转</strong>。</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922220406992.png" alt="image-20210922220406992"></p>
<ul>
<li>degrees：旋转角度设置</li>
<li>translate：平移区间设置，如（a，b），a设置宽（width），b设置高（height）。图像在宽维度平移的区间为-img_width* a &lt; dx &lt;img_width * a</li>
<li>scale：缩放比例（以面积为单位）</li>
<li>fill_color：填充颜色设置</li>
<li>shear：错切角度设置，有水平错切和垂直错切。若为a，则仅在x轴错切，错切角度在（-a，a）之间；若为（a，b），则a设置x轴角度，b设置y的角度。若为（a，b，c，d），则a，b设置x轴角度，c，d设置y轴角度</li>
<li>resample：重采样方式，用NEAREST，BILINEAR，BICUBIC</li>
</ul>
<h5 id="transforms-RandomErasing"><a href="#transforms-RandomErasing" class="headerlink" title="transforms.RandomErasing"></a>transforms.RandomErasing</h5><p>对图像进行随机遮挡</p>
<ul>
<li>p：概率值，执行该操作的概率</li>
<li>scale：遮挡区域的面积</li>
<li>ratio：遮挡区域长宽比</li>
<li>value：设置遮挡区域的像素值（R，G，B）or（Gray）,或者任意字符串（则填入随机像素值，还挺好看）</li>
</ul>
<h5 id="transforms-Lambda"><a href="#transforms-Lambda" class="headerlink" title="transforms.Lambda"></a>transforms.Lambda</h5><p>用户自定义lambda方法</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922221913501.png" alt="image-20210922221913501"></p>
<ul>
<li><p>lambda：lambda匿名函数</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922222014416.png" alt="image-20210922222014416"></p>
</li>
</ul>
<h4 id="4-transforms选择操作"><a href="#4-transforms选择操作" class="headerlink" title="4. transforms选择操作"></a>4. transforms选择操作</h4><h5 id="transforms-RandomChoice"><a href="#transforms-RandomChoice" class="headerlink" title="transforms.RandomChoice"></a>transforms.RandomChoice</h5><p>一组transforms方法中每次随机挑选一个</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922222201258.png" alt="image-20210922222201258"></p>
<h5 id="transforms-RandomApply"><a href="#transforms-RandomApply" class="headerlink" title="transforms.RandomApply"></a>transforms.RandomApply</h5><p>依据概率执行一组transforms操作</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922222233010.png" alt="image-20210922222233010"></p>
<h5 id="transforms-RandomOrder"><a href="#transforms-RandomOrder" class="headerlink" title="transforms.RandomOrder"></a>transforms.RandomOrder</h5><p>对一组transforms操作打乱顺序</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922222310180.png" alt="image-20210922222310180"></p>
<h4 id="5-自定义transforms"><a href="#5-自定义transforms" class="headerlink" title="5. 自定义transforms"></a>5. 自定义transforms</h4><p>自定义transforms要素：</p>
<ul>
<li>仅接受一个参数，返回一个参数</li>
<li>注意上游和下游的操作</li>
</ul>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922222513169.png" alt="image-20210922222513169"></p>
<h5 id="椒盐噪声"><a href="#椒盐噪声" class="headerlink" title="椒盐噪声"></a>椒盐噪声</h5><p>椒盐噪声又称脉冲噪声，是一种随机出现的白点或者黑点，白点称为盐噪声，黑点称为椒噪声</p>
<p><strong>信噪比（Signal-Noise Rate，SNR）</strong>：衡量噪声的比例。</p>
<h4 id="6-数据增强思想"><a href="#6-数据增强思想" class="headerlink" title="6. 数据增强思想"></a>6. 数据增强思想</h4><p>原则：让训练集与测试集更接近</p>
<ul>
<li><strong>空间位置</strong>：平移</li>
<li><strong>色彩</strong>：灰度图，色彩抖动</li>
<li><strong>形状</strong>：仿射变换</li>
<li><strong>上下文场景</strong>：遮挡，填充</li>
</ul>
]]></content>
      <categories>
        <category>PyTorch框架学习</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>用随机梯度下降来优化人生</title>
    <url>/2021/09/27/limu-SGD/</url>
    <content><![CDATA[<p>作者：李沐 | 来源：知乎</p>
<p><strong>要有目标</strong>。你需要有目标。短的也好，长的也好。认真定下的也好，别人那里捡的也好。就跟随机梯度下降需要有个目标函数一样。</p>
<p><strong>目标要大</strong>。不管是人生目标还是目标函数，你最好不要知道最后可以走到哪里。如果你知道，那么你的目标就太简单了，可能是个凸函数。你可以在一开始的时候给自己一些小目标，例如期末考个80分，训练一个线性模型。但接下来得有更大的目标，财富自由也好，100亿参数的变形金刚也好，得足够一颗赛艇。</p>
<p><strong>坚持走</strong>。不管你的目标多复杂，随机梯度下降都是最简单的。每一次你找一个大概还行的方向（梯度），然后迈一步（下降）。两个核心要素是方向和步子的长短。但最重要的是你得一直走下去，能多走几步就多走几步。</p>
<p><strong>痛苦的卷</strong>。每一步里你都在试图改变你自己或者你的模型参数。改变带来痛苦。但没有改变就没有进步。你过得很痛苦不代表在朝着目标走，因为你可能走反了。但过得很舒服那一定在原地踏步。需要时刻跟自己作对。</p>
<p><strong>可以躺平</strong>。你用你内心的激情来迈步子。步子太小走不动，步子太长容易过早消耗掉了激情。周期性的调大调小步长效果挺好。所以你可以时不时休息休息。</p>
<p><strong>四处看看</strong>。每一步走的方向是你对世界的认识。如果你探索的世界不怎么变化，那么要么你的目标太简单，要么你困在你的舒适区了。随机梯度下降的第一个词是随机，就是你需要四处走走，看过很多地方，做些错误的决定，这样你可以在前期迈过一些不是很好的舒适区。</p>
<p><strong>快也是慢</strong>。你没有必要特意去追求找到最好的方向和最合适的步子。你身边当然会有幸运之子，他们每一步都在别人前面。但经验告诉我们，随机梯度下降前期进度太快，后期可能乏力。就是说你过早的找到一个舒适区，忘了世界有多大。所以你不要急，前面徘徊一段时间不是坏事。成名无需太早。</p>
<p><strong>赢在起点</strong>。起点当然重要。如果你在终点附近起步，可以少走很多路。而且终点附近的路都比较平，走着舒服。当你发现别人不如你的时候，看看自己站在哪里。可能你就是运气很好，赢在了起跑线。如果你跟别人在同一起跑线，不见得你能做更好。</p>
<p><strong>很远也能到达</strong>。如果你是在随机起点，那么做好准备前面的路会非常不平坦。越远离终点，越人迹罕见。四处都是悬崖。但随机梯度下降告诉我们，不管起点在哪里，最后得到的解都差不多。当然这个前提是你得一直按照梯度的方向走下去。如果中间梯度炸掉了，那么你随机一个起点，调整步子节奏，重新来。</p>
<p><strong>独一无二</strong>。也许大家有着差不多的目标，在差不多的时间毕业买房结婚生娃。但每一步里，每个人内心中看到的世界都不一样，导致走的路不一样。你如果跑多次随机梯度下降，在各个时间点的目标函数值可能都差不多，但每次的参数千差万别。不会有人关心你每次训练出来的模型里面参数具体是什么值，除了你自己。</p>
<p><strong>简单最好</strong> 。当然有比随机梯度下降更复杂的算法。他们想每一步看想更远更准，想步子迈最大。但如果你的目标很复杂，简单的随机梯度下降反而效果最好。深度学习里大家都用它。关注当前，每次抬头瞄一眼世界，快速做个决定，然后迈一小步。小步快跑。只要你有目标，不要停，就能到达。</p>
]]></content>
      <categories>
        <category>杂文</category>
      </categories>
      <tags>
        <tag>杂文</tag>
      </tags>
  </entry>
</search>
