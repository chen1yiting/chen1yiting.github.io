<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>PyTorch-DataProcess&amp;Dataset</title>
    <url>/2021/09/26/PyTorch-DataProcess-Dataset/</url>
    <content><![CDATA[<h2 id="二-PyTorch数据处理（Dataset）"><a href="#二-PyTorch数据处理（Dataset）" class="headerlink" title="二. PyTorch数据处理（Dataset）"></a>二. PyTorch数据处理（Dataset）</h2><h3 id="A-数据读取机制Dataloader与Dataset"><a href="#A-数据读取机制Dataloader与Dataset" class="headerlink" title="A. 数据读取机制Dataloader与Dataset"></a>A. 数据读取机制Dataloader与Dataset</h3><p><strong>数据</strong>：</p>
<ul>
<li><strong>数据收集</strong>：Img &amp; Label</li>
<li><strong>数据划分</strong>：train valid test</li>
<li><strong>数据读取</strong>：Dataloader（Sampler生成索引 / Dataset根据索引读取数据）</li>
<li><strong>数据预处理</strong>：transform</li>
</ul>
<p><strong>Epoch</strong>: 所有训练样本都输入到模型当中一次，称为一个epoch</p>
<p><strong>Iteration</strong>：一批样本输入到模型中</p>
<p><strong>Batchsize</strong>：批大小，决定了一个Epoch进行多少次Iteration</p>
<p><strong>例</strong>：</p>
<ul>
<li><p>样本总数：80，Batchsize：8，1 Epoch = 10 Iteration</p>
</li>
<li><p>样本总数：87，Batchsize：8</p>
<p>drop_last = True/False，1 Epoch = 10 Iteration/11 Iteration</p>
</li>
</ul>
<p>数据读取机制：</p>
<ul>
<li>读哪些数据：在每一个iteration时，读一个batchsize大小，该读取哪几个数据。一般是sampler输出的index。</li>
<li>从哪读数据：在硬盘中从哪找数据。dataset中的data_dir。</li>
<li>怎么读数据： Dataset中的getitem</li>
</ul>
<p>训练时：</p>
<p>以Epoch为主循环，其中嵌入每次Iteration的循环。</p>
<h4 id="1-Dataloader和Dataset"><a href="#1-Dataloader和Dataset" class="headerlink" title="1. Dataloader和Dataset"></a>1. Dataloader和Dataset</h4><h5 id="torch-utils-data-Dataloader"><a href="#torch-utils-data-Dataloader" class="headerlink" title="torch.utils.data.Dataloader"></a>torch.utils.data.Dataloader</h5><p>构建可迭代的数据装载器</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922133310174.png" alt="image-20210922133310174"></p>
<ul>
<li>dataset：Dataset类，决定数据从哪读取及如何读取</li>
<li>batchsize：批大小</li>
<li>num_works：是否多进程读取数据</li>
<li>shuffle：每个epoch是否乱序</li>
<li>drop_last：当样本数不能被batchsize整除时，是否舍弃最后一批数据</li>
</ul>
<h5 id="torch-utils-data-Dataset"><a href="#torch-utils-data-Dataset" class="headerlink" title="torch.utils.data.Dataset"></a>torch.utils.data.Dataset</h5><p>Dataset抽象类，所有自定义的Dataset需要继承它，并且复写__ getitem __( )</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922155733171.png" alt="image-20210922155733171"></p>
<p>getitem: 接受一个索引，返回一个样本。</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922170343251.png" alt="image-20210922170343251"></p>
<h3 id="B-数据预处理模块"><a href="#B-数据预处理模块" class="headerlink" title="B. 数据预处理模块"></a>B. 数据预处理模块</h3><p><strong>torchvision</strong>简介：</p>
<p>计算机视觉工具包</p>
<ul>
<li>torchvision.transforms：常用的图像预处理方法</li>
<li>torchvision.dataset：常用数据集的dataset实现，MNIST，CIFAR-10，ImageNet</li>
<li>torchvision.model：常用的模型预训练，AlexNet，VGG，ResNet，GoogleLeNet</li>
</ul>
<h4 id="1-transform运行机制"><a href="#1-transform运行机制" class="headerlink" title="1.transform运行机制"></a>1.transform运行机制</h4><p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922171520294.png" alt="image-20210922171520294"></p>
<h5 id="transforms-Compose"><a href="#transforms-Compose" class="headerlink" title="transforms.Compose"></a>transforms.Compose</h5><p>将一系列方法进行有序的包装，之后安装顺序对数据进行处理。</p>
<h5 id="transforms-Normalize"><a href="#transforms-Normalize" class="headerlink" title="transforms.Normalize"></a>transforms.Normalize</h5><p>逐channel的对图像进行标准化</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922172839907.png" alt="image-20210922172839907"></p>
<p>output = (input - mean) / std</p>
<ul>
<li>mean：各通道的均值</li>
<li>std：各通道的标准差</li>
<li>inplace：是否原地操作</li>
</ul>
<h4 id="2-数据标准化"><a href="#2-数据标准化" class="headerlink" title="2.数据标准化"></a>2.数据标准化</h4><h3 id="C-数据增强"><a href="#C-数据增强" class="headerlink" title="C. 数据增强"></a>C. 数据增强</h3><h4 id="1-transforms——剪裁"><a href="#1-transforms——剪裁" class="headerlink" title="1. transforms——剪裁"></a>1. transforms——剪裁</h4><h5 id="transforms-CenterCrop"><a href="#transforms-CenterCrop" class="headerlink" title="transforms.CenterCrop"></a>transforms.CenterCrop</h5><p>从图像中心裁剪图片</p>
<ul>
<li>size：所需裁剪图片尺寸</li>
</ul>
<h5 id="transforms-RandomCrop"><a href="#transforms-RandomCrop" class="headerlink" title="transforms.RandomCrop"></a>transforms.RandomCrop</h5><p>从图片中随机裁剪尺寸为size的图片</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922210309872.png" alt="image-20210922210309872"></p>
<ul>
<li>size：所需裁剪图片尺寸</li>
<li>padding：设置填充大小。当为a时，上下左右均填充a个像素；当为（a，b）时，上下填充b个像素，左右填充a个。当为（a，b，c，d）时，左，上，右，下分别填充a，b，c，d个像素。</li>
<li>pad_is_need：若图像小于设定的size，则填充</li>
<li>padding_mode：四种填充模式<ol>
<li>constant：像素值由fill设定</li>
<li>edge：像素值由图像边缘像素决定</li>
<li>reflect：镜像填充，最后一个像素不镜像[1, 2, 3, 4]——&gt; [3,2,1,2,3,4,3,2]</li>
<li>symmetric：镜像填充，最后一个像素镜像[1, 2, 3, 4]——&gt;[2,1,1,2,3,4,4,3]</li>
</ol>
</li>
<li>fill：constant时，设置填充的像素值</li>
</ul>
<h5 id="transforms-RandomResizeCrop"><a href="#transforms-RandomResizeCrop" class="headerlink" title="transforms.RandomResizeCrop"></a>transforms.RandomResizeCrop</h5><p>随机大小，长宽比裁剪图片</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922211513888.png" alt="image-20210922211513888"></p>
<ul>
<li><p>size：所需裁剪图片尺寸</p>
</li>
<li><p>scale：随即裁剪面积比例，默认（0.08，1）</p>
</li>
<li><p>ratio：随机长宽比，默认（3/4, 4/3）</p>
</li>
<li><p>interpolation：插值方法 </p>
<p>PIL.Image.NEAREST</p>
<p>PIL.Image.BILINEAR</p>
<p>PIL.Image.BICUBIC</p>
</li>
</ul>
<h5 id="FiveCrop-TenCrop"><a href="#FiveCrop-TenCrop" class="headerlink" title="FiveCrop/TenCrop"></a>FiveCrop/TenCrop</h5><p>在图片的上下左右以及中心裁剪出尺寸为size的5张图片，TenCrop对这5张图片进行水平或者垂直镜像获得10张图片</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922212510007.png" alt="image-20210922212510007"></p>
<p>size：所需裁剪图片的尺寸</p>
<p>vertical_flip：是否垂直翻转</p>
<h4 id="2-transforms——翻转、旋转"><a href="#2-transforms——翻转、旋转" class="headerlink" title="2. transforms——翻转、旋转"></a>2. transforms——翻转、旋转</h4><h5 id="transforms-RandomHorizontalFlip-VerticalFlip"><a href="#transforms-RandomHorizontalFlip-VerticalFlip" class="headerlink" title="transforms.RandomHorizontalFlip/VerticalFlip"></a>transforms.RandomHorizontalFlip/VerticalFlip</h5><p>依概率水平（左右）或垂直（上下）翻转图片</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922213455329.png" alt="image-20210922213455329"></p>
<ul>
<li>p：翻转概率</li>
</ul>
<h5 id="transforms-RandomRotation"><a href="#transforms-RandomRotation" class="headerlink" title="transforms.RandomRotation"></a>transforms.RandomRotation</h5><p>随机旋转图片</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922213841911.png" alt="image-20210922213841911"></p>
<ul>
<li>degrees：旋转角度。当为a时，在（-a，a）之间选择旋转角度；当为（a，b）时，在（a，b）之间选择旋转角度</li>
<li>resample：重采样方法</li>
<li>expand：是否扩大图片，以保持原图信息</li>
</ul>
<h4 id="3-transfroms——图像变换"><a href="#3-transfroms——图像变换" class="headerlink" title="3. transfroms——图像变换"></a>3. transfroms——图像变换</h4><h5 id="transforms-Pad"><a href="#transforms-Pad" class="headerlink" title="transforms.Pad"></a>transforms.Pad</h5><p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922215241192.png" alt="image-20210922215241192"></p>
<ul>
<li><p>padding：设置填充大小。当为a时，上下左右均填充a个像素；当为（a，b）时，上下填充b个像素，左右填充a个。当为（a，b，c，d）时，左，上，右，下分别填充a，b，c，d个像素。</p>
</li>
<li><p>padding_mode：四种填充模式</p>
<ol>
<li>constant：像素值由fill设定</li>
<li>edge：像素值由图像边缘像素决定</li>
<li>reflect：镜像填充，最后一个像素不镜像[1, 2, 3, 4]——&gt; [3,2,1,2,3,4,3,2]</li>
<li>symmetric：镜像填充，最后一个像素镜像[1, 2, 3, 4]——&gt;[2,1,1,2,3,4,4,3]</li>
</ol>
</li>
<li><p>fill：constant时，设置填充的像素值</p>
</li>
</ul>
<h5 id="transforms-ColorJitter"><a href="#transforms-ColorJitter" class="headerlink" title="transforms.ColorJitter"></a>transforms.ColorJitter</h5><p>调整亮度，对比度，饱和度和色相</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922215447677.png" alt="image-20210922215447677"></p>
<ul>
<li>brightness：亮度调整参数。当为a时，从[max(0, 1-a), 1+a]中随机选择，当为（a，b）时，从[a, b]中选择</li>
<li>contrast：对比度参数，同上</li>
<li>saturation：饱和度参数，同上</li>
<li>hue：色相参数，当为a时，从[-a，a]中选择参数（0&lt;= a &lt;=0.5）当为（a，b）时，从[a, b]中选择参数，注：-0.5 &lt;= a &lt;= b &lt;= 0.5</li>
</ul>
<h5 id="transforms-Grayscale-RandomGrayscale"><a href="#transforms-Grayscale-RandomGrayscale" class="headerlink" title="transforms.Grayscale/RandomGrayscale"></a>transforms.Grayscale/RandomGrayscale</h5><p>依据概率将图片转换为灰度图</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922220031371.png" alt="image-20210922220031371"></p>
<ul>
<li>num_output_channels：输出通道数只能设置为1或3</li>
<li>p：概率值，图像被转换为灰度图的概率</li>
</ul>
<h5 id="transforms-RandomAffine"><a href="#transforms-RandomAffine" class="headerlink" title="transforms.RandomAffine"></a>transforms.RandomAffine</h5><p>对图像进行仿射变换，仿射变换是二维的线性变换，由五种基本原子变换构成，分别是<strong>旋转</strong>、<strong>平移</strong>、<strong>缩放</strong>、<strong>错切</strong>和<strong>翻转</strong>。</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922220406992.png" alt="image-20210922220406992"></p>
<ul>
<li>degrees：旋转角度设置</li>
<li>translate：平移区间设置，如（a，b），a设置宽（width），b设置高（height）。图像在宽维度平移的区间为-img_width* a &lt; dx &lt;img_width * a</li>
<li>scale：缩放比例（以面积为单位）</li>
<li>fill_color：填充颜色设置</li>
<li>shear：错切角度设置，有水平错切和垂直错切。若为a，则仅在x轴错切，错切角度在（-a，a）之间；若为（a，b），则a设置x轴角度，b设置y的角度。若为（a，b，c，d），则a，b设置x轴角度，c，d设置y轴角度</li>
<li>resample：重采样方式，用NEAREST，BILINEAR，BICUBIC</li>
</ul>
<h5 id="transforms-RandomErasing"><a href="#transforms-RandomErasing" class="headerlink" title="transforms.RandomErasing"></a>transforms.RandomErasing</h5><p>对图像进行随机遮挡</p>
<ul>
<li>p：概率值，执行该操作的概率</li>
<li>scale：遮挡区域的面积</li>
<li>ratio：遮挡区域长宽比</li>
<li>value：设置遮挡区域的像素值（R，G，B）or（Gray）,或者任意字符串（则填入随机像素值，还挺好看）</li>
</ul>
<h5 id="transforms-Lambda"><a href="#transforms-Lambda" class="headerlink" title="transforms.Lambda"></a>transforms.Lambda</h5><p>用户自定义lambda方法</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922221913501.png" alt="image-20210922221913501"></p>
<ul>
<li><p>lambda：lambda匿名函数</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922222014416.png" alt="image-20210922222014416"></p>
</li>
</ul>
<h4 id="4-transforms选择操作"><a href="#4-transforms选择操作" class="headerlink" title="4. transforms选择操作"></a>4. transforms选择操作</h4><h5 id="transforms-RandomChoice"><a href="#transforms-RandomChoice" class="headerlink" title="transforms.RandomChoice"></a>transforms.RandomChoice</h5><p>一组transforms方法中每次随机挑选一个</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922222201258.png" alt="image-20210922222201258"></p>
<h5 id="transforms-RandomApply"><a href="#transforms-RandomApply" class="headerlink" title="transforms.RandomApply"></a>transforms.RandomApply</h5><p>依据概率执行一组transforms操作</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922222233010.png" alt="image-20210922222233010"></p>
<h5 id="transforms-RandomOrder"><a href="#transforms-RandomOrder" class="headerlink" title="transforms.RandomOrder"></a>transforms.RandomOrder</h5><p>对一组transforms操作打乱顺序</p>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922222310180.png" alt="image-20210922222310180"></p>
<h4 id="5-自定义transforms"><a href="#5-自定义transforms" class="headerlink" title="5. 自定义transforms"></a>5. 自定义transforms</h4><p>自定义transforms要素：</p>
<ul>
<li>仅接受一个参数，返回一个参数</li>
<li>注意上游和下游的操作</li>
</ul>
<p><img src="/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922222513169.png" alt="image-20210922222513169"></p>
<h5 id="椒盐噪声"><a href="#椒盐噪声" class="headerlink" title="椒盐噪声"></a>椒盐噪声</h5><p>椒盐噪声又称脉冲噪声，是一种随机出现的白点或者黑点，白点称为盐噪声，黑点称为椒噪声</p>
<p><strong>信噪比（Signal-Noise Rate，SNR）</strong>：衡量噪声的比例。</p>
<h4 id="6-数据增强思想"><a href="#6-数据增强思想" class="headerlink" title="6. 数据增强思想"></a>6. 数据增强思想</h4><p>原则：让训练集与测试集更接近</p>
<ul>
<li><strong>空间位置</strong>：平移</li>
<li><strong>色彩</strong>：灰度图，色彩抖动</li>
<li><strong>形状</strong>：仿射变换</li>
<li><strong>上下文场景</strong>：遮挡，填充</li>
</ul>
]]></content>
      <categories>
        <category>PyTorch框架学习</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch-EnvSetup&amp;BasicConcept</title>
    <url>/2021/09/26/PyTorch-EnvSetup-BasicConcept/</url>
    <content><![CDATA[<h2 id="一-PyTorch基础概念"><a href="#一-PyTorch基础概念" class="headerlink" title="一. PyTorch基础概念"></a>一. PyTorch基础概念</h2><h3 id="A-环境搭建"><a href="#A-环境搭建" class="headerlink" title="A. 环境搭建"></a>A. 环境搭建</h3><h4 id="1-系统显卡驱动，CUDA和cudnn配置"><a href="#1-系统显卡驱动，CUDA和cudnn配置" class="headerlink" title="1. 系统显卡驱动，CUDA和cudnn配置"></a>1. 系统显卡驱动，CUDA和cudnn配置</h4><p>本机配置：Win10，Geforce 3070 140w 8GB显存，Intel 11th i7 八核十六线程</p>
<p>当前Nvidia显卡驱动版本462.62，最高支持cuda版本11.2（安装cuda版本11.1）</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921151203907.png" alt="image-20210921151203907"></p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921151508628.png" alt="image-20210921151508628"></p>
<p>本机目前安装CUDA版本11.1，可通过nvcc -V查看</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921152447031.png" alt="image-20210921152447031"></p>
<p>对应cudnn版本为8.1.1</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921155417638.png" alt="image-20210921155417638"></p>
<p>通过运行CUDA自带的deviceQuery.exe可知，</p>
<p>当前cuda driver version为11.2，当前cuda runtime version为11.1。两者允许不一致，但是runtime version必须 &lt;= driver version</p>
<h4 id="2-PyTorch安装"><a href="#2-PyTorch安装" class="headerlink" title="2. PyTorch安装"></a>2. PyTorch安装</h4><p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921155822943.png" alt="image-20210921155822943"></p>
<p>CUDA版本应该与runtime version相对应。</p>
<p>可以下载对应的torch，torchvision和torchaudio手动安装：</p>
<p><a href="https://download.pytorch.org/whl/torch_stable.html">https://download.pytorch.org/whl/torch_stable.html</a></p>
<p>直接到conda 环境下运行辣个command/或者pip install 各个.whl文件。</p>
<h3 id="B-张量简介与创建"><a href="#B-张量简介与创建" class="headerlink" title="B. 张量简介与创建"></a>B. 张量简介与创建</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>张量<strong>tensor</strong>就是<strong>多维数组</strong>，是标量、向量、矩阵的高维拓展。具备以下属性。</p>
<ul>
<li>卷积之类的处理之后，数据类型默认为32-bit floating point, torch.float/float32 ；</li>
<li>图像的标签则默认为64-bit integer (signed) torch.int/torch.int64</li>
</ul>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921163051863.png" alt="image-20210921163051863"></p>
<h4 id="1-直接创建"><a href="#1-直接创建" class="headerlink" title="1. 直接创建"></a>1. 直接创建</h4><h5 id="torch-tensor"><a href="#torch-tensor" class="headerlink" title="torch.tensor()"></a>torch.tensor()</h5><p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921163600998.png" alt="image-20210921163600998"></p>
<ul>
<li>data：数据，可以是list，numpy</li>
<li>dtype：数据类型，默认与data的一致</li>
<li>device：所在设备，cuda/cpu</li>
<li>requires_grad：是否需要梯度</li>
<li>pin_memory：是否存于锁页内存（目前不懂作用，默认为false即可）</li>
</ul>
<h5 id="torch-from-numpy"><a href="#torch-from-numpy" class="headerlink" title="torch.from_numpy()"></a>torch.from_numpy()</h5><p>从ndarray创建</p>
<p><em><strong><u><code>torch.from_numpy(ndarray)</code></u></strong></em></p>
<p>从numpy创建tensor，<strong>注意：两者之间共享内存，改动其中一个另一个也会被更改。</strong></p>
<h4 id="2-依据数值创建"><a href="#2-依据数值创建" class="headerlink" title="2. 依据数值创建"></a>2. 依据数值创建</h4><h5 id="torch-xxx-like-xxx"><a href="#torch-xxx-like-xxx" class="headerlink" title="torch.xxx_like()/xxx()"></a>torch.xxx_like()/xxx()</h5><p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921164613808.png" alt="image-20210921164613808"></p>
<ul>
<li>out: 输出的张量（给生成的数据打上新标签）</li>
<li>layout: 内存中的布局形式，有strided, sparse_coo等（目前不懂作用，不过默认strided就好）</li>
</ul>
<p>根据input形状创建全零或全一张量</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921164936568.png" alt="image-20210921164936568"></p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921165012519.png" alt="image-20210921165012519"></p>
<p><em><strong><u>torch.full( )</u><em><strong>和</strong></em><u>torch.full_like( )</u></strong></em></p>
<p>依据input形状创建全（fill_value）值的向量</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921165151658.png" alt="image-20210921165151658"></p>
<ul>
<li>size：张量的形状，如（3，3）</li>
<li>fill_value：张量的值</li>
</ul>
<h5 id="torch-arange"><a href="#torch-arange" class="headerlink" title="torch.arange( )"></a>torch.arange( )</h5><p>创建等差的1维张量，数值区间为[start, end)</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921165611232.png" alt="image-20210921165611232"></p>
<ul>
<li>start: 初始数值</li>
<li>end: 结束数值</li>
<li>step：数列公差</li>
</ul>
<h5 id="torch-linspace"><a href="#torch-linspace" class="headerlink" title="torch.linspace( )"></a>torch.linspace( )</h5><p>创建均分的1维张量，数值区间[start, end]</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921165758777.png" alt="image-20210921165758777"></p>
<ul>
<li>steps: 数列长度</li>
</ul>
<h5 id="torch-logspace"><a href="#torch-logspace" class="headerlink" title="torch.logspace( )"></a>torch.logspace( )</h5><p>创建对数均分的一维张量，长度为<em>steps</em>，底为<em>base</em>（默认为10）</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921170039498.png" alt="image-20210921170039498"></p>
<h5 id="torch-eyes"><a href="#torch-eyes" class="headerlink" title="torch.eyes( )"></a>torch.eyes( )</h5><p>创建单位对角矩阵（2维），默认为方阵，n行数（方阵只需设置行数），m列数</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921170217035.png" alt="image-20210921170217035"></p>
<h4 id="3-依概率分布创建"><a href="#3-依概率分布创建" class="headerlink" title="3.依概率分布创建"></a>3.依概率分布创建</h4><h5 id="torch-normal"><a href="#torch-normal" class="headerlink" title="torch.normal( )"></a>torch.normal( )</h5><p>生成正态分布（Gaussian）</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921170751238.png" alt="image-20210921170751238"></p>
<ul>
<li><p>mean：均值</p>
</li>
<li><p>std：标准差</p>
</li>
<li><p>四种模式：</p>
<ul>
<li><p>mean为<strong>标</strong>量，std为<strong>标</strong>量，例：torch.normal(0., 1., size=(4, ))；</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921171403037.png" alt="image-20210921171403037"></p>
</li>
<li><p>mean为<strong>标</strong>量，std为张量, 同下，</p>
</li>
<li><p>mean为张量，std为<strong>标</strong>量, 例如mean = torch.arrange(1, 5, dtype=torch.float), std = 1</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921171600585.png" alt="image-20210921171600585"></p>
</li>
<li><p>mean为张量，std为张量,例: mean = torch.arrange(1, 5, dypte=torch.float) , std = torch.arrange(1, 5, dypte=torch.float) <img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921171237469.png" alt="image-20210921171237469"></p>
</li>
</ul>
</li>
</ul>
<h5 id="torch-randn-randn-like"><a href="#torch-randn-randn-like" class="headerlink" title="torch.randn()/randn_like()"></a>torch.randn()/randn_like()</h5><p>生成<strong>标准正态分布</strong></p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921171733876.png" alt="image-20210921171733876"></p>
<ul>
<li>size：张量的形状</li>
</ul>
<h5 id="torch-rand-rand-like"><a href="#torch-rand-rand-like" class="headerlink" title="torch.rand()/rand_like()"></a>torch.rand()/rand_like()</h5><p>在区间**[0,1)**上，生成**均匀分布**<img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921171938510.png" alt="image-20210921171938510"></p>
<h5 id="torch-randint-randint-like"><a href="#torch-randint-randint-like" class="headerlink" title="torch.randint()/randint_like()"></a>torch.randint()/randint_like()</h5><p>在区间[low, high)生成<strong>整数均匀分布</strong></p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921172147889.png" alt="image-20210921172147889"></p>
<h5 id="torch-randperm"><a href="#torch-randperm" class="headerlink" title="torch.randperm()"></a>torch.randperm()</h5><p>生成从0到n-1的随机排列</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921172235710.png" alt="image-20210921172235710"></p>
<h5 id="torch-bernoulli"><a href="#torch-bernoulli" class="headerlink" title="torch.bernoulli()"></a>torch.bernoulli()</h5><p>以input为概率，生成伯努利分布（0-1分布，两点分布），input为概率值</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921172334929.png" alt="image-20210921172334929"></p>
<h3 id="C-张量的操作"><a href="#C-张量的操作" class="headerlink" title="C. 张量的操作"></a>C. 张量的操作</h3><h4 id="1-张量拼接与切分"><a href="#1-张量拼接与切分" class="headerlink" title="1.张量拼接与切分"></a>1.张量拼接与切分</h4><h5 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat( )"></a>torch.cat( )</h5><p>将张量按维度dim进行拼接，**<u>cat()操作并不会拓展张量的维度</u>**</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921193450329.png" alt="image-20210921193450329"></p>
<ul>
<li>tensor：张量序列</li>
<li>dim：要拼接的维度</li>
</ul>
<h5 id="torch-stack"><a href="#torch-stack" class="headerlink" title="torch.stack()"></a>torch.stack()</h5><p>在**<u>新创建</u>**的维度dim上进行拼接</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921193619709.png" alt="image-20210921193619709"></p>
<ul>
<li>tensor：张量序列</li>
<li>dim：要拼接的维度</li>
</ul>
<h5 id="torch-chunk"><a href="#torch-chunk" class="headerlink" title="torch.chunk()"></a>torch.chunk()</h5><p>将张量按维度dim进行平均切分，返回张量列表。若不能整除，最后一份张量最小。</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921194449301.png" alt="image-20210921194449301"></p>
<ul>
<li>input：要切分的张量</li>
<li>chunks：要切分的份数</li>
<li>dim：要切分的维度</li>
</ul>
<p>例：</p>
<p><code>a = torch.ones((2, 7))</code> </p>
<p><code>list_of_tensors = torch.chunk(a, dim=1, chunk = 3)</code></p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921195940743.png" alt="image-20210921195940743"></p>
<h5 id="torch-split"><a href="#torch-split" class="headerlink" title="torch.split()"></a>torch.split()</h5><p>将张量按维度dim进行切分，返回值是张量列表</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921200047221.png" alt="image-20210921200047221"></p>
<ul>
<li>tensor：要切分的张量</li>
<li>split_size_or_sections：为int时，表示每一份的长度；为list时，按list元素切分</li>
<li>dim：要切分的维度</li>
</ul>
<h4 id="2-张量索引"><a href="#2-张量索引" class="headerlink" title="2. 张量索引"></a>2. 张量索引</h4><h5 id="torch-index-select"><a href="#torch-index-select" class="headerlink" title="torch.index_select()"></a>torch.index_select()</h5><p>在维度dim上，按index索引数据，返回值为依index索引数据拼接而成的张量。注意，此函数中，index类型必须为torch.long。</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921200540590.png" alt="image-20210921200540590"></p>
<ul>
<li>input：要索引的张量</li>
<li>dim：要索引的维度</li>
<li>index：要索引数据的序号</li>
</ul>
<p>例：</p>
<p><code>t = torch.randint(0, 9, size=(3,3))</code></p>
<p><code>idx = torch.tensor([0, 2], dtype=torch.long)</code></p>
<p><code>t_select = torch.index_select(t, dim=0, index=idx)</code></p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921200914186.png" alt="image-20210921200914186"></p>
<h5 id="torch-masked-select"><a href="#torch-masked-select" class="headerlink" title="torch.masked_select()"></a>torch.masked_select()</h5><p>按mask中的True进行索引，返回值为一维张量。</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921201241414.png" alt="image-20210921201241414"></p>
<ul>
<li>input：要索引的张量</li>
<li>mask：与input同形状的布尔类型张量</li>
</ul>
<p><code>热知识：t = torch.randint(0, 9, size=(3,3))</code></p>
<p>​                <code>mask = t.ge(5) # ge means greater or equal</code>    </p>
<p>​        <code>#        mask = t.gt(5) # ge means greater</code>    </p>
<p>​        <code>#        mask = t.le(5) # ge means less or equal</code>    </p>
<p>​        <code>#        mask = t.lt(5) # ge means less</code></p>
<p>​        <code>t_select = torch.masked_select(t, mask)</code></p>
<p>Output:</p>
<p>​        <img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921201821510.png" alt="image-20210921201821510"></p>
<h4 id="3-张量的变换"><a href="#3-张量的变换" class="headerlink" title="3. 张量的变换"></a>3. 张量的变换</h4><h5 id="torch-reshape"><a href="#torch-reshape" class="headerlink" title="torch.reshape()"></a>torch.reshape()</h5><p>变换张量的形状，<strong>注意</strong>：当张量在内存中是连续时，新张量与input<strong>共享数据内存</strong>。</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921202011330.png" alt="image-20210921202011330"></p>
<p>新张量形状大小应该与input相匹配。若为某一维度长度由其他维度计算得知，<strong>可设置为-1</strong>。</p>
<h5 id="torch-transpose-torch-t"><a href="#torch-transpose-torch-t" class="headerlink" title="torch.transpose()/torch.t()"></a>torch.transpose()/torch.t()</h5><p>交换张量的两个维度。**(torch.t()适用于二维张量，等于torch.transpose(input, 0, 1))**</p>
<p><strong>图像处理时</strong>经常会用到，将channels * h * w变成h * w * channels。</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921202318278.png" alt="image-20210921202318278"></p>
<ul>
<li>input：要变换的张量</li>
<li>dim0：要交换的维度</li>
<li>dim1：要交换的维度</li>
</ul>
<h5 id="torch-squeeze-unsqueeze"><a href="#torch-squeeze-unsqueeze" class="headerlink" title="torch.squeeze()/unsqueeze()"></a>torch.squeeze()/unsqueeze()</h5><p><strong>squeeze()</strong>: 压缩长度为1的维度（轴）</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921202819633.png" alt="image-20210921202819633"></p>
<p>dim：若为None，移除所有长度为1的轴；若指定维度，当且仅当该轴长度为1时，可以被移除。</p>
<p>**unsqueeze()**：依据dim扩展维度</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921202948318.png" alt="image-20210921202948318"></p>
<p>dim: 扩展的维度</p>
<h3 id="D-张量数学运算"><a href="#D-张量数学运算" class="headerlink" title="D. 张量数学运算"></a>D. 张量数学运算</h3><p>分为三类：</p>
<ol>
<li>加减乘除</li>
<li>对数，指数，幂函数</li>
<li>三角函数</li>
</ol>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921203343226.png" alt="image-20210921203343226"></p>
<h4 id="两个pythonic的方法"><a href="#两个pythonic的方法" class="headerlink" title="两个pythonic的方法"></a>两个pythonic的方法</h4><p>两个特别pythonic的方法：</p>
<p>torch.addcdiv()</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921203603682.png" alt="image-20210921203603682"></p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921203550310.png" alt="image-20210921203550310"></p>
<p>torch.addcmul()</p>
<p>理解方式同上</p>
<h3 id="E-计算图与动态图机制"><a href="#E-计算图与动态图机制" class="headerlink" title="E. 计算图与动态图机制"></a>E. 计算图与动态图机制</h3><h4 id="1-线性回归（Linear-Regression）"><a href="#1-线性回归（Linear-Regression）" class="headerlink" title="1. 线性回归（Linear Regression）"></a>1. 线性回归（Linear Regression）</h4><p><strong>线性回归</strong>是分析一个<strong>变量</strong>与另一个（多个）<strong>变量</strong>之间<strong>关系</strong>的方法。</p>
<p>因变量：y</p>
<p>自变量：x</p>
<p>关系：y = w*x + b</p>
<p>分析：求解w，b</p>
<h5 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h5><p>Model：y = w*x + b</p>
<h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><p>MSE： <img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921204404418.png" alt="image-20210921204404418"></p>
<h5 id="求解梯度并更新参数"><a href="#求解梯度并更新参数" class="headerlink" title="求解梯度并更新参数"></a>求解梯度并更新参数</h5><p>w = w - LR * w.grad</p>
<p>b = w - LR * b.grad</p>
<h4 id="2-计算图（Computational-Graph）"><a href="#2-计算图（Computational-Graph）" class="headerlink" title="2. 计算图（Computational Graph）"></a>2. 计算图（Computational Graph）</h4><h5 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h5><p>计算图是用来<strong>描述运算</strong>的<strong>有向无环图</strong>。</p>
<p>计算图的两个主要元素：<strong>结点</strong>（Node）和<strong>边</strong>（Edge）</p>
<p>结点表示<strong>数据</strong>，如向量，矩阵，张量</p>
<p>边表示<strong>运算</strong>，如加减乘除卷积等<br>$$<br>y = (x + w) * (w + 1)<br>$$<br>a = x + w<br>b = w + 1<br>y = a * b</p>
<img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921212833057.png" alt="image-20210921212833057" style="zoom: 25%;">

<h5 id="计算图与求导机制"><a href="#计算图与求导机制" class="headerlink" title="计算图与求导机制"></a>计算图与求导机制</h5><p>当我们对w进行求导</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921213436761.png" alt="image-20210921213436761"></p>
<img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921213452525.png" alt="image-20210921213452525" style="zoom:25%;">

<h5 id="is-leaf-叶子结点"><a href="#is-leaf-叶子结点" class="headerlink" title="is_leaf(叶子结点)"></a>is_leaf(叶子结点)</h5><p>torch.Tensor的is_leaf属性。用户创建的结点为叶子结点。</p>
<p>在反向传播（BP）中，所有的计算都要依赖于叶子结点，反向传播之后，所有的非叶子结点的梯度都会被释放掉。</p>
<p>is_leaf：张量是否为叶子结点（结点代表数据，边代表运算）。</p>
<p><strong>如果想使用非叶子结点的梯度，则需要用torch.Tensor.retain_grad()</strong></p>
<h5 id="grad-fn-方法"><a href="#grad-fn-方法" class="headerlink" title="grad_fn(方法)"></a>grad_fn(方法)</h5><p>记录创建该张量时所用的方法（函数）。</p>
<p><strong>叶子结点的gradient_function 为None。</strong></p>
<p>例：</p>
<p>y.grad_fn = &lt; MulBackward0 &gt;</p>
<p>a.grad_fn = &lt; AddBackward0 &gt;</p>
<p>b.grad_fn = &lt; AddBackward0 &gt;</p>
<h4 id="3-动态图vs静态图"><a href="#3-动态图vs静态图" class="headerlink" title="3. 动态图vs静态图"></a>3. 动态图vs静态图</h4><p>(PyTorch) 动态图：运算与搭建<strong>同时</strong>进行（自驾游出行）。</p>
<p>(Tensorflow1.0) 静态图：<strong>先</strong>搭建图，<strong>后</strong>运算（跟旅游团出行）。</p>
<h3 id="F-autograd与逻辑回归"><a href="#F-autograd与逻辑回归" class="headerlink" title="F. autograd与逻辑回归"></a>F. autograd与逻辑回归</h3><p>深度学习模型训练就是不断的更新权值。</p>
<p>我们不需要手动计算梯度，只需要搭好前向计算图。</p>
<h4 id="1-torch-autograd"><a href="#1-torch-autograd" class="headerlink" title="1. torch.autograd"></a>1. torch.autograd</h4><h5 id="torch-autograd-backward"><a href="#torch-autograd-backward" class="headerlink" title="torch.autograd.backward"></a>torch.autograd.backward</h5><p>用于自动求取梯度 </p>
<p>一般常见的loss.backward()就是调用的这个方法。</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210922005408519.png" alt="image-20210922005408519"></p>
<ul>
<li>tensors: 用于求导的张量，如loss</li>
<li>retain_graph：保存计算图</li>
<li>create_graph：创建导数计算图，用于高阶求导</li>
<li>grad_tensors：多梯度权重（用于多个梯度之间权重的设置）</li>
</ul>
<p><strong>注意</strong>：在retain_graph为false的情况，无法连续执行两次反向传播，会提示”the buffers have already been freed“。</p>
<h5 id="torch-autograd-grad"><a href="#torch-autograd-grad" class="headerlink" title="torch.autograd.grad"></a>torch.autograd.grad</h5><p>求取梯度，返回值是我们想要的那一个张量的梯度。</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210922010704023.png" alt="image-20210922010704023"></p>
<ul>
<li>outputs：用于求导的张量，如loss</li>
<li>inputs：需要梯度的张量</li>
<li>create_graph：创建导数计算图（用于高阶求导时要为True，例如对导数再次求导）</li>
<li>retain_graph：保存计算图</li>
<li>grad_outputs：多梯度权重</li>
</ul>
<h5 id="注意"><a href="#注意" class="headerlink" title="注意"></a><strong><u><em>注意</em></u></strong></h5><ol>
<li><p><strong>梯度不自动清零</strong></p>
<p>所以一般在执行backward( )操作之后要进行grad.zero_( )</p>
<p>_下划线表示原地，原位操作（in-place）</p>
</li>
<li><p><strong>依赖于叶子结点的结点，required_grad默认为True</strong></p>
</li>
<li><p><strong>叶子结点不可执行in-place操作</strong></p>
<p>反向传播时根据地址寻找数据，并且用到了叶子结点该地址当中的数据。如果在反向传播前改变了该地址的数据，就会引起反向传播出错。</p>
</li>
</ol>
<h4 id="2-逻辑回归（Logisti-Regress）"><a href="#2-逻辑回归（Logisti-Regress）" class="headerlink" title="2. 逻辑回归（Logisti Regress）"></a>2. 逻辑回归（Logisti Regress）</h4><p>逻辑回归是线性的二分类模型</p>
<p>模型表达式:</p>
<img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210922012147317.png" alt="image-20210922012147317" style="zoom:25%;">

<img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210922012334999.png" alt="image-20210922012334999" style="zoom: 50%;">

<p>线性回归是分析自变量x与因变量y（标量）之间关系的方法</p>
<p>逻辑回归是分析自变量x与因变量y（概率）之间关系的方法</p>
<p><img src="/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210922012729014.png" alt="image-20210922012729014"></p>
]]></content>
      <categories>
        <category>PyTorch框架学习</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch-Loss&amp;Optimizer</title>
    <url>/2021/09/26/PyTorch-Loss-Optimizer/</url>
    <content><![CDATA[<h2 id="四-PyTorch损失优化（Loss-amp-Optimizer）"><a href="#四-PyTorch损失优化（Loss-amp-Optimizer）" class="headerlink" title="四. PyTorch损失优化（Loss&amp;Optimizer）"></a>四. PyTorch损失优化（Loss&amp;Optimizer）</h2><h3 id="A-权值初始化"><a href="#A-权值初始化" class="headerlink" title="A. 权值初始化"></a>A. 权值初始化</h3><h4 id="1-梯度消失与爆炸Gradient-Vanishing-and-Exploring"><a href="#1-梯度消失与爆炸Gradient-Vanishing-and-Exploring" class="headerlink" title="1. 梯度消失与爆炸Gradient Vanishing and Exploring"></a>1. 梯度消失与爆炸Gradient Vanishing and Exploring</h4><h5 id="方差一致性原则"><a href="#方差一致性原则" class="headerlink" title="方差一致性原则"></a>方差一致性原则</h5><p>$$</p>
<ol>
<li> E(X*Y) = E(X)*E(Y)\</li>
<li> D(X) = E(X^2) - [E(X)]^2\</li>
<li> D(X+Y) = D(X) + D(Y)<br>$$</li>
</ol>
<p>$$<br>1.2.3 \Rightarrow D(X<em>Y) = D(X)<em>D(Y) + D(X)</em>[E(Y)]^2 + D(Y)</em>[E(X)]^2\<br>若E(X)=0, E(Y)=0\<br>则D(X*Y) = D(X)*D(Y)<br>$$</p>
<p>$$<br>第一个隐藏层的第一个神经元, 输入层的每一个神经元✖权值求和：\<br>H_{11} = \sum^{n}<em>{i=0}X_i*W</em>{1i}\由公式 D(X<em>Y) = D(X)<em>D(Y)\<br>可知D(H_{11}) = \sum^{n}<em>{i=0}D(X_i)*D(W</em>{1i})\ = n*(1</em>1)n为神经元的个数，X_i和W_i的方差均为1\<br>=n\<br>std(H_{11}) = \sqrt{D(H_{11})} = \sqrt{n}\<br>意味着经过一层神经元的传播标准差就会扩大\sqrt{n}倍\<br>因此，想让方差D(H_1) = n</em>D(X)*D(W) = 1\<br>D(W)=\frac{1}{n}\Rightarrow std(W)=\sqrt{\frac{1}{n}}\<br>所以权值的标准差为\sqrt{\frac{1}{n}}时（n为每一层神经元个数），\才可以保证尺度维持在恰当范围。<br>$$</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210924001951770.png" alt="image-20210924001951770"></p>
<p>需要保持尺度不变</p>
<h4 id="2-Xavier方法和Kaiming方法"><a href="#2-Xavier方法和Kaiming方法" class="headerlink" title="2. Xavier方法和Kaiming方法"></a>2. Xavier方法和Kaiming方法</h4><p>Xavier初始化</p>
<p>方差一致性：保持数据尺度维持在恰当范围，通常方差为1。</p>
<p>适用激活函数：饱和函数，如sigmoid，Tanh</p>
<img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210924004244068.png" alt="image-20210924004244068" style="zoom: 50%;">

<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210924004442111.png" alt="image-20210924004442111"></p>
<p>Kaiming初始化</p>
<p>方差一致性：保持数据尺度维持在恰当范围，通常方差为1。</p>
<p>适用激活函数：ReLU及其变种</p>
<img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210924005023102.png" alt="image-20210924005023102" style="zoom: 33%;">





<h4 id="3-常用初始化方法"><a href="#3-常用初始化方法" class="headerlink" title="3. 常用初始化方法"></a>3. 常用初始化方法</h4><h5 id="PyTorch方法查询"><a href="#PyTorch方法查询" class="headerlink" title="PyTorch方法查询"></a>PyTorch方法查询</h5><p><a href="https://pytorch.org/docs/stable/nn.init.html">https://pytorch.org/docs/stable/nn.init.html</a></p>
<ul>
<li>Xavier均匀/正态分布</li>
<li>Kaiming均匀/正态分布</li>
<li>均匀分布 / 正态分布 / 常熟分布</li>
<li>正交矩阵初始化 / 单位矩阵初始化 / 稀疏矩阵初始化</li>
</ul>
<h5 id="nn-init-calculate-gain"><a href="#nn-init-calculate-gain" class="headerlink" title="nn.init.calculate_gain()"></a>nn.init.calculate_gain()</h5><p>nn.init.calculate_gain(nonlinearity, param=None)</p>
<p>计算激活函数的方差变换尺度</p>
<p>主要参数：</p>
<ul>
<li>nonlinearity：激活函数名称</li>
<li>param：激活函数的参数，如leakyReLU的negative_slop</li>
</ul>
<h3 id="B-损失函数"><a href="#B-损失函数" class="headerlink" title="B. 损失函数"></a>B. 损失函数</h3><h4 id="1-损失函数概念"><a href="#1-损失函数概念" class="headerlink" title="1. 损失函数概念"></a>1. 损失函数概念</h4><p><strong>损失函数</strong>：衡量模型输出与真实标签的<strong>差异</strong></p>
<img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210924105850749.png" alt="image-20210924105850749" style="zoom:50%;">

<p>损失函数（Loss Function）：</p>
<p>计算<strong>一个样本</strong><br>$$<br>Loss = f（y’，y)<br>$$<br>代价函数（Cost Function）：</p>
<p>计算<strong>训练集（样本集）</strong>中各个Loss的<strong>平均值</strong><br>$$<br>Cost = \frac{1}{N}\sum^{N}_{i}f(y’_i, y_i)<br>$$<br>目标函数（Objective Function）：<br>$$<br>Obj = Cost（代价函数） + Regularization<br>$$<br>Cost代价函数<strong>并不是越小越好</strong>，可能会有过拟合。约束项是Regularization（L1，L2，稀疏约束），和Cost一起构成目标函数。</p>
<h4 id="2-损失函数"><a href="#2-损失函数" class="headerlink" title="2. 损失函数"></a>2. 损失函数</h4><h5 id="nn-CrossEntropyLoss"><a href="#nn-CrossEntropyLoss" class="headerlink" title="nn.CrossEntropyLoss"></a>nn.CrossEntropyLoss</h5><p><strong>nn.LogSoftmax()与nn.NLLLoss()结合</strong>，进行交叉熵计算</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210924112655749.png" alt="image-20210924112655749"></p>
<p>主要参数：</p>
<ul>
<li>weight：各类别的loss设置权值</li>
<li>ignore_index：忽略某个类别</li>
<li>reduction：计算模式，可为none/sum/mean。‘none’不会造成维度上的衰减，‘sum’会将loss值相加，‘mean’会对所有loss求平均。  </li>
</ul>
<p>none- 逐个元素计算</p>
<p>sum- 所有元素求和，返回标量</p>
<p>mean- 加权平均，返回标量</p>
<p>交叉熵 = 信息熵 + 相对熵</p>
<p>熵（用来描述一件事情的不确定性，不确定性的概率越大，熵就越大）：</p>
<p>熵是自信息的一个期望，是描述整个概率分布：<br>$$<br>H(P)=E_{x\thicksim p}[I(x)] = -\sum^{N}<em>{i}P(x_i)logP(x_i)<br>$$<br>自信息的公式，对概率取一个负log：<br>$$<br>I(x) = -log[p(x)]<br>$$<br>相对熵，用来衡量两个分布之间的距离（差异）：<br>$$<br>D</em>{KL}(P, Q) = E_{x\thicksim p}[log\frac{P(x)}{Q(x)}]\<br>= E_{x\thicksim p}[logP(x) - logQ(x)]\<br>=\sum^{N}<em>{i=1}P(x_i)[logP(x_i)-logQ(x_i)]\<br>=\sum^{N}</em>{i=1}P(x_i)logP(x_i)-\sum^{N}_{i=1}P(x_i)logQ(x_i)\<br>=H(P,Q) - H(P)\<br>P是真实分布，Q是模型拟合的分布。我们要用Q去拟合（逼近）P<br>$$<br>weight参数的使用：</p>
<ul>
<li>向量的形式，有多少个类别，就要有多少个weight</li>
<li>使用之后，会给不同的类别赋予不同的权值</li>
</ul>
<h5 id="nn-NLLLoss"><a href="#nn-NLLLoss" class="headerlink" title="nn.NLLLoss"></a>nn.NLLLoss</h5><p>实现负对数似然函数中的负号功能（其实只是执行了加个负号的功能）</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210924163054623.png" alt="image-20210924163054623"></p>
<p>主要参数：</p>
<ul>
<li>weight：各类别的loss设置权值</li>
<li>ignore_index：忽略某个类别</li>
<li>reduction：计算模式，可为none/sum/mean</li>
</ul>
<p>none- 逐个元素计算</p>
<p>sum- 所有元素求和，返回标量</p>
<p>mean- 加权平均，返回标量</p>
<h5 id="nn-BCELoss"><a href="#nn-BCELoss" class="headerlink" title="nn.BCELoss"></a>nn.BCELoss</h5><p>二分类交叉熵</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925145330328.png" alt="image-20210925145330328"></p>
<p><strong>注意事项</strong>：输入值取值在[0, 1]，符合概率取值的区间</p>
<p>主要参数：</p>
<ul>
<li>weight：各类别的loss设置权值</li>
<li>ignore_index：忽略某个类别</li>
<li>reduction：计算模式，可为none/sum/mean</li>
</ul>
<h5 id="nn-BCEWithLogitsLoss"><a href="#nn-BCEWithLogitsLoss" class="headerlink" title="nn.BCEWithLogitsLoss"></a>nn.BCEWithLogitsLoss</h5><p>结合<strong>Sigmoid</strong>与<strong>二分类交叉熵</strong></p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925145853652.png" alt="image-20210925145853652"></p>
<p><strong>注意事项：网络后面不加sigmoid函数</strong></p>
<p>主要参数：</p>
<ul>
<li>pos_weight：正样本的权值</li>
<li>weight：各类别的loss设置权值</li>
<li>ignore_index：忽略某个类别</li>
<li>reduction：计算模式，可为none/sum/mean</li>
</ul>
<h5 id="nn-L1Loss"><a href="#nn-L1Loss" class="headerlink" title="nn.L1Loss"></a>nn.L1Loss</h5><p>计算inputs与target之差的绝对值</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925150840297.png" alt="image-20210925150840297"></p>
<h5 id="nn-MSELoss"><a href="#nn-MSELoss" class="headerlink" title="nn.MSELoss"></a>nn.MSELoss</h5><p>计算inputs与target之差的平方</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925150852293.png" alt="image-20210925150852293"></p>
<p>主要参数：</p>
<ul>
<li>reduction：计算模式，可为none/ sum/ mean</li>
</ul>
<h5 id="nn-SmoothL1Loss"><a href="#nn-SmoothL1Loss" class="headerlink" title="nn.SmoothL1Loss"></a>nn.SmoothL1Loss</h5><p>平滑的L1Loss。</p>
<p>Xi是模型输出，Yi是标签。</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925151138993.png" alt="image-20210925151138993"></p>
<p>主要参数：</p>
<ul>
<li>reduction：计算模式，可为none/ sum/ mean</li>
<li><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925151323568.png" alt="image-20210925151323568" style="zoom:33%;"></li>
</ul>
<h5 id="nn-PoissonNLLLoss"><a href="#nn-PoissonNLLLoss" class="headerlink" title="nn.PoissonNLLLoss"></a>nn.PoissonNLLLoss</h5><p>泊松分布的负对数似然损失函数</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925151752707.png" alt="image-20210925151752707"></p>
<p>主要参数：</p>
<ul>
<li><strong>log_input</strong>：输入是否为对数形式，决定计算公式</li>
<li><strong>full</strong>：计算所有loss，默认为False</li>
<li><strong>eps</strong>：修正项，避免log（input）为nan</li>
</ul>
<h5 id="nn-KLDivLoss"><a href="#nn-KLDivLoss" class="headerlink" title="nn.KLDivLoss"></a>nn.KLDivLoss</h5><p>计算<strong>KLD（divergence）</strong>，KL散度，<strong>相对熵</strong>。衡量两个分布之间的距离。</p>
<p><strong>注意事项：需提前将输入计算log-probabilities，如通过nn.logsoftmax()</strong></p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925152326201.png" alt="image-20210925152326201"></p>
<p>主要参数：</p>
<ul>
<li>reduction：none/ sum/ mean /batchmean</li>
</ul>
<p>batchmean- batchsize维度求平均值</p>
<h5 id="nn-MarginRankingLoss"><a href="#nn-MarginRankingLoss" class="headerlink" title="nn.MarginRankingLoss"></a>nn.MarginRankingLoss</h5><p>计算两个向量之间的相似度，<strong>用于排序任务</strong></p>
<p>y=1时，希望x1与x2大，当x1&gt;x2时，不产生loss</p>
<p>y=-1时，希望x2比x1大，当x2&gt;x1时，不产生loss</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925153835354.png" alt="image-20210925153835354"></p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925153849860.png" alt="image-20210925153849860"></p>
<p><strong>特别说明：该方法计算两组数据之间的差异，返回一个n*n的loss矩阵</strong></p>
<p>主要参数：</p>
<ul>
<li>margin：边界值，x1与x2之间的差异值</li>
<li>reduction：计算模式</li>
</ul>
<h5 id="nn-MultiLabelMarginLoss"><a href="#nn-MultiLabelMarginLoss" class="headerlink" title="nn.MultiLabelMarginLoss"></a>nn.MultiLabelMarginLoss</h5><p>多标签边界损失函数</p>
<p>举例：四分类任务，样本x属于0类或者3类</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925155812561.png" alt="image-20210925155812561"></p>
<p>标签：[0，3，-1，1]，不是[1，0，0，1]</p>
<h5 id="nn-SofrMarginLoss"><a href="#nn-SofrMarginLoss" class="headerlink" title="nn.SofrMarginLoss"></a>nn.SofrMarginLoss</h5><p>计算二分类的logistic损失</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925160607942.png" alt="image-20210925160607942"></p>
<h5 id="nn-MultiLabelSoftMarginLoss"><a href="#nn-MultiLabelSoftMarginLoss" class="headerlink" title="nn.MultiLabelSoftMarginLoss"></a>nn.MultiLabelSoftMarginLoss</h5><p>SoftMarginLoss多标签版本</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925160853772.png" alt="image-20210925160853772"></p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925160909202.png" alt="image-20210925160909202"></p>
<p>主要参数：</p>
<p><strong>weight</strong>：各类别的loss设置权值</p>
<p><strong>reduction</strong>：计算模式，可为none / sum / mean</p>
<h5 id="nn-MultiMarginLoss"><a href="#nn-MultiMarginLoss" class="headerlink" title="nn.MultiMarginLoss"></a>nn.MultiMarginLoss</h5><p>计算多分类的折页损失</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925161602192.png" alt="image-20210925161602192"></p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925161628228.png" alt="image-20210925161628228"></p>
<p>主要参数：</p>
<ul>
<li>p：可选1或2</li>
<li>weight：各类别的loss设置权值</li>
<li>margin：边界值</li>
<li>reduction：计算模式，可为none/sum/mean</li>
</ul>
<h5 id="nn-TripletMarginLoss"><a href="#nn-TripletMarginLoss" class="headerlink" title="nn.TripletMarginLoss"></a>nn.TripletMarginLoss</h5><p>计算三元组损失，人脸验证中常用</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925162115800.png" alt="image-20210925162115800"></p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925162211016.png" alt="image-20210925162211016"></p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925162237021.png" alt="image-20210925162237021"></p>
<p>主要参数：</p>
<ul>
<li>p：范数的阶，默认为2</li>
<li>margin：边界值</li>
<li>reduction：计算模式，可为none/sum/mean</li>
</ul>
<h5 id="nn-HingeEmbeddingLoss"><a href="#nn-HingeEmbeddingLoss" class="headerlink" title="nn.HingeEmbeddingLoss"></a>nn.HingeEmbeddingLoss</h5><p>计算两个输入的相似性，常用于非线性embedding和半监督学习</p>
<p><strong>特别注意：输入x应为两个输入只差的绝对值</strong></p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925162430193.png" alt="image-20210925162430193"></p>
<p>主要参数：</p>
<ul>
<li>margin：边界值</li>
<li>reduction：计算模式，可为none/sum/mean</li>
</ul>
<h5 id="nn-CosineEmbeddingLoss"><a href="#nn-CosineEmbeddingLoss" class="headerlink" title="nn.CosineEmbeddingLoss"></a>nn.CosineEmbeddingLoss</h5><p>采用余弦相似度计算两个输入的相似性，主要关注方向上的差异。</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925162632490.png" alt="image-20210925162632490"></p>
<p>主要参数：</p>
<p>margin：可取值[-1，1]，推荐为[0，0.5]</p>
<p>reduction：计算模式，可为none/sum/mean</p>
<h5 id="nn-CTCLoss"><a href="#nn-CTCLoss" class="headerlink" title="nn.CTCLoss"></a>nn.CTCLoss</h5><p>计算CTC损失，解决时序类数据的分类Connectionist Temporal Classification</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925163057884.png" alt="image-20210925163057884"></p>
<p>主要参数：</p>
<ul>
<li>blank：blank label</li>
<li>zero_infinity：无穷大的值或者梯度置0</li>
<li>reduction：计算模式，可为none/sum/mean</li>
</ul>
<h3 id="C-优化器-Optimizer"><a href="#C-优化器-Optimizer" class="headerlink" title="C. 优化器 Optimizer"></a>C. 优化器 Optimizer</h3><h4 id="1-什么是优化器"><a href="#1-什么是优化器" class="headerlink" title="1. 什么是优化器"></a>1. 什么是优化器</h4><p>pytorch的优化器：管理并更新模型中的可学习参数的值，使得模型输出更接近真实标签</p>
<ul>
<li><strong>导数</strong>：函数在指定坐标轴上的变化率</li>
<li><strong>方向导数</strong>：指定方向上的变化率</li>
<li><strong>梯度</strong>：一个向量，方向为方向导数取得最大值的方向</li>
</ul>
<img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925164501982.png" alt="image-20210925164501982" style="zoom:50%;">

<h4 id="2-optimizer的属性"><a href="#2-optimizer的属性" class="headerlink" title="2. optimizer的属性"></a>2. optimizer的属性</h4><p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925164937096.png" alt="image-20210925164937096"></p>
<p>基本属性：</p>
<ul>
<li>defaults：优化器超参数</li>
<li>state：参数的缓存，如momentum的缓存</li>
<li>param_groups：管理的参数组</li>
<li>_step_count：记录更新的次数，学习率调整中使用</li>
</ul>
<p>基本方法：</p>
<ul>
<li><p>zero_grad()：清空所管理参数的梯度<img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925165616845.png" alt="image-20210925165616845" style="zoom:50%;"></p>
</li>
<li><p>step()：执行一步更新，更新一次我们的权值参数</p>
</li>
<li><p>add_param_group()：添加参数组</p>
</li>
<li><p>state_dict()：获取优化器当前状态信息字典</p>
</li>
<li><p>load_state_dict()：加载状态信息字典（用于模型断点的续训练）</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925211631557.png" alt="image-20210925211631557"></p>
</li>
</ul>
<p><strong>pytorch特性：张量梯度不自动清零，会将每一次计算的梯度加在一起。所以需要使用完梯度，或者backward之前，要zero_grad()一下。</strong></p>
<h4 id="3-optimizer的方法"><a href="#3-optimizer的方法" class="headerlink" title="3. optimizer的方法"></a>3. optimizer的方法</h4><h4 id="4-learning-rate学习率"><a href="#4-learning-rate学习率" class="headerlink" title="4. learning rate学习率"></a>4. learning rate学习率</h4><p>梯度下降：<br>$$<br>W_{i+1} = W_{i} - g(W_{i})<br>$$<br>假设现在函数为<br>$$<br>y = f(x) = 4<em>x^2\<br>y’ = f’(x)=8</em>x<br>$$<br><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925215213471.png" alt="image-20210925215213471" style="zoom:33%;"><br>$$<br>x_0 =2,y_0 = 16,f’(x_0)=16\<br>x_1=x_0 - f’(x_0)=2-16=14\<br>x_1=-14,y_1 = 784, f’(x_1)=-112\<br>x_2 = x_1 - f’(x_1)=-14+112=98,y_2=38416\<br>……<br>$$<br>如果没有学习率的话，就会梯度爆炸。</p>
<img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925220758203.png" alt="image-20210925220758203" style="zoom: 50%;">

<p>学习率（learning rate）控制更新的步伐。<br>$$<br>W_{i+1} = W_{i} - LR*g(W_{i})<br>$$<br>此时我们将LR设置为0.2：</p>
<img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925221017651.png" alt="image-20210925221017651" style="zoom: 50%;">

<p>将LR设置为0.1时：</p>
<img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925221141585.png" alt="image-20210925221141585" style="zoom: 50%;">



<h4 id="5-momentum-动量"><a href="#5-momentum-动量" class="headerlink" title="5. momentum 动量"></a>5. momentum 动量</h4><p>Momentum（动量，冲量）：<strong>结合当前梯度与上一次更新信息</strong>，用于当前更新。</p>
<p>指数加权平均：<br>$$<br>v_t = \beta * v_{t-1} + (1-\beta)*\theta_t\<br>v_{100} = \beta * v_{99} + (1-\beta)*\theta_{100}\<br>= (1-\beta)<em>\theta_{100} + (1-\beta)<em>\beta</em>\theta_{99}+(\beta^2</em>v_{98})\<br>=\sum^N_i(1-\beta)<em>\beta^i</em>\theta_{N-i}<br>$$<br><strong>核心思想</strong>：距离当前时间越近，影响越大；距离当前时间越远，影响越小。</p>
<p>PyTorch中的更新公式：<br>$$<br>v_i = m<em>v_{i-1}+g(w_i)\<br>w_{i+1}=w_i - lr</em>v_i\<br>w_{w+1}：第i+1次更新的参数\<br>lr：学习率\<br>v_i = 更新量\<br>m：momentum系数\<br>g(w_i)：w_i的梯度<br>$$</p>
<h4 id="6-torch-optim-SGD"><a href="#6-torch-optim-SGD" class="headerlink" title="6. torch.optim.SGD"></a>6. torch.optim.SGD</h4><p>用的最多的优化器，绝大部分模型都可以用SGD获得一个不错的效果。</p>
<p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925230235496.png" alt="image-20210925230235496"></p>
<p>主要参数：</p>
<ul>
<li>params：管理的参数组</li>
<li>lr：初始学习率</li>
<li>momentum：动量参数，贝塔</li>
<li>weight_decay：L2正则化系数</li>
<li>nesterov：是否采用NAG（通常为false，不会采用）</li>
</ul>
<h3 id="D-Pytorch的十种优化器"><a href="#D-Pytorch的十种优化器" class="headerlink" title="D. Pytorch的十种优化器"></a>D. Pytorch的十种优化器</h3><p><img src="/2021/09/26/PyTorch-Loss-Optimizer/image-20210925230441311.png" alt="image-20210925230441311"></p>
<h4 id="TODO：详细使用方法以后补充"><a href="#TODO：详细使用方法以后补充" class="headerlink" title="TODO：详细使用方法以后补充"></a>TODO：详细使用方法以后补充</h4>]]></content>
      <categories>
        <category>PyTorch框架学习</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch-ModelsConstruction</title>
    <url>/2021/09/26/PyTorch-ModelsConstruction/</url>
    <content><![CDATA[<h2 id="三-PyTorch模型搭建（Models）"><a href="#三-PyTorch模型搭建（Models）" class="headerlink" title="三. PyTorch模型搭建（Models）"></a>三. PyTorch模型搭建（Models）</h2><h3 id="A-模型创建步骤与nn-Module"><a href="#A-模型创建步骤与nn-Module" class="headerlink" title="A. 模型创建步骤与nn.Module"></a>A. 模型创建步骤与nn.Module</h3><h4 id="1-网路模型创建步骤"><a href="#1-网路模型创建步骤" class="headerlink" title="1. 网路模型创建步骤"></a>1. 网路模型创建步骤</h4><p>模型（nn.Module）：</p>
<ul>
<li>模型创建：<ul>
<li>构建网络<ul>
<li>卷积层，池化层，激活函数层等等</li>
</ul>
</li>
<li>拼接网络<ul>
<li>LeNet，ResNet，AlexNet等等</li>
</ul>
</li>
</ul>
</li>
<li>权值初始化<ul>
<li>Xavier，Kaiming，正态分布，均匀分布等</li>
</ul>
</li>
</ul>
<p>例：</p>
<p>LeNet模型如下：</p>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923111900159-16326570841981.png" alt="image-20210923111900159"></p>
<p>LeNet计算图：</p>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923111953621-16326570841983.png" alt="image-20210923111953621"></p>
<p>从左向右便是前向传播。</p>
<p>构建模型的两个要素：</p>
<ul>
<li>构建子模块：conv，pool，fc等等：在__ init __()中实现</li>
<li>拼接子模块：按一定的顺序，一定的拓扑结构进行组装：在forward()当中实现</li>
</ul>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># 模型初始化，就是构建子模块</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LeNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, classes)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 拼接子模块，也就是前向传播的一个实现</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = F.relu(self.conv1(x))</span><br><span class="line">        out = F.max_pool2d(out, <span class="number">2</span>)</span><br><span class="line">        out = F.relu(self.conv2(out))</span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = F.relu(self.fc1(out))</span><br><span class="line">        out = F.relu(self.fc2(out))</span><br><span class="line">        out = self.fc3(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line">        </span><br></pre></td></tr></table></figure>



<h4 id="2-nn-Module"><a href="#2-nn-Module" class="headerlink" title="2. nn.Module"></a>2. nn.Module</h4><p><strong>torch.nn</strong>：</p>
<ul>
<li>nn.Parameter：张量子类，表示可学习的参数，如weight，bias</li>
<li>nn.Module：所有网络层的基类，管理网络的属性（例如LeNet要继承它）</li>
<li>nn.functional：函数的具体实现，如卷积，池化，激活函数等等</li>
<li>nn.init：提供了丰富的参数初始化的方法</li>
</ul>
<p><strong>nn.Module</strong>：</p>
<ul>
<li><strong>parameters（重要）</strong>：存储管理nn.Parameter类</li>
<li><strong>modules（重要）</strong>：存储管理nn.Module类（构建子模块）</li>
<li>buffers：存储管理缓冲属性，如BN层中的running_mean</li>
<li>***_hooks：存储管理钩子函数（共五个和hooks有关的字典）</li>
</ul>
<p><strong>在对创建的新module（例如LeNet）进行赋值时（__ init  <strong>()中的属性构建），会被一个</strong> setAttr __()函数进行拦截，判断其为parameter类还是module类。然后在存储进各自对应的字典当中。</strong></p>
<p><strong>总结</strong>：</p>
<ul>
<li>一个module可以包含多个子module</li>
<li>一个module相当于一个运算，必须实现forward()函数</li>
<li>每个module都有8个字典管理它的属性</li>
</ul>
<h3 id="B-模型容器与AlexNet构建"><a href="#B-模型容器与AlexNet构建" class="headerlink" title="B. 模型容器与AlexNet构建"></a>B. 模型容器与AlexNet构建</h3><h4 id="1-Containers"><a href="#1-Containers" class="headerlink" title="1. Containers"></a>1. Containers</h4><p><strong>Containers（容器）</strong>:</p>
<ul>
<li><strong>nn.Sequential</strong>：按顺序的将一组网络层包裹起来</li>
<li><strong>nn.ModuleList</strong>：像python的list一样包装多个网络层</li>
<li><strong>nn.ModuleDict</strong>：像python的dict一样包装多个网络层</li>
</ul>
<h5 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a>nn.Sequential</h5><p><strong>nn.Sequential</strong>是nn.Module的容器，<strong>按顺序</strong>包装一组网络层：</p>
<ul>
<li>顺序性：各网络层之间严格按照顺序构建</li>
<li>自带forward()：自带的forward里，通过for循环一次执行前向运算</li>
</ul>
<p>例1：用Sequential搭建网络</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNetSequential</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># 构建子模块</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LeNetSequential, self).__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">        	nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),)</span><br><span class="line">       	</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">        	nn.Linear(<span class="number">6</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, classes),)</span><br><span class="line">    <span class="comment"># 拼接子模块</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>通常以全连接层为界限，分为特征提取器和分类器两部分。</p>
<p>例2：对Sequential输入OrderDict</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNetSequentialOrderDict</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># 构建子模块</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LeNetSequentialOrderDict, self).__init__()</span><br><span class="line">        self.features = nn.Sequential(OrderDict(&#123;</span><br><span class="line">        	<span class="string">&#x27;conv1&#x27;</span> : nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>),</span><br><span class="line">            <span class="string">&#x27;relu1&#x27;</span> : nn.ReLU(),</span><br><span class="line">            <span class="string">&#x27;pool1&#x27;</span> : nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            </span><br><span class="line">            <span class="string">&#x27;conv2&#x27;</span> : nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            <span class="string">&#x27;relu2&#x27;</span> : nn.ReLU(),</span><br><span class="line">            <span class="string">&#x27;pool2&#x27;</span> : nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>),</span><br><span class="line">        &#125;))</span><br><span class="line">       	</span><br><span class="line">        self.classifier = nn.Sequential(OrderDict(&#123;</span><br><span class="line">        	<span class="string">&#x27;fc1&#x27;</span> : nn.Linear(<span class="number">6</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>),</span><br><span class="line">            <span class="string">&#x27;relu3&#x27;</span> : nn.ReLU(),</span><br><span class="line">            <span class="string">&#x27;fc2&#x27;</span> : nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            <span class="string">&#x27;relu4&#x27;</span> : nn.ReLU(),</span><br><span class="line">            <span class="string">&#x27;fc3&#x27;</span> : nn.Linear(<span class="number">84</span>, classes),</span><br><span class="line">        &#125;))</span><br><span class="line">    <span class="comment"># 拼接子模块</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h5 id="nn-ModuleList"><a href="#nn-ModuleList" class="headerlink" title="nn.ModuleList"></a>nn.ModuleList</h5><p>nn.ModuleList是nn.Module的容器，用于包装一组网络层，以迭代方式调用一组网络层。</p>
<p><strong>主要方法</strong>：</p>
<ul>
<li>append()：在ModuleList后面添加网络层</li>
<li>extend()：拼接两个ModuleList</li>
<li>insert()：指定在ModuleList中位置插入网络层</li>
</ul>
<p>例1：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModuleList</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ModuleList, self).__init__()</span><br><span class="line">        self.linears = nn.ModuleList([nn.Linear(<span class="number">10</span>, <span class="number">10</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>)])</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i, linear <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.linears):</span><br><span class="line">            x = linear(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br></pre></td></tr></table></figure>

<h5 id="nn-ModuleDict"><a href="#nn-ModuleDict" class="headerlink" title="nn.ModuleDict"></a>nn.ModuleDict</h5><p><strong>nn.ModuleDict</strong>是 nn.module的容器， 用于包装一组网络层，以<strong>索引</strong>的方式调用网络层。</p>
<ul>
<li>clear()：清空ModuleDict</li>
<li>items()：返回可迭代的键值对(key-value pairs)</li>
<li>keys()：返回字典的键(key)</li>
<li>values()：返回字典的值(value)</li>
<li>pop()：返回一对键值，并从字典中删除</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModuleDict</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ModuleDict, self).__init__()</span><br><span class="line">        self.choices = nn.ModuleDict(&#123;</span><br><span class="line">            <span class="string">&#x27;conv&#x27;</span> : nn.Conv2d(<span class="number">10</span>, <span class="number">10</span>, <span class="number">3</span>),</span><br><span class="line">            <span class="string">&#x27;pool&#x27;</span> : nn.MaxPool2d(<span class="number">3</span>)</span><br><span class="line">        &#125;)</span><br><span class="line">        </span><br><span class="line">        self.activations = nn.ModuleDict(&#123;</span><br><span class="line">            <span class="string">&#x27;relu&#x27;</span> : nn.ReLU(),</span><br><span class="line">            <span class="string">&#x27;prelu&#x27;</span> : nn.PReLU()</span><br><span class="line">        &#125;)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, choice, act</span>):</span></span><br><span class="line">        x = self.choices[choise](x)</span><br><span class="line">        x = self.activations[act](x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">output = net(fake_img, <span class="string">&#x27;conv&#x27;</span>, <span class="string">&#x27;prelu&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h5 id="容器总结："><a href="#容器总结：" class="headerlink" title="容器总结："></a>容器总结：</h5><ul>
<li>nn.Sequential：<strong>顺序性</strong>，个网络层之间严格按顺序执行，常用于block构建</li>
<li>nn.ModuleList：<strong>迭代性</strong>，常用于大量重复网络构建，通过for循环实现重复构建</li>
<li>nn.ModuleDict：<strong>索引性</strong>，常用于可选择的网络层</li>
</ul>
<h4 id="2-AlexNet"><a href="#2-AlexNet" class="headerlink" title="2.AlexNet"></a>2.AlexNet</h4><p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923150239978-16326570841982.png" alt="image-20210923150239978"></p>
<p>前面卷积池化的部分组装成<strong>features</strong>，后面的全连接部分组装成<strong>classifier</strong></p>
<h3 id="C-nn网络层-卷积层"><a href="#C-nn网络层-卷积层" class="headerlink" title="C. nn网络层-卷积层"></a>C. nn网络层-卷积层</h3><h4 id="1-1d-2d-3d卷积"><a href="#1-1d-2d-3d卷积" class="headerlink" title="1. 1d/2d/3d卷积"></a>1. 1d/2d/3d卷积</h4><p>卷积运算：卷积核在输入信号（图像）上滑动，相应位置上进行<strong>乘加</strong>。</p>
<p>卷积核：又称为滤波器，过滤器，可认为是某种模式，特征。</p>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923195219275-16326570841984.png" alt="image-20210923195219275"></p>
<p>卷积过程类似于用一个模板去图像上寻找与他类似的区域，与卷积核模式越相似，激活值越高，从而实现特征提取。</p>
<p>AlexNet卷积核可视化（如下图），发现卷积核学习到的是边缘，条纹，色彩着一些细节模式。<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923195435689-16326570841985.png" alt="image-20210923195435689" style="zoom:50%;"></p>
<p><strong>卷积维度</strong>：一般情况下，卷积核<strong>在几个维度下滑动，就是几维卷积</strong>。</p>
<p>一个卷积核在一个信号上的几维卷积。</p>
<p>一维卷积：</p>
<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923201126680-16326570841987.png" alt="image-20210923201126680" style="zoom:33%;">

<p>二维卷积：</p>
<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923201301917-16326570841986.png" alt="image-20210923201301917" style="zoom:33%;">

<p>三维卷积：                        <img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923201323469-16326570841988.png" alt="image-20210923201323469" style="zoom:33%;">、</p>
<p>nn.Conv2d</p>
<p>对多个二维信号进行二维卷积</p>
<p>主要参数：</p>
<ul>
<li>in_channels：输入通道数</li>
<li>out_channels：输出通道数，等价于卷积核个数</li>
<li>kernel_size：卷积核尺寸</li>
<li>stride：步长（每次滑动经过多少像素）</li>
<li>padding：填充个数（保持输入和输出尺寸匹配）</li>
<li>dilation：孔洞卷积大小（常用于图像分割任务，用于提升感受区域）</li>
<li>groups：分组卷积设置</li>
<li>bias：偏置</li>
</ul>
<p>图像尺寸变化：</p>
<p>（简化版）：outsize = （Insize - kernelsize）/ stride + 1</p>
<p>（完整版）：<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923202218467-16326570841989.png" alt="image-20210923202218467"></p>
<h4 id="2-卷积-nn-Conv2d"><a href="#2-卷积-nn-Conv2d" class="headerlink" title="2. 卷积-nn.Conv2d()"></a>2. 卷积-nn.Conv2d()</h4><p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923205449306-163265708419810.png" alt="image-20210923205449306"></p>
<h4 id="3-转置卷积-nn-ConvTranspose"><a href="#3-转置卷积-nn-ConvTranspose" class="headerlink" title="3.转置卷积-nn.ConvTranspose"></a>3.转置卷积-nn.ConvTranspose</h4><p>转置卷积又称为反卷积（Deconvolution)和部分跨越卷积（Fractionally strided Convolution），用于对图像进行上采样（UpSample）。</p>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923210001350-163265708419811.png" alt="image-20210923210001350"></p>
<p>nn.Transpose2d</p>
<p>转置卷积实现上采样</p>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923210527210-163265708419812.png" alt="image-20210923210527210"></p>
<p>主要参数：</p>
<ul>
<li>in_channels：输入通道数</li>
<li>out_channels：输出通道数，等价于卷积核个数</li>
<li>kernel_size：卷积核尺寸</li>
<li>stride：步长（每次滑动经过多少像素）</li>
<li>padding：填充个数（保持输入和输出尺寸匹配）</li>
<li>dilation：孔洞卷积大小（常用于图像分割任务，用于提升感受区域）</li>
<li>groups：分组卷积设置</li>
<li>bias：偏置</li>
</ul>
<p>转置卷积的尺寸计算：</p>
<p>（简化版）outsize = （insize - 1）*stride + kernelsize</p>
<p>（完整版）：<br><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923210734836-163265708419814.png" alt="image-20210923210734836"></p>
<h3 id="D-池化、线性、激活函数层"><a href="#D-池化、线性、激活函数层" class="headerlink" title="D. 池化、线性、激活函数层"></a>D. 池化、线性、激活函数层</h3><h4 id="1-池化层——Pooling-Layer"><a href="#1-池化层——Pooling-Layer" class="headerlink" title="1. 池化层——Pooling Layer"></a>1. 池化层——Pooling Layer</h4><p>池化运算：对信号进行 <strong>“收集”</strong> 并 <strong>“总结”</strong>，类似水池收集水资源，因而得名池化层。</p>
<p><strong>“收集”</strong>：多变少</p>
<p><strong>“总结”</strong>：最大值 / 平均值</p>
<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923211736432-163265708419813.png" alt="image-20210923211736432" style="zoom:50%;">

<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923211835083-163265708419815.png" alt="image-20210923211835083" style="zoom:50%;">

<h5 id="nn-MaxPool2d"><a href="#nn-MaxPool2d" class="headerlink" title="nn.MaxPool2d"></a>nn.MaxPool2d</h5><p>对二维信号（图像）进行最大值池化</p>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923212056580-163265708419916.png" alt="image-20210923212056580"></p>
<p>主要参数：</p>
<ul>
<li>kernel_size：池化核尺寸</li>
<li>stride：步长（步长通常与窗口大小一样）</li>
<li>padding：填充个数</li>
<li>dilation：池化核间隔大小</li>
<li>ceil_mode：尺寸向上取整</li>
<li>return_indices：记录池化像素索引（记录最大值在的位置，在反池化的时候把最大值放在对应的位置上）</li>
</ul>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923212830427-163265708419917.png" alt="image-20210923212830427"></p>
<h5 id="nn-AvgPool2d"><a href="#nn-AvgPool2d" class="headerlink" title="nn.AvgPool2d"></a>nn.AvgPool2d</h5><p>对二维信号（图像）进行平均值池化</p>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923213658337-163265708419918.png" alt="image-20210923213658337"></p>
<p>主要参数：</p>
<ul>
<li>kernel_size：池化核尺寸</li>
<li>stride：步长（步长通常与窗口大小一样）</li>
<li>padding：填充个数</li>
<li>ceil_mode：尺寸向上取整</li>
<li>count_include_pad：填充值用于计算</li>
<li>divisor_override：除法因子</li>
</ul>
<h5 id="nn-MaxUnpool2d"><a href="#nn-MaxUnpool2d" class="headerlink" title="nn.MaxUnpool2d"></a>nn.MaxUnpool2d</h5><p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923221525532-163265708419919.png" alt="image-20210923221525532"></p>
<p>对二维信号（图像）进行最大值池化上采样</p>
<p>主要参数：</p>
<ul>
<li>kernel_size：池化核尺寸</li>
<li>stride：步长</li>
<li>padding：填充个数</li>
</ul>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923212830427-163265708419917.png" alt="image-20210923212830427"></p>
<p>在forward( )的时候要传入索引层，就是indices。</p>
<h4 id="2-线性层——Linear-Layer"><a href="#2-线性层——Linear-Layer" class="headerlink" title="2. 线性层——Linear Layer"></a>2. 线性层——Linear Layer</h4><p>线性层又称全连接层，其<strong>每个神经元与上一层所有神经元相连</strong>。</p>
<p>实现对前一层的<strong>线性组合，线性变换</strong></p>
<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923222244138-163265708419920.png" alt="image-20210923222244138" style="zoom:25%;">

<h5 id="nn-Linear"><a href="#nn-Linear" class="headerlink" title="nn.Linear"></a>nn.Linear</h5><p>对一维信号（向量）进行线性组合</p>
<p><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923222813962-163265708419921.png" alt="image-20210923222813962"></p>
<p>主要参数：</p>
<ul>
<li>in_features：输入结点数</li>
<li>out_features：输出结点数</li>
<li>bias：是否需要偏置</li>
</ul>
<p>计算公式：<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923222749821-163265708419922.png" alt="image-20210923222749821" style="zoom:50%;"></p>
<h4 id="3-激活函数层——Activation-Layer"><a href="#3-激活函数层——Activation-Layer" class="headerlink" title="3. 激活函数层——Activation Layer"></a>3. 激活函数层——Activation Layer</h4><p>激活函数对特征进行<strong>非线性变换</strong>，赋予多层神经网络具有<strong>深度意义</strong>。</p>
<p>如果<strong>没有</strong>非线性变化，<strong>无论</strong>多少层线性层叠加都没有意义，一个矩阵就都能搞定。</p>
<h5 id="nn-Sigmoid"><a href="#nn-Sigmoid" class="headerlink" title="nn.Sigmoid"></a>nn.Sigmoid</h5><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923223344685-163265708419923.png" alt="image-20210923223344685" style="zoom:33%;">

<p>特性：</p>
<ul>
<li>输出值在（0，1），符合概率</li>
<li>导数范围是[0, 0.25]，容易梯度消失</li>
<li>输出为非0均值，破坏数据分布</li>
</ul>
<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923223502929-163265708419924.png" alt="image-20210923223502929" style="zoom: 25%;">

<h5 id="nn-tanh"><a href="#nn-tanh" class="headerlink" title="nn.tanh"></a>nn.tanh</h5><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923223628333-163265708419925.png" alt="image-20210923223628333" style="zoom: 50%;">

<p>特性：</p>
<ul>
<li>输出值在（-1，1），数据符合0均值</li>
<li>导数范围是（0，1），易导致梯度消失</li>
</ul>
<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923223736195-163265708419926.png" alt="image-20210923223736195" style="zoom: 25%;">

<h5 id="nn-ReLU"><a href="#nn-ReLU" class="headerlink" title="nn.ReLU"></a>nn.ReLU</h5><img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923223932829-163265708419927.png" alt="image-20210923223932829" style="zoom: 33%;">

<p>特性：</p>
<ul>
<li>输出值均为正数，负半轴导致死神经元（有一部分神经元就死掉了）</li>
<li>导数是1（不改变梯度的尺度），缓解梯度消失，但易引发梯度爆炸</li>
</ul>
<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923224046062-163265708419928.png" alt="image-20210923224046062" style="zoom:25%;">

<h5 id="nn-ReLU变式"><a href="#nn-ReLU变式" class="headerlink" title="nn.ReLU变式"></a>nn.ReLU变式</h5><p>nn.LeakyReLU</p>
<ul>
<li>negative_slope：负半轴斜率</li>
</ul>
<p>nn.PReLU</p>
<ul>
<li>init：可学习斜率</li>
</ul>
<p>nn.RReLU</p>
<p>R代表random</p>
<ul>
<li>lower：均匀分布下限</li>
<li>upper：均匀分布上限</li>
</ul>
<img src="/2021/09/26/PyTorch-ModelsConstruction/image-20210923224438706-163265708419929.png" alt="image-20210923224438706" style="zoom:25%;">

<p><strong>还有很多其他改进</strong>，日后补充</p>
]]></content>
      <categories>
        <category>PyTorch框架学习</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch-NormalizationMethods</title>
    <url>/2021/09/29/PyTorch-NormalizationMethods/</url>
    <content><![CDATA[<h2 id="六-PyTorch正则化"><a href="#六-PyTorch正则化" class="headerlink" title="六. PyTorch正则化"></a>六. PyTorch正则化</h2><h3 id="A-正则化之weight-decay"><a href="#A-正则化之weight-decay" class="headerlink" title="A. 正则化之weight decay"></a>A. 正则化之weight decay</h3><h4 id="1-正则化与偏差-方差分解"><a href="#1-正则化与偏差-方差分解" class="headerlink" title="1.正则化与偏差-方差分解"></a>1.正则化与偏差-方差分解</h4><p>Regularization：减小<strong>方差</strong>的策略。</p>
<p>误差可分解为：偏差，方差与噪音之和。即<strong>误差 = 偏差 + 方差 + 噪音</strong>之和。</p>
<ul>
<li><strong>偏差</strong>度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；</li>
<li><strong>方差</strong>度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；</li>
<li><strong>噪音</strong>则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界</li>
</ul>
<p>通常不考虑噪声，<strong>方差</strong>是<strong>训练集与验证集</strong>之间的差异。Regularization就是拿来解决方差过大的问题，也就是过拟合现象。</p>
<p><img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210928144756798.png" alt="image-20210928144756798"></p>
<p>L1和L2正则项的表达式如下图所示：</p>
<p><img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210928144900258.png" alt="image-20210928144900258"></p>
<p>L1容易获得稀疏分布。</p>
<p>L1和L2都可以让参数值比较小，模型不会很复杂。</p>
<h4 id="2-PyTorch中的L2正则项-weight-decay"><a href="#2-PyTorch中的L2正则项-weight-decay" class="headerlink" title="2.PyTorch中的L2正则项-weight decay"></a>2.PyTorch中的L2正则项-weight decay</h4><p>L2 Regularization = weight decay(权值衰减)</p>
<p>目标函数（Objective Function）：</p>
<p>Obj = Cost + Regularization Term</p>
<p><img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210928150418596.png" alt="image-20210928150418596"></p>
<p>加入之后很明显Wi发生了一个权值衰减。</p>
<p><img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210928150700661.png" alt="image-20210928150700661"></p>
<p>通过直接向SGD递参数选择是否weight decay：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optim_wdecay = torch.optim.SGD(net_weight_decay.parameters(), lr=lr_init, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1e-2</span>)</span><br></pre></td></tr></table></figure>

<h3 id="B-正则化之dropout"><a href="#B-正则化之dropout" class="headerlink" title="B. 正则化之dropout"></a>B. 正则化之dropout</h3><h4 id="1-Dropout概念"><a href="#1-Dropout概念" class="headerlink" title="1. Dropout概念"></a>1. Dropout概念</h4><p>Dropout：随机失活</p>
<p><img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210928170616222.png" alt="image-20210928170616222"></p>
<ul>
<li>随机：dropout probability</li>
<li>失活：weight = 0</li>
</ul>
<p><strong>数据尺度变化</strong>：测试时，所有权重乘以1-drop_prob。drop_prob = 0.3， 1- drop_prob = 0.7</p>
<p>每一次前向传播，训练的模型都是不一样的，随机失活一批神经元。</p>
<p>假设一个神经元接受上一层的五个输出值，假设其特别依赖第一个特征，别的神经元权值就很小。加了dropout之后，就不知道上一层哪个神经元会出现，就不会特别依赖某一个输出值，所以就不会给某一个神经元过大的权重。基本上就是一会平均对每一道特征的依赖。</p>
<h4 id="2-Dropout注意事项"><a href="#2-Dropout注意事项" class="headerlink" title="2. Dropout注意事项"></a>2. Dropout注意事项</h4><p>nn.Dropout()</p>
<p>功能：Dropout层</p>
<p><img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210928172047828.png" alt="image-20210928172047828"></p>
<p>参数：</p>
<ul>
<li>p：被舍弃的概率，失活概率</li>
</ul>
<p>使用方法：</p>
<p>直接在模型的init里面扔nn.Dropout(d_prob)，放在需要dropout网络层的前一层。</p>
<p><strong>实现细节：</strong>训练时权重均乘以1/1-p，即除以1-p。</p>
<h3 id="C-Batch-Normalization"><a href="#C-Batch-Normalization" class="headerlink" title="C. Batch Normalization"></a>C. Batch Normalization</h3><h4 id="1-Batch-Normalization概念"><a href="#1-Batch-Normalization概念" class="headerlink" title="1. Batch Normalization概念"></a>1. Batch Normalization概念</h4><p>概念：批标准化（广泛使用，自2015年问世）</p>
<ul>
<li>批：一批数据，通常为mini-batch</li>
<li>标准化：0均值，1方差</li>
</ul>
<p>优点：</p>
<ul>
<li>可以<strong>用</strong>更大的学习率，加速模型的收敛</li>
<li>可以<strong>不用</strong>精心设计权值初始化</li>
<li>可以<strong>不用</strong>dropout或较小的dropout</li>
<li>可以<strong>不用</strong>L2或者较小的weight decay</li>
<li>可以<strong>不用</strong>LRN（local response normalization）</li>
</ul>
<p><img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210928174311292.png" alt="image-20210928174311292"></p>
<p>最后一步是affine transform，增强Capacity。scale&amp;shift，并且γ和β都是可学习参数。</p>
<h4 id="2-PyTorch的Batch-Normalization-1d-2d-3d实现"><a href="#2-PyTorch的Batch-Normalization-1d-2d-3d实现" class="headerlink" title="2. PyTorch的Batch Normalization 1d/2d/3d实现"></a>2. PyTorch的Batch Normalization 1d/2d/3d实现</h4><p>在网络的init模块中定义nn.BatchNorm2d/1d/3d。</p>
<h5 id="BatchNorm（所有batchnorm的实现都得继承它）"><a href="#BatchNorm（所有batchnorm的实现都得继承它）" class="headerlink" title="_BatchNorm（所有batchnorm的实现都得继承它）"></a>_BatchNorm（所有batchnorm的实现都得继承它）</h5><p><img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210928204020955.png" alt="image-20210928204020955"></p>
<ul>
<li>nn.BatchNorm1d input = B*特征数（几个神经元几个特征） * 1d特征</li>
<li>nn.BatchNorm2d input = B*特征数 * 2d特征 </li>
<li>nn.BatchNorm3d input = B*特征数 * 3d特征</li>
</ul>
<p>xd特征就是指输入tensor是x维。</p>
<p>参数：</p>
<ul>
<li>num_features：一个样本特征数量(最重要)</li>
<li>eps：分母修正项</li>
<li>momentum：指数加权平均估计当前mean/var</li>
<li>affine：是否需要affine transform</li>
<li>track_running_stats：是训练状态，还是测试状态</li>
</ul>
<p><img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210928204237860.png" alt="image-20210928204237860"></p>
<p>主要属性：</p>
<ul>
<li>running_mean：均值</li>
<li>running_var：方差</li>
<li>weight：affine transform中的γ</li>
<li>bias：affine transform中的β</li>
</ul>
<p>每一个维度都会有自己的四个参数值。</p>
<p><img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210928204609151.png" alt="image-20210928204609151"></p>
<p>训练：均值和方差采用指数加权平均计算</p>
<p>测试：当前统计值</p>
<h3 id="D-BN、LN、IN-and-GN"><a href="#D-BN、LN、IN-and-GN" class="headerlink" title="D. BN、LN、IN and GN"></a>D. BN、LN、IN and GN</h3><h4 id="1-为什么要Normalization"><a href="#1-为什么要Normalization" class="headerlink" title="1. 为什么要Normalization"></a>1. 为什么要Normalization</h4><p>Internal Covariate Shift（ICS）：数据尺度/分布一场，导致训练困难。</p>
<p>进行normalization可以约束数据的尺度和分布。</p>
<p>常见的normalization方法：</p>
<h4 id="2-Batch-Normalization（BN）"><a href="#2-Batch-Normalization（BN）" class="headerlink" title="2. Batch Normalization（BN）"></a>2. Batch Normalization（BN）</h4><h4 id="3-Layer-Normalization（LN）"><a href="#3-Layer-Normalization（LN）" class="headerlink" title="3. Layer Normalization（LN）"></a>3. Layer Normalization（LN）</h4><p>起因：BN不适用于变长的网络，如RNN</p>
<p>思路：<strong>逐层</strong>计算均值和方差</p>
<img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210928233554595.png" alt="image-20210928233554595" style="zoom:33%;">

<p>注意事项：</p>
<p>不再有running_mean和running_var</p>
<p>gamma和beta为逐元素（特征）的</p>
<h5 id="nn-LayerNorm"><a href="#nn-LayerNorm" class="headerlink" title="nn.LayerNorm"></a>nn.LayerNorm</h5><p><img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210928233745338.png" alt="image-20210928233745338"></p>
<p>主要参数：</p>
<ul>
<li>normalized_shape：该层特征形状</li>
<li>eps：分母修正项</li>
<li>elementwise_affine：是否需要affine transform</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ln = nn.LayerNorm(feature_maps_bs.size()[<span class="number">1</span>:], elementwise_affine=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>对于feature_map_bs.size的设置是有规矩的：</p>
<p>假设feature_maps_bs的形状是 [8, 6, 3, 4], B * C * H * W</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ln = nn.LayerNorm([<span class="number">4</span>])</span><br><span class="line">ln = nn.LayerNorm([<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">ln = nn.LayerNorm([<span class="number">6</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">ln = nn.LayerNorm([<span class="number">8</span>,<span class="number">6</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"><span class="comment"># size从后往前必须与输入数据吻合</span></span><br></pre></td></tr></table></figure>



<h4 id="4-Instance-Normalization（IN）"><a href="#4-Instance-Normalization（IN）" class="headerlink" title="4. Instance Normalization（IN）"></a>4. Instance Normalization（IN）</h4><p>起因：BN在图像生成（Image Generation）中不适用</p>
<p>思路：逐Instance（channel通道）计算均值和方差。</p>
<p>在每一个<strong>特征通道</strong>里面计算方差和均值。</p>
<img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210929013113431.png" alt="image-20210929013113431" style="zoom:33%;">

<h5 id="nn-InstanceNorm"><a href="#nn-InstanceNorm" class="headerlink" title="nn.InstanceNorm"></a>nn.InstanceNorm</h5><p><img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210929013529249.png" alt="image-20210929013529249"></p>
<p>主要参数：</p>
<ul>
<li>num_features：一个样本特征数量（更重要）</li>
<li>eps：分母修正项</li>
<li>momentum：指数加权平均估计当前mean/var</li>
<li>affine：是否需要affine transform</li>
<li>track_running_stats：是训练状态，还是测试状态</li>
</ul>
<h4 id="5-Group-Normalization（GN）"><a href="#5-Group-Normalization（GN）" class="headerlink" title="5. Group Normalization（GN）"></a>5. Group Normalization（GN）</h4><p>起因：小batch样本中，BN估计的值不准</p>
<p>思路：数据不够，通道来凑</p>
<p>注意事项：</p>
<ol>
<li>不再有running_mean和running_var</li>
<li>gamma和beta为逐通道（channel）的</li>
</ol>
<p>应用场景：大模型（小batch size）任务。</p>
<p>GPU吃不下太多的batchsize，所有均值和方差估计不准，BN失效。</p>
<h5 id="nn-GroupNorm"><a href="#nn-GroupNorm" class="headerlink" title="nn.GroupNorm"></a>nn.GroupNorm</h5><p><img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210929014311324.png" alt="image-20210929014311324"></p>
<p>主要参数：</p>
<ul>
<li>num_groups：分组数，通道设置为2的n次方，根据通道图设置个合适的。</li>
<li>num_channels：通道数（特征数）。除以组数就是每个group有几个通道。</li>
<li>eps：分母修正项</li>
<li>affine：是否需要affine transform</li>
</ul>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>BN、LN、IN和GN都是为了客服Internal Covariate Shift（ICS）</p>
<p><img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210929014606099.png" alt="image-20210929014606099"></p>
<p>*Group Norm其实应该是四维。</p>
<p>BN：</p>
<img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210929014751878.png" alt="image-20210929014751878" style="zoom:33%;">

<p>LN：</p>
<p>按layer计算</p>
<img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210929014847346.png" alt="image-20210929014847346" style="zoom:33%;">

<p>IN：</p>
<p>一般用于图像生成，保留风格。</p>
<p><img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210929015053479.png" alt="image-20210929015053479"></p>
<p>GN：</p>
<p>网络太大，batch太小。</p>
<p><img src="/2021/09/29/PyTorch-NormalizationMethods/image-20210929015248450.png" alt="image-20210929015248450"></p>
<p>减均值除以标准差乘γ加β，所有normalization计算都一样。</p>
]]></content>
      <categories>
        <category>PyTorch框架学习</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PythonLibraries</title>
    <url>/2021/09/27/PythonLabraries/</url>
    <content><![CDATA[<h1 id="一些好用且常用的库"><a href="#一些好用且常用的库" class="headerlink" title="一些好用且常用的库"></a>一些好用且常用的库</h1><h2 id="collections"><a href="#collections" class="headerlink" title="collections"></a>collections</h2><h3 id="namedtuple"><a href="#namedtuple" class="headerlink" title="namedtuple()"></a>namedtuple()</h3><p>collections模块的namedtuple子类不仅可以使用索引 (index) 访问item，还可以通过类似字典的方式（key-value）进行访问。在仿真引擎中对机械臂进行控制建模时，需要对同一个关节有多维度的设定，此时namedtuple用起来就很方便。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 适用于仿真中处理描述机器人运动状态和设定的各种参数</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line">author = namedtuple(<span class="string">&#x27;author&#x27;</span>, [<span class="string">&#x27;firstname&#x27;</span>, <span class="string">&#x27;lastname&#x27;</span>])</span><br><span class="line">me = author(Yiting, Chen)</span><br><span class="line"><span class="built_in">print</span> me.firstname, me.lastname</span><br><span class="line"><span class="built_in">print</span> me[<span class="number">0</span>], me[<span class="number">1</span>]</span><br><span class="line">me = author._make([Eating, Chen])</span><br><span class="line"><span class="built_in">print</span> me.firstname, me.lastname</span><br><span class="line">me = me._replace(firstname = Eating716)</span><br><span class="line"><span class="built_in">print</span> me.firstname, me.lastname</span><br><span class="line"></span><br><span class="line"><span class="comment"># results</span></span><br><span class="line"><span class="comment"># Yiting Chen</span></span><br><span class="line"><span class="comment"># Yiting Chen</span></span><br><span class="line"><span class="comment"># Eating Chen</span></span><br><span class="line"><span class="comment"># Eating716 Chen</span></span><br><span class="line"> </span><br><span class="line">websites = [</span><br><span class="line">    (<span class="string">&#x27;Sohu&#x27;</span>, <span class="string">&#x27;http://www.sohu.com/&#x27;</span>, <span class="string">u&#x27;张朝阳&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;Baidu&#x27;</span>, <span class="string">&#x27;http://www.baidu.com.cn/&#x27;</span>, <span class="string">u&#x27;李彦宏&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;163&#x27;</span>, <span class="string">&#x27;http://www.163.com/&#x27;</span>, <span class="string">u&#x27;丁磊&#x27;</span>)</span><br><span class="line">]</span><br><span class="line"> </span><br><span class="line">Website = namedtuple(<span class="string">&#x27;Website&#x27;</span>, [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;url&#x27;</span>, <span class="string">&#x27;founder&#x27;</span>])</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> website <span class="keyword">in</span> websites:</span><br><span class="line">    website = Website._make(website)</span><br><span class="line">    <span class="built_in">print</span> website</span><br><span class="line">    </span><br><span class="line"><span class="comment"># results:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Website(name=&#x27;Sohu&#x27;, url=&#x27;http://www.sohu.com/&#x27;, founder=u&#x27;\u5f20\u671d\u9633&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Website(name=&#x27;Baidu&#x27;, url=&#x27;http://www.baidu.com.cn/&#x27;, founder=u&#x27;\u738b\u5fd7\u4e1c&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Website(name=&#x27;163&#x27;, url=&#x27;http://www.163.com/&#x27;, founder=u&#x27;\u4e01\u78ca&#x27;)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>PythonCoding</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Prologue💬</title>
    <url>/2021/09/26/SayHi/</url>
    <content><![CDATA[<p><strong><font size="10">Hi, this is Yiting Chen🚬. </font></strong></p>
<p><strong><font size="6">I’m an amateur in Robotics &amp; ML🤖. </font></strong></p>
<p><strong><font size="4">Wishing to make some progress in this area📈 </font></strong></p>
<p><strong><font size="6">This blog is for recording my daily study📝.</font></strong></p>
<p><strong><font size="4">Please let me 📬(<a href="mailto:&#x63;&#x68;&#x65;&#x6e;&#121;&#x69;&#x74;&#105;&#110;&#x67;&#64;&#119;&#104;&#117;&#46;&#101;&#x64;&#x75;&#x2e;&#x63;&#110;">&#x63;&#x68;&#x65;&#x6e;&#121;&#x69;&#x74;&#105;&#110;&#x67;&#64;&#119;&#104;&#117;&#46;&#101;&#x64;&#x75;&#x2e;&#x63;&#110;</a>) know if there is any question. </font></strong></p>
]]></content>
  </entry>
  <entry>
    <title>用随机梯度下降来优化人生</title>
    <url>/2021/09/27/limu-SGD/</url>
    <content><![CDATA[<p>作者：李沐 | 来源：知乎</p>
<p><strong>要有目标</strong>。你需要有目标。短的也好，长的也好。认真定下的也好，别人那里捡的也好。就跟随机梯度下降需要有个目标函数一样。</p>
<p><strong>目标要大</strong>。不管是人生目标还是目标函数，你最好不要知道最后可以走到哪里。如果你知道，那么你的目标就太简单了，可能是个凸函数。你可以在一开始的时候给自己一些小目标，例如期末考个80分，训练一个线性模型。但接下来得有更大的目标，财富自由也好，100亿参数的变形金刚也好，得足够一颗赛艇。</p>
<p><strong>坚持走</strong>。不管你的目标多复杂，随机梯度下降都是最简单的。每一次你找一个大概还行的方向（梯度），然后迈一步（下降）。两个核心要素是方向和步子的长短。但最重要的是你得一直走下去，能多走几步就多走几步。</p>
<p><strong>痛苦的卷</strong>。每一步里你都在试图改变你自己或者你的模型参数。改变带来痛苦。但没有改变就没有进步。你过得很痛苦不代表在朝着目标走，因为你可能走反了。但过得很舒服那一定在原地踏步。需要时刻跟自己作对。</p>
<p><strong>可以躺平</strong>。你用你内心的激情来迈步子。步子太小走不动，步子太长容易过早消耗掉了激情。周期性的调大调小步长效果挺好。所以你可以时不时休息休息。</p>
<p><strong>四处看看</strong>。每一步走的方向是你对世界的认识。如果你探索的世界不怎么变化，那么要么你的目标太简单，要么你困在你的舒适区了。随机梯度下降的第一个词是随机，就是你需要四处走走，看过很多地方，做些错误的决定，这样你可以在前期迈过一些不是很好的舒适区。</p>
<p><strong>快也是慢</strong>。你没有必要特意去追求找到最好的方向和最合适的步子。你身边当然会有幸运之子，他们每一步都在别人前面。但经验告诉我们，随机梯度下降前期进度太快，后期可能乏力。就是说你过早的找到一个舒适区，忘了世界有多大。所以你不要急，前面徘徊一段时间不是坏事。成名无需太早。</p>
<p><strong>赢在起点</strong>。起点当然重要。如果你在终点附近起步，可以少走很多路。而且终点附近的路都比较平，走着舒服。当你发现别人不如你的时候，看看自己站在哪里。可能你就是运气很好，赢在了起跑线。如果你跟别人在同一起跑线，不见得你能做更好。</p>
<p><strong>很远也能到达</strong>。如果你是在随机起点，那么做好准备前面的路会非常不平坦。越远离终点，越人迹罕见。四处都是悬崖。但随机梯度下降告诉我们，不管起点在哪里，最后得到的解都差不多。当然这个前提是你得一直按照梯度的方向走下去。如果中间梯度炸掉了，那么你随机一个起点，调整步子节奏，重新来。</p>
<p><strong>独一无二</strong>。也许大家有着差不多的目标，在差不多的时间毕业买房结婚生娃。但每一步里，每个人内心中看到的世界都不一样，导致走的路不一样。你如果跑多次随机梯度下降，在各个时间点的目标函数值可能都差不多，但每次的参数千差万别。不会有人关心你每次训练出来的模型里面参数具体是什么值，除了你自己。</p>
<p><strong>简单最好</strong> 。当然有比随机梯度下降更复杂的算法。他们想每一步看想更远更准，想步子迈最大。但如果你的目标很复杂，简单的随机梯度下降反而效果最好。深度学习里大家都用它。关注当前，每次抬头瞄一眼世界，快速做个决定，然后迈一小步。小步快跑。只要你有目标，不要停，就能到达。</p>
]]></content>
      <categories>
        <category>杂文</category>
      </categories>
      <tags>
        <tag>杂文</tag>
      </tags>
  </entry>
  <entry>
    <title>面向对象编程</title>
    <url>/2021/09/27/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="面向对象编程"><a href="#面向对象编程" class="headerlink" title="面向对象编程"></a>面向对象编程</h2><h3 id="面向对象的思想"><a href="#面向对象的思想" class="headerlink" title="面向对象的思想"></a>面向对象的思想</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我们现在先整个类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">human</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, nation</span>):</span></span><br><span class="line">        self.planet = earth</span><br><span class="line">        self.nation = nation</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">whereufrom</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;I come from %s&quot;</span> % self.nation)</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Yiting</span>(<span class="params">human</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, age, surname, nation</span>):</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">super</span>(<span class="params">Yiting, self</span>).<span class="title">__init__</span>(<span class="params">nation</span>)</span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">age</span> = <span class="title">age</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">surname</span> = <span class="title">surname</span></span></span><br><span class="line"><span class="function">    	<span class="title">self</span>.<span class="title">firstname</span> = &#x27;<span class="title">Yiting</span>&#x27;</span></span><br><span class="line"><span class="function">        </span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">eating</span>(<span class="params">self, food</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Yiting is eating&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">whoamI</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;My name is Yiting &#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.surname))</span><br><span class="line">        <span class="keyword">return</span> self.firstname + self.surname</span><br><span class="line">    </span><br><span class="line">me = Yiting(<span class="number">21</span>, Chen, China)</span><br></pre></td></tr></table></figure>



<ul>
<li><p><strong>类(Class):</strong> 用来描述具有相同的属性和方法的对象的集合。它定义了该集合中每个对象所共有的属性和方法。对象是类的实例。<strong>实例化</strong>创建一个类的实例，类的具体对象。例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">me = Yiting(<span class="number">21</span>, Chen)</span><br></pre></td></tr></table></figure></li>
<li><p><strong>类变量：</strong>类变量在整个实例化的对象中是公用的。类变量定义在类中且在函数体之外。类变量通常不作为实例变量使用。可以使用点号 <strong>.</strong> 来访问对象的属性。使用如下类的名称访问类变量:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Yiting.age, Yiting.surname</span><br><span class="line"><span class="comment"># 21, Chen</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>方法重写：</strong>如果<strong>从父类继承的方法不能满足子类的需求</strong>，可以对其进行改写，这个过程叫方法的覆盖（override），也称为方法的重写。</p>
</li>
<li><p><strong>局部变量：</strong>定义在方法中的变量，<strong>只作用于当前实例</strong>的类。</p>
</li>
</ul>
<ul>
<li><strong>实例变量：</strong>在类的声明中，属性是用变量来表示的。这种变量就称为实例变量，是在类声明的内部但是在类的其他成员方法之外声明的。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Yiting.age, Yiting.surname, etc.</span><br></pre></td></tr></table></figure>



<ul>
<li><strong>继承：</strong>即一个派生类（derived class）继承基类（base class）的字段和方法。继承也允许把一个派生类的对象作为一个基类对象对待。例如，有这样一个设计：一个Dog类型的对象派生自Animal类，这是模拟”是一个（is-a）”关系（例如Dog是一个Animal）。Python 总是<strong>首先查找对应类型的方法，如果它不能在派生类中找到对应的方法，它才开始到基类中逐个查找。</strong>（先在本类中查找调用的方法，找不到才去基类中找）。</li>
</ul>
<ul>
<li><p><strong>方法：</strong>类中定义的函数。__ init __()方法是一种特殊的方法，被称为类的构造函数或初始化方法，当创建了这个类的实例时就会调用该方法</p>
</li>
<li><p><strong>对象：</strong>通过类定义的数据结构实例。对象包括两个数据成员（类变量和实例变量）和方法。例如me就是对象。</p>
</li>
</ul>
<h3 id="类的私有（保护）属性和方法"><a href="#类的私有（保护）属性和方法" class="headerlink" title="类的私有（保护）属性和方法"></a>类的私有（保护）属性和方法</h3><p>__ private_attrs：两个下划线开头，声明该属性为私有，<strong>不能在类的外部被使用或直接访问。</strong>在类内部的方法中使用时：self.__private_attrs。</p>
<p>__ private_method：两个下划线开头，声明该方法为私有方法，<strong>不能在类的外部调用。</strong>在类的内部调用 self. __private_methods</p>
<p>_foo: 以单下划线开头的表示的是 protected 类型的变量，<strong>即保护类型只能允许其本身与子类进行访问</strong>，不能用于 <strong>from module import *</strong></p>
<h3 id="super-的使用方法"><a href="#super-的使用方法" class="headerlink" title="super()的使用方法"></a>super()的使用方法</h3><p><strong>super()</strong> 函数是用于调用父类(超类)的一个方法。</p>
<p>注意，python2.x和3.x有所不同：</p>
<ul>
<li>Python 3 可以使用直接使用 <strong>super().xxx</strong> 代替 <strong>super(Class, self).xxx</strong> </li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FooParent</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="comment"># 这个为父类</span></span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.parent = <span class="string">&#x27;I\&#x27;m the parent.&#x27;</span></span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&#x27;Parent&#x27;</span>)</span><br><span class="line">    <span class="comment"># 定义函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span>(<span class="params">self,message</span>):</span></span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;%s from Parent&quot;</span> % message)</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FooChild</span>(<span class="params">FooParent</span>):</span></span><br><span class="line">    <span class="comment"># 这个为子类</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># super(FooChild,self) 首先找到 FooChild 的父类（就是类 FooParent），然后把类 FooChild 的对象转换为类 FooParent 的对象</span></span><br><span class="line">        <span class="built_in">super</span>(FooChild,self).__init__()    </span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&#x27;Child&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span>(<span class="params">self,message</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(FooChild, self).bar(message)</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&#x27;Child bar fuction&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span> (self.parent)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    fooChild = FooChild()</span><br><span class="line">    fooChild.bar(<span class="string">&#x27;HelloWorld&#x27;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Parent</span></span><br><span class="line"><span class="comment"># Child</span></span><br><span class="line"><span class="comment"># HelloWorld from Parent</span></span><br><span class="line"><span class="comment"># Child bar fuction</span></span><br><span class="line"><span class="comment"># I&#x27;m the parent.</span></span><br></pre></td></tr></table></figure>

<p>三种情况：</p>
<p>情况一：<strong>子类需要自动调用父类的方法：</strong>子类不重写__ init __ ()方法，实例化子类后，会自动调用父类的 __ init __()的方法。</p>
<p>情况二：<strong>子类不需要自动调用父类的方法：</strong>子类重写 __ init __ ()方法，实例化子类后，将不会自动调用父类的__ init __()的方法。</p>
<p>情况三：<strong>子类重写__init__()方法又需要调用父类的方法：</strong>使用super关键词：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">super</span>(子类，self).__init__(参数<span class="number">1</span>，参数<span class="number">2</span>，....)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Son</span>(<span class="params">Father</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name</span>):</span>   </span><br><span class="line">    <span class="built_in">super</span>(Son, self).__init__(name)</span><br></pre></td></tr></table></figure>



<h2 id><a href="#" class="headerlink" title></a></h2>]]></content>
      <categories>
        <category>PythonCoding</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch-TrainingProcess</title>
    <url>/2021/09/28/PyTorch-TrainingProcess/</url>
    <content><![CDATA[<h2 id="五-PyTorch训练过程"><a href="#五-PyTorch训练过程" class="headerlink" title="五. PyTorch训练过程"></a>五. PyTorch训练过程</h2><h4 id="A-学习率调整策略"><a href="#A-学习率调整策略" class="headerlink" title="A. 学习率调整策略"></a>A. 学习率调整策略</h4><h5 id="1-为什么要调整学习率"><a href="#1-为什么要调整学习率" class="headerlink" title="1. 为什么要调整学习率"></a>1. 为什么要调整学习率</h5><p>学习率（learning rate）控制了参数更新的步伐，一般前期会比较大，后期会下降。</p>
<p><strong>就像打高尔夫。</strong></p>
<p>PyTorch提供了很好的学习率调整方法。</p>
<h5 id="2-调整方法"><a href="#2-调整方法" class="headerlink" title="2. 调整方法"></a>2. 调整方法</h5><h6 id="class-LRScheduler"><a href="#class-LRScheduler" class="headerlink" title="class _LRScheduler"></a>class _LRScheduler</h6><p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210926212016387.png" alt="image-20210926212016387"></p>
<p>主要属性：</p>
<ul>
<li>optimizer：关联的优化器（学习率放在优化器当中，所以必须关联）</li>
<li>last_epoch：记录epoch数，学习率的调整以epoch为周期</li>
<li>base_lrs：记录初始学习率</li>
</ul>
<p><strong>主要方法：</strong></p>
<ul>
<li>step()：更新下一个epoch的学习率</li>
<li>get_lr()：虚函数，计算下一个epoch的学习率。用来给子类override。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 例如：</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=<span class="number">10</span>, gamma=<span class="number">0.1</span> )</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StepLR</span>(<span class="params">_LRScheduler</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, optimizer, step_size, gamma=<span class="number">0.1</span>, last_epoch=-<span class="number">1</span></span>):</span></span><br><span class="line">        self.step_size = gamma</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        <span class="built_in">super</span>(StepLR, self).__init__(optimizer, last_epoch)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_lr</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> [base_lr * self.gamma ** (self.last_epoch//self.step_size) <span class="keyword">for</span> base_lr <span class="keyword">in</span> self.base_lrs]</span><br></pre></td></tr></table></figure>

<p>整个scheduler只出现在两行，第一次是构建，第二次是执行step()。执行step()时注意一定要<strong>放在epoch当中</strong>，而不是iteration。</p>
<h6 id="StepLR"><a href="#StepLR" class="headerlink" title="StepLR"></a>StepLR</h6><p>等间隔调整学习率</p>
<p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210926212157451.png" alt="image-20210926212157451"></p>
<p>主要参数：</p>
<ul>
<li>step_size：等间隔数</li>
<li>gamma：调整系数</li>
</ul>
<p><strong>调整方式：lr = lr * gamma</strong></p>
<h6 id="MultiStepLR"><a href="#MultiStepLR" class="headerlink" title="MultiStepLR"></a>MultiStepLR</h6><p>按给定时间调整学习率</p>
<p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210926220926788.png" alt="image-20210926220926788"></p>
<p>主要参数：</p>
<ul>
<li>milestones：设定调整时刻数（构建一个list放入，例如[10, 120, 200], 根据list数值调整）</li>
<li>gamma：调整系数</li>
</ul>
<h6 id="ExponentialLR"><a href="#ExponentialLR" class="headerlink" title="ExponentialLR"></a>ExponentialLR</h6><p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210926221144356.png" alt="image-20210926221144356"></p>
<p>按指数衰减调整学习率</p>
<p>主要参数：</p>
<ul>
<li>gamma：指数的底（通常设置为接近1的一个数）</li>
</ul>
<p>调整方式：lr = lr * gamma ** epoch</p>
<h6 id="CosineAnnealingLR"><a href="#CosineAnnealingLR" class="headerlink" title="CosineAnnealingLR"></a>CosineAnnealingLR</h6><p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210926221836558.png" alt="image-20210926221836558"></p>
<p>余弦周期调整学习率</p>
<p>主要参数：</p>
<ul>
<li>T_max：下降周期</li>
<li>eta_min：学习率的下限</li>
<li><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210926222018170.png" alt="image-20210926222018170" style="zoom: 25%;"></li>
</ul>
<p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210926221851244.png" alt="image-20210926221851244"></p>
<h6 id="ReduceLRonPlateau"><a href="#ReduceLRonPlateau" class="headerlink" title="ReduceLRonPlateau"></a>ReduceLRonPlateau</h6><p>监控指标，当指标不再变化则调整</p>
<p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210926222248004.png" alt="image-20210926222248004"></p>
<p>主要参数：</p>
<ul>
<li>mode：min/max 两种模式</li>
<li>factor：调整系数</li>
<li>patience：”耐心“，接受几次不变化</li>
<li>cooldown：”冷却时间“，停止监控一段时间</li>
<li>verbose：是否打印日志</li>
<li>min_lr：学习率的下限</li>
<li>eps：学习率衰减最小值</li>
</ul>
<h6 id="LambaLR"><a href="#LambaLR" class="headerlink" title="LambaLR"></a>LambaLR</h6><p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210926224426382.png" alt="image-20210926224426382"></p>
<p>自定义调整策略</p>
<p>主要参数：</p>
<ul>
<li>lr_lambda：function or list</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = optim.SGD([&#123;<span class="string">&#x27;params&#x27;</span> : [weights_1]&#125;, &#123;<span class="string">&#x27;params&#x27;</span>: [weights_2]&#125;], lr = lr_init)</span><br><span class="line"></span><br><span class="line">lambda1 = <span class="keyword">lambda</span> epoch: <span class="number">0.1</span> ** (epoch // <span class="number">20</span>)</span><br><span class="line">lambda2 = <span class="keyword">lambda</span> epoch: <span class="number">0.95</span> ** epoch</span><br><span class="line"></span><br><span class="line">scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])</span><br></pre></td></tr></table></figure>

<p>不同的参数组，有不同的调整策略。在finetune当中很常见。</p>
<h5 id="3-学习率调整小结"><a href="#3-学习率调整小结" class="headerlink" title="3. 学习率调整小结"></a>3. 学习率调整小结</h5><ol>
<li>有序调整：Step，MultiStep，Exponential 和 CosineAnnealing</li>
<li>自适应调整：ReduceLROnPleateau</li>
<li>自定义调整：Lambda</li>
</ol>
<p>学习率初始化：</p>
<ol>
<li>设置较小数：0.01、0.001、0.0001</li>
<li>搜索最大学习率：《Cyclical Learning Rates for Training Neural Networks》<ol>
<li><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210926225813725.png" alt="image-20210926225813725" style="zoom:33%;"></li>
<li><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210926225855652.png" alt="image-20210926225855652" style="zoom:33%;"></li>
</ol>
</li>
</ol>
<h4 id="B-可视化工具-——-Tensorboard"><a href="#B-可视化工具-——-Tensorboard" class="headerlink" title="B. 可视化工具 —— Tensorboard"></a>B. 可视化工具 —— Tensorboard</h4><h5 id="1-TensorBoard简介"><a href="#1-TensorBoard简介" class="headerlink" title="1. TensorBoard简介"></a>1. TensorBoard简介</h5><p>TensorBoard：Tensorflow中强大的可视化工具</p>
<p>支持标量、图像、文本、音频、视频和Embedding等多种数据的可视化。</p>
<p>用来监控当前训练是不是一个良好的训练状态。</p>
<p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210926230630563.png" alt="image-20210926230630563"></p>
<h5 id="2-TensorBoard安装"><a href="#2-TensorBoard安装" class="headerlink" title="2. TensorBoard安装"></a>2. TensorBoard安装</h5><p>“报错安装法”</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install future</span><br><span class="line">pip install tensorboard</span><br></pre></td></tr></table></figure>



<h5 id="3-TensorBoard运行可视化"><a href="#3-TensorBoard运行可视化" class="headerlink" title="3. TensorBoard运行可视化"></a>3. TensorBoard运行可视化</h5><p>运行机制：</p>
<p>python脚本：记录可视化的数据 ——&gt; 硬盘：event file ——&gt; 终端：tensorboard ——&gt; Web端：<img src="/2021/09/28/PyTorch-TrainingProcess/image-20210926230827260.png" alt="image-20210926230827260" style="zoom:25%;"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> summaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(comment=<span class="string">&#x27;test tensorboard&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;y=2x&#x27;</span>, x*<span class="number">2</span>,x)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;y=pow(2,x)&#x27;</span>, <span class="number">2</span>**x, x)</span><br><span class="line">    </span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;data/scalar_group&#x27;</span>, &#123;<span class="string">&quot;xsinx&quot;</span>: x*np.sin(x), <span class="string">&quot;xcosx&quot;</span>: x * np.cos(x), <span class="string">&quot;arctanx&quot;</span>: np.arctan(x)&#125;, x)</span><br><span class="line">    </span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<p>runs/文件夹下会有event file拿来可视化。cd到runs所在文件夹下，输入：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tensorboard --logdir=./runs</span><br></pre></td></tr></table></figure>

<p>之后点击终端里的网址，便在在Web中打开可视化界面。</p>
<h4 id="C-TensorBoard使用（一）"><a href="#C-TensorBoard使用（一）" class="headerlink" title="C. TensorBoard使用（一）"></a>C. TensorBoard使用（一）</h4><p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210926233724340.png" alt="image-20210926233724340"></p>
<h5 id="1-SummaryWriter"><a href="#1-SummaryWriter" class="headerlink" title="1. SummaryWriter"></a>1. SummaryWriter</h5><h6 id="SummaryWriter"><a href="#SummaryWriter" class="headerlink" title="SummaryWriter"></a>SummaryWriter</h6><p>提供创建event file的高级接口：</p>
<p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210926234221840.png" alt="image-20210926234221840"></p>
<p>主要属性：</p>
<ul>
<li>log_dir：event file输出文件夹</li>
<li>comment：不指定log_dir时，文件夹后缀</li>
<li>filename_suffix：event file文件名后缀</li>
</ul>
<p>（不设置log_dir的话，就会是/runs/comment/event_file）</p>
<p>设置log_dir之后，comment就不会起作用。</p>
<h5 id="2-SummaryWriter-add-scalar-amp-add-histogram"><a href="#2-SummaryWriter-add-scalar-amp-add-histogram" class="headerlink" title="2.SummaryWriter.add_scalar &amp; add_histogram"></a>2.SummaryWriter.add_scalar &amp; add_histogram</h5><h6 id="add-scalar"><a href="#add-scalar" class="headerlink" title="add_scalar()"></a>add_scalar()</h6><p>功能：记录标量</p>
<p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210926235143739.png" alt="image-20210926235143739"></p>
<ul>
<li>tag：图像的标签名，图的唯一标识</li>
<li>scalar_value：要记录的标量</li>
<li>global_step：x轴</li>
</ul>
<h6 id="add-scalars"><a href="#add-scalars" class="headerlink" title="add_scalars()"></a>add_scalars()</h6><p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210926235304794.png" alt="image-20210926235304794"></p>
<ul>
<li>main_tag：该图的标签（意义同上的tag）</li>
<li>tag_scalar_dict：key是变量的sub-tag，value是变量的值</li>
</ul>
<h6 id="add-histogram"><a href="#add-histogram" class="headerlink" title="add_histogram()"></a>add_histogram()</h6><p>功能：统计直方图与多分位数折线图</p>
<p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210927000249812.png" alt="image-20210927000249812"></p>
<ul>
<li>tag：图像的标签名，图的唯一标识</li>
<li>values：要统计的参数</li>
<li>global_step：y轴</li>
<li>bins：取直方图的bins</li>
</ul>
<h5 id="3-模型指标监控"><a href="#3-模型指标监控" class="headerlink" title="3. 模型指标监控"></a>3. 模型指标监控</h5><ol>
<li><p>在迭代训练之前，先构建一个SummaryWriter。</p>
</li>
<li><p>先设置iteration记录参数，iter_count += 1，然后</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">writer.add_scalars(<span class="string">&quot;Loss&quot;</span>, &#123;<span class="string">&quot;Train&quot;</span>：loss.item()&#125;, iter_count)</span><br><span class="line"></span><br><span class="line">writer.add_scalars(<span class="string">&quot;Accuracy&quot;</span>,&#123;<span class="string">&quot;Train&quot;</span>: correct / total&#125;, iter_count)</span><br></pre></td></tr></table></figure></li>
<li><p>```python<br>writer.add_scalars(“Loss”, {“Train”:np.mean(valid_curve)}, iter_count)</p>
<p>writer.add_scalars(“Accuracy”, {“Train”: correct / total}, iter_count)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">4. 每个epoch，记录梯度，权值：</span><br><span class="line"></span><br><span class="line">   ```python</span><br><span class="line">   for name, param in net.named_parameter():</span><br><span class="line">       writer.add_histogram(name + &#x27;_grad&#x27;, param.grad, epoch)</span><br><span class="line">       writer.add_histogram(name + &#x27;_data&#x27;, param, epoch)</span><br><span class="line">   </span><br></pre></td></tr></table></figure>

<p><strong>可以在训练过程中可视化，训练过程中即时生成可视化。</strong></p>
</li>
</ol>
<p>在观察每一层的数值和梯度分布时：</p>
<ol>
<li>当靠后的epoch梯度很小时，并不一定是梯度消失。loss的稳定也会导致梯度变小。</li>
<li>如果模型的参数发散（非正态），并且模型表现不佳，则训练很明显出了问题。</li>
<li>如果说最后一层梯度大，前面梯度小，则说明反向传播时，梯度的尺度在不断减小，出现了梯度消失，无法合理更新模型。</li>
</ol>
<h4 id="D-Tensorboard的使用（二）"><a href="#D-Tensorboard的使用（二）" class="headerlink" title="D. Tensorboard的使用（二）"></a>D. Tensorboard的使用（二）</h4><h5 id="1-add-image-and-torchvision-utils-make-grid"><a href="#1-add-image-and-torchvision-utils-make-grid" class="headerlink" title="1. add_image and torchvision.utils.make_grid"></a>1. add_image and torchvision.utils.make_grid</h5><h6 id="add-image"><a href="#add-image" class="headerlink" title="add_image()"></a>add_image()</h6><p>记录图像</p>
<p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210927110457174.png" alt="image-20210927110457174"></p>
<ul>
<li>tag：图像的标签名，图的唯一标识</li>
<li>img_tensor：图像数据，注意尺度（最终需要缩放到0-255之间，如果都是0-1区间，会默认放大到0-255区间）</li>
<li>global_step：x轴</li>
<li>dataformats：数据形式，CHW，HWC，HW（二维灰度图，不带颜色）</li>
</ul>
<h6 id="torchvision-utils-make-grid"><a href="#torchvision-utils-make-grid" class="headerlink" title="torchvision.utils.make_grid"></a>torchvision.utils.make_grid</h6><p>制作网格图像</p>
<p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210927145242252.png" alt="image-20210927145242252"></p>
<ul>
<li>tensor：图像数据，B（几张图） * C * H * W格式</li>
<li>nrow：行数（列数自动计算）</li>
<li>padding：图像间距（像素单位）</li>
<li>range：标准化范围</li>
<li>scale_each：是否单张图维度标准化</li>
<li>pad_value：padding的像素值</li>
</ul>
<p>如何可视化卷积特征图，部分代码示例如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> instance(sub_module, nn.Conv2d):</span><br><span class="line">    kernel_num += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> kernel_num &gt; vis_max:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    kernel = sub_module.weight</span><br><span class="line">    c_out, c_int, k_w, k_h = <span class="built_in">tuple</span>(kernels.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> o_icx <span class="keyword">in</span> <span class="built_in">range</span>(c_out):</span><br><span class="line">        kernel_idx = kernels[o_idx, :, :, :].unsqueeze(<span class="number">1</span>) <span class="comment"># make_grid需要BCHW，需要拓展C维数据</span></span><br><span class="line">        kernel_grid = vutils.make_grid(kernel_idx, normalize=<span class="literal">True</span>, scale_each=<span class="literal">True</span>, nrow=c_int)</span><br><span class="line">        writer.add_image(<span class="string">&#x27;&#123;&#125; Convlayer split_in_channel&#x27;</span>.<span class="built_in">format</span>(kernel_num), kernel_grid, global_step=o_idx)</span><br><span class="line">        kernel_all = kernels.view(-<span class="number">1</span>, <span class="number">3</span>, k_h, k_w)</span><br><span class="line">        kernel_grid = vutils.make_grid(kernel_all, normalize=<span class="literal">True</span>, scale_each=<span class="literal">True</span>, nrow=<span class="number">8</span>)</span><br><span class="line">        writer.add_image(<span class="string">&#x27;&#123;&#125;_all&#x27;</span>.<span class="built_in">format</span>(kernel_num), kernel_grid, global_step=<span class="number">322</span>)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 最后带个writer.close()</span></span><br></pre></td></tr></table></figure>



<h5 id="2-AlexNet卷积核与特征图可视化"><a href="#2-AlexNet卷积核与特征图可视化" class="headerlink" title="2. AlexNet卷积核与特征图可视化"></a>2. AlexNet卷积核与特征图可视化</h5><p>先对数据进行预处理，例如resize，正则化，ToTensor等操作。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型</span></span><br><span class="line">alexnet = models.alexnet(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># forward</span></span><br><span class="line">convlayer1 = alexnet.features[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 如果不在此手动获取，特征图便会被释放掉。高阶用法可以使用hook函数勾取。</span></span><br><span class="line">fmap_1 = convlayer1(img_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment">#预处理</span></span><br><span class="line">fmap_1.transpose_(<span class="number">0</span>, <span class="number">1</span>) <span class="comment"># bchw=(1, 64, 55, 55) --&gt; (64, 1, 55, 55)</span></span><br><span class="line">fmap_1_grid = vutils.make_grid(fmap_1, normalize=<span class="literal">True</span>, scale_each=<span class="literal">True</span>, nrow=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">writer.add_image(<span class="string">&#x27;feature map in conv1&#x27;</span>, fmap_1_grid, global_step=<span class="number">322</span>)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<p>部分特征图：</p>
<img src="/2021/09/28/PyTorch-TrainingProcess/image-20210927153107085.png" alt="image-20210927153107085" style="zoom:33%;">

<h5 id="3-add-graph-and-torchsummary"><a href="#3-add-graph-and-torchsummary" class="headerlink" title="3. add_graph and torchsummary"></a>3. add_graph and torchsummary</h5><h6 id="add-graph"><a href="#add-graph" class="headerlink" title="add_graph()"></a>add_graph()</h6><p>可视化模型计算图（看起来非常的宏伟）</p>
<p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210927153238642.png" alt="image-20210927153238642"></p>
<ul>
<li>model：模型，必须是nn.Module</li>
<li>input_to_model：输出给模型的数据</li>
<li>verbose：是否打印计算图结构信息</li>
</ul>
<h6 id="torchsummary"><a href="#torchsummary" class="headerlink" title="torchsummary"></a>torchsummary</h6><p>查看模型信息，便于调试</p>
<p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210927153850524.png" alt="image-20210927153850524"></p>
<ul>
<li>model：pytorch模型</li>
<li>input_size：模型输入size</li>
<li>batch_size：batch size</li>
<li>device：”cuda” or “cpu”</li>
</ul>
<p><strong>github</strong>：<a href="https://github.com/sksq96/pytorch-summary">https://github.com/sksq96/pytorch-summary</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 安装 pip install torchsummary</span></span><br><span class="line"><span class="keyword">from</span> torchsummary <span class="keyword">import</span> summary</span><br><span class="line"><span class="built_in">print</span>(summary(lenet, (<span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>), device=<span class="string">&quot;gpu&quot;</span>))</span><br></pre></td></tr></table></figure>

<p>打印信息如下：含有可训练参数，不可训练参数，模型所占内存。</p>
<img src="/2021/09/28/PyTorch-TrainingProcess/image-20210927154217792.png" alt="image-20210927154217792" style="zoom: 33%;">

<h4 id="E-Hook函数与CAM算法"><a href="#E-Hook函数与CAM算法" class="headerlink" title="E. Hook函数与CAM算法"></a>E. Hook函数与CAM算法</h4><h5 id="1-Hook函数概念"><a href="#1-Hook函数概念" class="headerlink" title="1. Hook函数概念"></a>1. Hook函数概念</h5><p>Hook函数机制：不改变主体，实现额外的功能，就像hook（挂件，挂钩）</p>
<p>在前向传播或者反向传播的主体上，通过hook获取中间的特征图，梯度。</p>
<ol>
<li>torch.Tensor.register_hook(hook)</li>
<li>torch.nn.Module.register_forward_hook</li>
<li>torch.nn.Module.register_forward_pre_hook</li>
<li>torch.nn.Module.register_backward_hook</li>
</ol>
<h6 id="Tensor-register-hook"><a href="#Tensor-register-hook" class="headerlink" title="Tensor.register_hook"></a>Tensor.register_hook</h6><p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210927161602936.png" alt="image-20210927161602936"></p>
<p>功能：注册一个<strong>反向传播</strong>hook函数</p>
<p>Hook函数仅一个输入参数，为张量的<strong>梯度</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">2.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">a = torch.add(w, x)</span><br><span class="line">b = torch.add(w, <span class="number">1</span>)</span><br><span class="line">y = torch.mul(a, b)</span><br><span class="line"></span><br><span class="line">a_grad = <span class="built_in">list</span>()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_hook</span>(<span class="params">grad</span>):</span></span><br><span class="line">    a_grad.append(grad)</span><br><span class="line">    <span class="comment"># return grad*3</span></span><br><span class="line">    </span><br><span class="line">handle = a.register_hook(grad_hook)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当backward结束之后，a的梯度将会被存储在list a_grad当中。我们如果带return的话，会覆盖掉原始张量的梯度。</span></span><br></pre></td></tr></table></figure>

<h6 id="Module-register-forward-hook"><a href="#Module-register-forward-hook" class="headerlink" title="Module.register_forward_hook"></a>Module.register_forward_hook</h6><p>注册module的前向传播hook函数</p>
<p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210927234212309.png" alt="image-20210927234212309"></p>
<ul>
<li>module：当前网络层</li>
<li>input：当前网络层输入数据</li>
<li>output：当前网络层输出数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_hook</span>(<span class="params">module, data_input, data_output</span>):</span></span><br><span class="line">    fmap_block.append(data_output) <span class="comment"># feature map</span></span><br><span class="line">    input_block.append(data_input) <span class="comment"># 网络层的输入数据</span></span><br><span class="line">    </span><br><span class="line">fmap_block = <span class="built_in">list</span>()</span><br><span class="line">input_block = <span class="built_in">list</span>()</span><br><span class="line"><span class="comment"># 将函数注册上去</span></span><br><span class="line">net.conv1.register_forward_hook(forward_hook)</span><br></pre></td></tr></table></figure>

<h6 id="Module-register-forward-pre-hook"><a href="#Module-register-forward-pre-hook" class="headerlink" title="Module.register_forward_pre_hook"></a>Module.register_forward_pre_hook</h6><p>注册module前向传播前的hook函数</p>
<p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210928005054070.png" alt="image-20210928005054070"></p>
<p>参数：</p>
<ul>
<li>module：当前网络层</li>
<li>input：当前网络层输入数据</li>
</ul>
<p>前向传播<strong>前</strong>，网络层还没有对数据进行运算。</p>
<h6 id="Module-register-backward-hook"><a href="#Module-register-backward-hook" class="headerlink" title="Module.register_backward_hook"></a>Module.register_backward_hook</h6><p>注册module反向传播的hook函数</p>
<p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210928010019514.png" alt="image-20210928010019514"></p>
<p>参数：</p>
<ul>
<li>module：当前网络层</li>
<li>grad_input：当前网络层输入梯度数据</li>
<li>grad_output：当前网络层输出梯度数据</li>
</ul>
<p>对hook函数设置好功能之后，直接挂在（register）网络上就行。</p>
<h5 id="2-Hook函数与特征图提取"><a href="#2-Hook函数与特征图提取" class="headerlink" title="2. Hook函数与特征图提取"></a>2. Hook函数与特征图提取</h5><p>代码示例如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        writer = SummaryWriter(comment=<span class="string">&#x27;test_your_comment&#x27;</span>, filename_suffix=<span class="string">&quot;_test_your_filename_suffix&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 数据</span></span><br><span class="line">        path_img = <span class="string">&quot;./lena.png&quot;</span>     <span class="comment"># your path to image</span></span><br><span class="line">        normMean = [<span class="number">0.49139968</span>, <span class="number">0.48215827</span>, <span class="number">0.44653124</span>]</span><br><span class="line">        normStd = [<span class="number">0.24703233</span>, <span class="number">0.24348505</span>, <span class="number">0.26158768</span>]</span><br><span class="line"></span><br><span class="line">        norm_transform = transforms.Normalize(normMean, normStd)</span><br><span class="line">        img_transforms = transforms.Compose([</span><br><span class="line">            transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            norm_transform</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">        img_pil = Image.<span class="built_in">open</span>(path_img).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> img_transforms <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            img_tensor = img_transforms(img_pil)</span><br><span class="line">        img_tensor.unsqueeze_(<span class="number">0</span>)    <span class="comment"># chw --&gt; bchw</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 模型</span></span><br><span class="line">        alexnet = models.alexnet(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 注册hook</span></span><br><span class="line">        fmap_dict = <span class="built_in">dict</span>()</span><br><span class="line">        <span class="keyword">for</span> name, sub_module <span class="keyword">in</span> alexnet.named_modules():</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(sub_module, nn.Conv2d):</span><br><span class="line">                key_name = <span class="built_in">str</span>(sub_module.weight.shape)</span><br><span class="line">                fmap_dict.setdefault(key_name, <span class="built_in">list</span>())</span><br><span class="line"></span><br><span class="line">                n1, n2 = name.split(<span class="string">&quot;.&quot;</span>)</span><br><span class="line"></span><br><span class="line">                <span class="function"><span class="keyword">def</span> <span class="title">hook_func</span>(<span class="params">m, i, o</span>):</span></span><br><span class="line">                    key_name = <span class="built_in">str</span>(m.weight.shape)</span><br><span class="line">                    fmap_dict[key_name].append(o)</span><br><span class="line"></span><br><span class="line">                alexnet._modules[n1]._modules[n2].register_forward_hook(hook_func)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward</span></span><br><span class="line">        output = alexnet(img_tensor)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># add image</span></span><br><span class="line">        <span class="keyword">for</span> layer_name, fmap_list <span class="keyword">in</span> fmap_dict.items():</span><br><span class="line">            fmap = fmap_list[<span class="number">0</span>]</span><br><span class="line">            fmap.transpose_(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            nrow = <span class="built_in">int</span>(np.sqrt(fmap.shape[<span class="number">0</span>]))</span><br><span class="line">            fmap_grid = vutils.make_grid(fmap, normalize=<span class="literal">True</span>, scale_each=<span class="literal">True</span>, nrow=nrow)</span><br><span class="line">            writer.add_image(<span class="string">&#x27;feature map in &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(layer_name), fmap_grid, global_step=<span class="number">322</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h5 id="3-CAM-class-activation-map，类激活图"><a href="#3-CAM-class-activation-map，类激活图" class="headerlink" title="3. CAM (class activation map，类激活图)"></a>3. CAM (class activation map，类激活图)</h5><p>CAM：类激活图，class activation map</p>
<p>对网络的最后一个特征图进行加权求和，得到网络的注意力放在哪里。</p>
<p>如何获取Wn的权值才是关键。必须要有GAP（global average pooling）。</p>
<p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210928012310456.png" alt="image-20210928012310456"></p>
<p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210928012408043.png" alt="image-20210928012408043"></p>
<h5 id="4-Grad-CAM"><a href="#4-Grad-CAM" class="headerlink" title="4. Grad-CAM"></a>4. Grad-CAM</h5><p>Grad-CAM：CAM改进版，利用梯度作为特征图权重</p>
<p><img src="/2021/09/28/PyTorch-TrainingProcess/image-20210928013456482.png" alt="image-20210928013456482"></p>
]]></content>
      <categories>
        <category>PyTorch框架学习</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
</search>
