[{"title":"PyTorch-Loss&Optimizer","url":"/2021/09/26/PyTorch-Loss-Optimizer/","content":"<h2 id=\"四-PyTorch损失优化（Loss-amp-Optimizer）\"><a href=\"#四-PyTorch损失优化（Loss-amp-Optimizer）\" class=\"headerlink\" title=\"四. PyTorch损失优化（Loss&amp;Optimizer）\"></a>四. PyTorch损失优化（Loss&amp;Optimizer）</h2><h3 id=\"A-权值初始化\"><a href=\"#A-权值初始化\" class=\"headerlink\" title=\"A. 权值初始化\"></a>A. 权值初始化</h3><h4 id=\"1-梯度消失与爆炸Gradient-Vanishing-and-Exploring\"><a href=\"#1-梯度消失与爆炸Gradient-Vanishing-and-Exploring\" class=\"headerlink\" title=\"1. 梯度消失与爆炸Gradient Vanishing and Exploring\"></a>1. 梯度消失与爆炸Gradient Vanishing and Exploring</h4><h5 id=\"方差一致性原则\"><a href=\"#方差一致性原则\" class=\"headerlink\" title=\"方差一致性原则\"></a>方差一致性原则</h5><p>$$</p>\n<ol>\n<li> E(X*Y) = E(X)*E(Y)\\</li>\n<li> D(X) = E(X^2) - [E(X)]^2\\</li>\n<li> D(X+Y) = D(X) + D(Y)<br>$$</li>\n</ol>\n<p>$$<br>1.2.3 \\Rightarrow D(X<em>Y) = D(X)<em>D(Y) + D(X)</em>[E(Y)]^2 + D(Y)</em>[E(X)]^2\\<br>若E(X)=0, E(Y)=0\\<br>则D(X*Y) = D(X)*D(Y)<br>$$</p>\n<p>$$<br>第一个隐藏层的第一个神经元, 输入层的每一个神经元✖权值求和：\\<br>H_{11} = \\sum^{n}<em>{i=0}X_i*W</em>{1i}\\由公式 D(X<em>Y) = D(X)<em>D(Y)\\<br>可知D(H_{11}) = \\sum^{n}<em>{i=0}D(X_i)*D(W</em>{1i})\\ = n*(1</em>1)n为神经元的个数，X_i和W_i的方差均为1\\<br>=n\\<br>std(H_{11}) = \\sqrt{D(H_{11})} = \\sqrt{n}\\<br>意味着经过一层神经元的传播标准差就会扩大\\sqrt{n}倍\\<br>因此，想让方差D(H_1) = n</em>D(X)*D(W) = 1\\<br>D(W)=\\frac{1}{n}\\Rightarrow std(W)=\\sqrt{\\frac{1}{n}}\\<br>所以权值的标准差为\\sqrt{\\frac{1}{n}}时（n为每一层神经元个数），\\才可以保证尺度维持在恰当范围。<br>$$</p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210924001951770.png\" alt=\"image-20210924001951770\"></p>\n<p>需要保持尺度不变</p>\n<h4 id=\"2-Xavier方法和Kaiming方法\"><a href=\"#2-Xavier方法和Kaiming方法\" class=\"headerlink\" title=\"2. Xavier方法和Kaiming方法\"></a>2. Xavier方法和Kaiming方法</h4><p>Xavier初始化</p>\n<p>方差一致性：保持数据尺度维持在恰当范围，通常方差为1。</p>\n<p>适用激活函数：饱和函数，如sigmoid，Tanh</p>\n<img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210924004244068.png\" alt=\"image-20210924004244068\" style=\"zoom: 50%;\">\n\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210924004442111.png\" alt=\"image-20210924004442111\"></p>\n<p>Kaiming初始化</p>\n<p>方差一致性：保持数据尺度维持在恰当范围，通常方差为1。</p>\n<p>适用激活函数：ReLU及其变种</p>\n<img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210924005023102.png\" alt=\"image-20210924005023102\" style=\"zoom: 33%;\">\n\n\n\n\n\n<h4 id=\"3-常用初始化方法\"><a href=\"#3-常用初始化方法\" class=\"headerlink\" title=\"3. 常用初始化方法\"></a>3. 常用初始化方法</h4><h5 id=\"PyTorch方法查询\"><a href=\"#PyTorch方法查询\" class=\"headerlink\" title=\"PyTorch方法查询\"></a>PyTorch方法查询</h5><p><a href=\"https://pytorch.org/docs/stable/nn.init.html\">https://pytorch.org/docs/stable/nn.init.html</a></p>\n<ul>\n<li>Xavier均匀/正态分布</li>\n<li>Kaiming均匀/正态分布</li>\n<li>均匀分布 / 正态分布 / 常熟分布</li>\n<li>正交矩阵初始化 / 单位矩阵初始化 / 稀疏矩阵初始化</li>\n</ul>\n<h5 id=\"nn-init-calculate-gain\"><a href=\"#nn-init-calculate-gain\" class=\"headerlink\" title=\"nn.init.calculate_gain()\"></a>nn.init.calculate_gain()</h5><p>nn.init.calculate_gain(nonlinearity, param=None)</p>\n<p>计算激活函数的方差变换尺度</p>\n<p>主要参数：</p>\n<ul>\n<li>nonlinearity：激活函数名称</li>\n<li>param：激活函数的参数，如leakyReLU的negative_slop</li>\n</ul>\n<h3 id=\"B-损失函数\"><a href=\"#B-损失函数\" class=\"headerlink\" title=\"B. 损失函数\"></a>B. 损失函数</h3><h4 id=\"1-损失函数概念\"><a href=\"#1-损失函数概念\" class=\"headerlink\" title=\"1. 损失函数概念\"></a>1. 损失函数概念</h4><p><strong>损失函数</strong>：衡量模型输出与真实标签的<strong>差异</strong></p>\n<img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210924105850749.png\" alt=\"image-20210924105850749\" style=\"zoom:50%;\">\n\n<p>损失函数（Loss Function）：</p>\n<p>计算<strong>一个样本</strong><br>$$<br>Loss = f（y’，y)<br>$$<br>代价函数（Cost Function）：</p>\n<p>计算<strong>训练集（样本集）</strong>中各个Loss的<strong>平均值</strong><br>$$<br>Cost = \\frac{1}{N}\\sum^{N}_{i}f(y’_i, y_i)<br>$$<br>目标函数（Objective Function）：<br>$$<br>Obj = Cost（代价函数） + Regularization<br>$$<br>Cost代价函数<strong>并不是越小越好</strong>，可能会有过拟合。约束项是Regularization（L1，L2，稀疏约束），和Cost一起构成目标函数。</p>\n<h4 id=\"2-损失函数\"><a href=\"#2-损失函数\" class=\"headerlink\" title=\"2. 损失函数\"></a>2. 损失函数</h4><h5 id=\"nn-CrossEntropyLoss\"><a href=\"#nn-CrossEntropyLoss\" class=\"headerlink\" title=\"nn.CrossEntropyLoss\"></a>nn.CrossEntropyLoss</h5><p><strong>nn.LogSoftmax()与nn.NLLLoss()结合</strong>，进行交叉熵计算</p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210924112655749.png\" alt=\"image-20210924112655749\"></p>\n<p>主要参数：</p>\n<ul>\n<li>weight：各类别的loss设置权值</li>\n<li>ignore_index：忽略某个类别</li>\n<li>reduction：计算模式，可为none/sum/mean。‘none’不会造成维度上的衰减，‘sum’会将loss值相加，‘mean’会对所有loss求平均。  </li>\n</ul>\n<p>none- 逐个元素计算</p>\n<p>sum- 所有元素求和，返回标量</p>\n<p>mean- 加权平均，返回标量</p>\n<p>交叉熵 = 信息熵 + 相对熵</p>\n<p>熵（用来描述一件事情的不确定性，不确定性的概率越大，熵就越大）：</p>\n<p>熵是自信息的一个期望，是描述整个概率分布：<br>$$<br>H(P)=E_{x\\thicksim p}[I(x)] = -\\sum^{N}<em>{i}P(x_i)logP(x_i)<br>$$<br>自信息的公式，对概率取一个负log：<br>$$<br>I(x) = -log[p(x)]<br>$$<br>相对熵，用来衡量两个分布之间的距离（差异）：<br>$$<br>D</em>{KL}(P, Q) = E_{x\\thicksim p}[log\\frac{P(x)}{Q(x)}]\\<br>= E_{x\\thicksim p}[logP(x) - logQ(x)]\\<br>=\\sum^{N}<em>{i=1}P(x_i)[logP(x_i)-logQ(x_i)]\\<br>=\\sum^{N}</em>{i=1}P(x_i)logP(x_i)-\\sum^{N}_{i=1}P(x_i)logQ(x_i)\\<br>=H(P,Q) - H(P)\\<br>P是真实分布，Q是模型拟合的分布。我们要用Q去拟合（逼近）P<br>$$<br>weight参数的使用：</p>\n<ul>\n<li>向量的形式，有多少个类别，就要有多少个weight</li>\n<li>使用之后，会给不同的类别赋予不同的权值</li>\n</ul>\n<h5 id=\"nn-NLLLoss\"><a href=\"#nn-NLLLoss\" class=\"headerlink\" title=\"nn.NLLLoss\"></a>nn.NLLLoss</h5><p>实现负对数似然函数中的负号功能（其实只是执行了加个负号的功能）</p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210924163054623.png\" alt=\"image-20210924163054623\"></p>\n<p>主要参数：</p>\n<ul>\n<li>weight：各类别的loss设置权值</li>\n<li>ignore_index：忽略某个类别</li>\n<li>reduction：计算模式，可为none/sum/mean</li>\n</ul>\n<p>none- 逐个元素计算</p>\n<p>sum- 所有元素求和，返回标量</p>\n<p>mean- 加权平均，返回标量</p>\n<h5 id=\"nn-BCELoss\"><a href=\"#nn-BCELoss\" class=\"headerlink\" title=\"nn.BCELoss\"></a>nn.BCELoss</h5><p>二分类交叉熵</p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925145330328.png\" alt=\"image-20210925145330328\"></p>\n<p><strong>注意事项</strong>：输入值取值在[0, 1]，符合概率取值的区间</p>\n<p>主要参数：</p>\n<ul>\n<li>weight：各类别的loss设置权值</li>\n<li>ignore_index：忽略某个类别</li>\n<li>reduction：计算模式，可为none/sum/mean</li>\n</ul>\n<h5 id=\"nn-BCEWithLogitsLoss\"><a href=\"#nn-BCEWithLogitsLoss\" class=\"headerlink\" title=\"nn.BCEWithLogitsLoss\"></a>nn.BCEWithLogitsLoss</h5><p>结合<strong>Sigmoid</strong>与<strong>二分类交叉熵</strong></p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925145853652.png\" alt=\"image-20210925145853652\"></p>\n<p><strong>注意事项：网络后面不加sigmoid函数</strong></p>\n<p>主要参数：</p>\n<ul>\n<li>pos_weight：正样本的权值</li>\n<li>weight：各类别的loss设置权值</li>\n<li>ignore_index：忽略某个类别</li>\n<li>reduction：计算模式，可为none/sum/mean</li>\n</ul>\n<h5 id=\"nn-L1Loss\"><a href=\"#nn-L1Loss\" class=\"headerlink\" title=\"nn.L1Loss\"></a>nn.L1Loss</h5><p>计算inputs与target之差的绝对值</p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925150840297.png\" alt=\"image-20210925150840297\"></p>\n<h5 id=\"nn-MSELoss\"><a href=\"#nn-MSELoss\" class=\"headerlink\" title=\"nn.MSELoss\"></a>nn.MSELoss</h5><p>计算inputs与target之差的平方</p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925150852293.png\" alt=\"image-20210925150852293\"></p>\n<p>主要参数：</p>\n<ul>\n<li>reduction：计算模式，可为none/ sum/ mean</li>\n</ul>\n<h5 id=\"nn-SmoothL1Loss\"><a href=\"#nn-SmoothL1Loss\" class=\"headerlink\" title=\"nn.SmoothL1Loss\"></a>nn.SmoothL1Loss</h5><p>平滑的L1Loss。</p>\n<p>Xi是模型输出，Yi是标签。</p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925151138993.png\" alt=\"image-20210925151138993\"></p>\n<p>主要参数：</p>\n<ul>\n<li>reduction：计算模式，可为none/ sum/ mean</li>\n<li><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925151323568.png\" alt=\"image-20210925151323568\" style=\"zoom:33%;\"></li>\n</ul>\n<h5 id=\"nn-PoissonNLLLoss\"><a href=\"#nn-PoissonNLLLoss\" class=\"headerlink\" title=\"nn.PoissonNLLLoss\"></a>nn.PoissonNLLLoss</h5><p>泊松分布的负对数似然损失函数</p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925151752707.png\" alt=\"image-20210925151752707\"></p>\n<p>主要参数：</p>\n<ul>\n<li><strong>log_input</strong>：输入是否为对数形式，决定计算公式</li>\n<li><strong>full</strong>：计算所有loss，默认为False</li>\n<li><strong>eps</strong>：修正项，避免log（input）为nan</li>\n</ul>\n<h5 id=\"nn-KLDivLoss\"><a href=\"#nn-KLDivLoss\" class=\"headerlink\" title=\"nn.KLDivLoss\"></a>nn.KLDivLoss</h5><p>计算<strong>KLD（divergence）</strong>，KL散度，<strong>相对熵</strong>。衡量两个分布之间的距离。</p>\n<p><strong>注意事项：需提前将输入计算log-probabilities，如通过nn.logsoftmax()</strong></p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925152326201.png\" alt=\"image-20210925152326201\"></p>\n<p>主要参数：</p>\n<ul>\n<li>reduction：none/ sum/ mean /batchmean</li>\n</ul>\n<p>batchmean- batchsize维度求平均值</p>\n<h5 id=\"nn-MarginRankingLoss\"><a href=\"#nn-MarginRankingLoss\" class=\"headerlink\" title=\"nn.MarginRankingLoss\"></a>nn.MarginRankingLoss</h5><p>计算两个向量之间的相似度，<strong>用于排序任务</strong></p>\n<p>y=1时，希望x1与x2大，当x1&gt;x2时，不产生loss</p>\n<p>y=-1时，希望x2比x1大，当x2&gt;x1时，不产生loss</p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925153835354.png\" alt=\"image-20210925153835354\"></p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925153849860.png\" alt=\"image-20210925153849860\"></p>\n<p><strong>特别说明：该方法计算两组数据之间的差异，返回一个n*n的loss矩阵</strong></p>\n<p>主要参数：</p>\n<ul>\n<li>margin：边界值，x1与x2之间的差异值</li>\n<li>reduction：计算模式</li>\n</ul>\n<h5 id=\"nn-MultiLabelMarginLoss\"><a href=\"#nn-MultiLabelMarginLoss\" class=\"headerlink\" title=\"nn.MultiLabelMarginLoss\"></a>nn.MultiLabelMarginLoss</h5><p>多标签边界损失函数</p>\n<p>举例：四分类任务，样本x属于0类或者3类</p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925155812561.png\" alt=\"image-20210925155812561\"></p>\n<p>标签：[0，3，-1，1]，不是[1，0，0，1]</p>\n<h5 id=\"nn-SofrMarginLoss\"><a href=\"#nn-SofrMarginLoss\" class=\"headerlink\" title=\"nn.SofrMarginLoss\"></a>nn.SofrMarginLoss</h5><p>计算二分类的logistic损失</p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925160607942.png\" alt=\"image-20210925160607942\"></p>\n<h5 id=\"nn-MultiLabelSoftMarginLoss\"><a href=\"#nn-MultiLabelSoftMarginLoss\" class=\"headerlink\" title=\"nn.MultiLabelSoftMarginLoss\"></a>nn.MultiLabelSoftMarginLoss</h5><p>SoftMarginLoss多标签版本</p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925160853772.png\" alt=\"image-20210925160853772\"></p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925160909202.png\" alt=\"image-20210925160909202\"></p>\n<p>主要参数：</p>\n<p><strong>weight</strong>：各类别的loss设置权值</p>\n<p><strong>reduction</strong>：计算模式，可为none / sum / mean</p>\n<h5 id=\"nn-MultiMarginLoss\"><a href=\"#nn-MultiMarginLoss\" class=\"headerlink\" title=\"nn.MultiMarginLoss\"></a>nn.MultiMarginLoss</h5><p>计算多分类的折页损失</p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925161602192.png\" alt=\"image-20210925161602192\"></p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925161628228.png\" alt=\"image-20210925161628228\"></p>\n<p>主要参数：</p>\n<ul>\n<li>p：可选1或2</li>\n<li>weight：各类别的loss设置权值</li>\n<li>margin：边界值</li>\n<li>reduction：计算模式，可为none/sum/mean</li>\n</ul>\n<h5 id=\"nn-TripletMarginLoss\"><a href=\"#nn-TripletMarginLoss\" class=\"headerlink\" title=\"nn.TripletMarginLoss\"></a>nn.TripletMarginLoss</h5><p>计算三元组损失，人脸验证中常用</p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925162115800.png\" alt=\"image-20210925162115800\"></p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925162211016.png\" alt=\"image-20210925162211016\"></p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925162237021.png\" alt=\"image-20210925162237021\"></p>\n<p>主要参数：</p>\n<ul>\n<li>p：范数的阶，默认为2</li>\n<li>margin：边界值</li>\n<li>reduction：计算模式，可为none/sum/mean</li>\n</ul>\n<h5 id=\"nn-HingeEmbeddingLoss\"><a href=\"#nn-HingeEmbeddingLoss\" class=\"headerlink\" title=\"nn.HingeEmbeddingLoss\"></a>nn.HingeEmbeddingLoss</h5><p>计算两个输入的相似性，常用于非线性embedding和半监督学习</p>\n<p><strong>特别注意：输入x应为两个输入只差的绝对值</strong></p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925162430193.png\" alt=\"image-20210925162430193\"></p>\n<p>主要参数：</p>\n<ul>\n<li>margin：边界值</li>\n<li>reduction：计算模式，可为none/sum/mean</li>\n</ul>\n<h5 id=\"nn-CosineEmbeddingLoss\"><a href=\"#nn-CosineEmbeddingLoss\" class=\"headerlink\" title=\"nn.CosineEmbeddingLoss\"></a>nn.CosineEmbeddingLoss</h5><p>采用余弦相似度计算两个输入的相似性，主要关注方向上的差异。</p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925162632490.png\" alt=\"image-20210925162632490\"></p>\n<p>主要参数：</p>\n<p>margin：可取值[-1，1]，推荐为[0，0.5]</p>\n<p>reduction：计算模式，可为none/sum/mean</p>\n<h5 id=\"nn-CTCLoss\"><a href=\"#nn-CTCLoss\" class=\"headerlink\" title=\"nn.CTCLoss\"></a>nn.CTCLoss</h5><p>计算CTC损失，解决时序类数据的分类Connectionist Temporal Classification</p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925163057884.png\" alt=\"image-20210925163057884\"></p>\n<p>主要参数：</p>\n<ul>\n<li>blank：blank label</li>\n<li>zero_infinity：无穷大的值或者梯度置0</li>\n<li>reduction：计算模式，可为none/sum/mean</li>\n</ul>\n<h3 id=\"C-优化器-Optimizer\"><a href=\"#C-优化器-Optimizer\" class=\"headerlink\" title=\"C. 优化器 Optimizer\"></a>C. 优化器 Optimizer</h3><h4 id=\"1-什么是优化器\"><a href=\"#1-什么是优化器\" class=\"headerlink\" title=\"1. 什么是优化器\"></a>1. 什么是优化器</h4><p>pytorch的优化器：管理并更新模型中的可学习参数的值，使得模型输出更接近真实标签</p>\n<ul>\n<li><strong>导数</strong>：函数在指定坐标轴上的变化率</li>\n<li><strong>方向导数</strong>：指定方向上的变化率</li>\n<li><strong>梯度</strong>：一个向量，方向为方向导数取得最大值的方向</li>\n</ul>\n<img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925164501982.png\" alt=\"image-20210925164501982\" style=\"zoom:50%;\">\n\n<h4 id=\"2-optimizer的属性\"><a href=\"#2-optimizer的属性\" class=\"headerlink\" title=\"2. optimizer的属性\"></a>2. optimizer的属性</h4><p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925164937096.png\" alt=\"image-20210925164937096\"></p>\n<p>基本属性：</p>\n<ul>\n<li>defaults：优化器超参数</li>\n<li>state：参数的缓存，如momentum的缓存</li>\n<li>param_groups：管理的参数组</li>\n<li>_step_count：记录更新的次数，学习率调整中使用</li>\n</ul>\n<p>基本方法：</p>\n<ul>\n<li><p>zero_grad()：清空所管理参数的梯度<img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925165616845.png\" alt=\"image-20210925165616845\" style=\"zoom:50%;\"></p>\n</li>\n<li><p>step()：执行一步更新，更新一次我们的权值参数</p>\n</li>\n<li><p>add_param_group()：添加参数组</p>\n</li>\n<li><p>state_dict()：获取优化器当前状态信息字典</p>\n</li>\n<li><p>load_state_dict()：加载状态信息字典（用于模型断点的续训练）</p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925211631557.png\" alt=\"image-20210925211631557\"></p>\n</li>\n</ul>\n<p><strong>pytorch特性：张量梯度不自动清零，会将每一次计算的梯度加在一起。所以需要使用完梯度，或者backward之前，要zero_grad()一下。</strong></p>\n<h4 id=\"3-optimizer的方法\"><a href=\"#3-optimizer的方法\" class=\"headerlink\" title=\"3. optimizer的方法\"></a>3. optimizer的方法</h4><h4 id=\"4-learning-rate学习率\"><a href=\"#4-learning-rate学习率\" class=\"headerlink\" title=\"4. learning rate学习率\"></a>4. learning rate学习率</h4><p>梯度下降：<br>$$<br>W_{i+1} = W_{i} - g(W_{i})<br>$$<br>假设现在函数为<br>$$<br>y = f(x) = 4<em>x^2\\<br>y’ = f’(x)=8</em>x<br>$$<br><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925215213471.png\" alt=\"image-20210925215213471\" style=\"zoom:33%;\"><br>$$<br>x_0 =2,y_0 = 16,f’(x_0)=16\\<br>x_1=x_0 - f’(x_0)=2-16=14\\<br>x_1=-14,y_1 = 784, f’(x_1)=-112\\<br>x_2 = x_1 - f’(x_1)=-14+112=98,y_2=38416\\<br>……<br>$$<br>如果没有学习率的话，就会梯度爆炸。</p>\n<img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925220758203.png\" alt=\"image-20210925220758203\" style=\"zoom: 50%;\">\n\n<p>学习率（learning rate）控制更新的步伐。<br>$$<br>W_{i+1} = W_{i} - LR*g(W_{i})<br>$$<br>此时我们将LR设置为0.2：</p>\n<img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925221017651.png\" alt=\"image-20210925221017651\" style=\"zoom: 50%;\">\n\n<p>将LR设置为0.1时：</p>\n<img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925221141585.png\" alt=\"image-20210925221141585\" style=\"zoom: 50%;\">\n\n\n\n<h4 id=\"5-momentum-动量\"><a href=\"#5-momentum-动量\" class=\"headerlink\" title=\"5. momentum 动量\"></a>5. momentum 动量</h4><p>Momentum（动量，冲量）：<strong>结合当前梯度与上一次更新信息</strong>，用于当前更新。</p>\n<p>指数加权平均：<br>$$<br>v_t = \\beta * v_{t-1} + (1-\\beta)*\\theta_t\\<br>v_{100} = \\beta * v_{99} + (1-\\beta)*\\theta_{100}\\<br>= (1-\\beta)<em>\\theta_{100} + (1-\\beta)<em>\\beta</em>\\theta_{99}+(\\beta^2</em>v_{98})\\<br>=\\sum^N_i(1-\\beta)<em>\\beta^i</em>\\theta_{N-i}<br>$$<br><strong>核心思想</strong>：距离当前时间越近，影响越大；距离当前时间越远，影响越小。</p>\n<p>PyTorch中的更新公式：<br>$$<br>v_i = m<em>v_{i-1}+g(w_i)\\<br>w_{i+1}=w_i - lr</em>v_i\\<br>w_{w+1}：第i+1次更新的参数\\<br>lr：学习率\\<br>v_i = 更新量\\<br>m：momentum系数\\<br>g(w_i)：w_i的梯度<br>$$</p>\n<h4 id=\"6-torch-optim-SGD\"><a href=\"#6-torch-optim-SGD\" class=\"headerlink\" title=\"6. torch.optim.SGD\"></a>6. torch.optim.SGD</h4><p>用的最多的优化器，绝大部分模型都可以用SGD获得一个不错的效果。</p>\n<p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925230235496.png\" alt=\"image-20210925230235496\"></p>\n<p>主要参数：</p>\n<ul>\n<li>params：管理的参数组</li>\n<li>lr：初始学习率</li>\n<li>momentum：动量参数，贝塔</li>\n<li>weight_decay：L2正则化系数</li>\n<li>nesterov：是否采用NAG（通常为false，不会采用）</li>\n</ul>\n<h3 id=\"D-Pytorch的十种优化器\"><a href=\"#D-Pytorch的十种优化器\" class=\"headerlink\" title=\"D. Pytorch的十种优化器\"></a>D. Pytorch的十种优化器</h3><p><img src=\"/2021/09/26/PyTorch-Loss-Optimizer/image-20210925230441311.png\" alt=\"image-20210925230441311\"></p>\n<h4 id=\"TODO：详细使用方法以后补充\"><a href=\"#TODO：详细使用方法以后补充\" class=\"headerlink\" title=\"TODO：详细使用方法以后补充\"></a>TODO：详细使用方法以后补充</h4>","categories":["PyTorch框架学习"],"tags":["PyTorch"]},{"title":"PyTorch-DataProcess&Dataset","url":"/2021/09/26/PyTorch-DataProcess-Dataset/","content":"<h2 id=\"二-PyTorch数据处理（Dataset）\"><a href=\"#二-PyTorch数据处理（Dataset）\" class=\"headerlink\" title=\"二. PyTorch数据处理（Dataset）\"></a>二. PyTorch数据处理（Dataset）</h2><h3 id=\"A-数据读取机制Dataloader与Dataset\"><a href=\"#A-数据读取机制Dataloader与Dataset\" class=\"headerlink\" title=\"A. 数据读取机制Dataloader与Dataset\"></a>A. 数据读取机制Dataloader与Dataset</h3><p><strong>数据</strong>：</p>\n<ul>\n<li><strong>数据收集</strong>：Img &amp; Label</li>\n<li><strong>数据划分</strong>：train valid test</li>\n<li><strong>数据读取</strong>：Dataloader（Sampler生成索引 / Dataset根据索引读取数据）</li>\n<li><strong>数据预处理</strong>：transform</li>\n</ul>\n<p><strong>Epoch</strong>: 所有训练样本都输入到模型当中一次，称为一个epoch</p>\n<p><strong>Iteration</strong>：一批样本输入到模型中</p>\n<p><strong>Batchsize</strong>：批大小，决定了一个Epoch进行多少次Iteration</p>\n<p><strong>例</strong>：</p>\n<ul>\n<li><p>样本总数：80，Batchsize：8，1 Epoch = 10 Iteration</p>\n</li>\n<li><p>样本总数：87，Batchsize：8</p>\n<p>drop_last = True/False，1 Epoch = 10 Iteration/11 Iteration</p>\n</li>\n</ul>\n<p>数据读取机制：</p>\n<ul>\n<li>读哪些数据：在每一个iteration时，读一个batchsize大小，该读取哪几个数据。一般是sampler输出的index。</li>\n<li>从哪读数据：在硬盘中从哪找数据。dataset中的data_dir。</li>\n<li>怎么读数据： Dataset中的getitem</li>\n</ul>\n<p>训练时：</p>\n<p>以Epoch为主循环，其中嵌入每次Iteration的循环。</p>\n<h4 id=\"1-Dataloader和Dataset\"><a href=\"#1-Dataloader和Dataset\" class=\"headerlink\" title=\"1. Dataloader和Dataset\"></a>1. Dataloader和Dataset</h4><h5 id=\"torch-utils-data-Dataloader\"><a href=\"#torch-utils-data-Dataloader\" class=\"headerlink\" title=\"torch.utils.data.Dataloader\"></a>torch.utils.data.Dataloader</h5><p>构建可迭代的数据装载器</p>\n<p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922133310174.png\" alt=\"image-20210922133310174\"></p>\n<ul>\n<li>dataset：Dataset类，决定数据从哪读取及如何读取</li>\n<li>batchsize：批大小</li>\n<li>num_works：是否多进程读取数据</li>\n<li>shuffle：每个epoch是否乱序</li>\n<li>drop_last：当样本数不能被batchsize整除时，是否舍弃最后一批数据</li>\n</ul>\n<h5 id=\"torch-utils-data-Dataset\"><a href=\"#torch-utils-data-Dataset\" class=\"headerlink\" title=\"torch.utils.data.Dataset\"></a>torch.utils.data.Dataset</h5><p>Dataset抽象类，所有自定义的Dataset需要继承它，并且复写__ getitem __( )</p>\n<p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922155733171.png\" alt=\"image-20210922155733171\"></p>\n<p>getitem: 接受一个索引，返回一个样本。</p>\n<p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922170343251.png\" alt=\"image-20210922170343251\"></p>\n<h3 id=\"B-数据预处理模块\"><a href=\"#B-数据预处理模块\" class=\"headerlink\" title=\"B. 数据预处理模块\"></a>B. 数据预处理模块</h3><p><strong>torchvision</strong>简介：</p>\n<p>计算机视觉工具包</p>\n<ul>\n<li>torchvision.transforms：常用的图像预处理方法</li>\n<li>torchvision.dataset：常用数据集的dataset实现，MNIST，CIFAR-10，ImageNet</li>\n<li>torchvision.model：常用的模型预训练，AlexNet，VGG，ResNet，GoogleLeNet</li>\n</ul>\n<h4 id=\"1-transform运行机制\"><a href=\"#1-transform运行机制\" class=\"headerlink\" title=\"1.transform运行机制\"></a>1.transform运行机制</h4><p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922171520294.png\" alt=\"image-20210922171520294\"></p>\n<h5 id=\"transforms-Compose\"><a href=\"#transforms-Compose\" class=\"headerlink\" title=\"transforms.Compose\"></a>transforms.Compose</h5><p>将一系列方法进行有序的包装，之后安装顺序对数据进行处理。</p>\n<h5 id=\"transforms-Normalize\"><a href=\"#transforms-Normalize\" class=\"headerlink\" title=\"transforms.Normalize\"></a>transforms.Normalize</h5><p>逐channel的对图像进行标准化</p>\n<p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922172839907.png\" alt=\"image-20210922172839907\"></p>\n<p>output = (input - mean) / std</p>\n<ul>\n<li>mean：各通道的均值</li>\n<li>std：各通道的标准差</li>\n<li>inplace：是否原地操作</li>\n</ul>\n<h4 id=\"2-数据标准化\"><a href=\"#2-数据标准化\" class=\"headerlink\" title=\"2.数据标准化\"></a>2.数据标准化</h4><h3 id=\"C-数据增强\"><a href=\"#C-数据增强\" class=\"headerlink\" title=\"C. 数据增强\"></a>C. 数据增强</h3><h4 id=\"1-transforms——剪裁\"><a href=\"#1-transforms——剪裁\" class=\"headerlink\" title=\"1. transforms——剪裁\"></a>1. transforms——剪裁</h4><h5 id=\"transforms-CenterCrop\"><a href=\"#transforms-CenterCrop\" class=\"headerlink\" title=\"transforms.CenterCrop\"></a>transforms.CenterCrop</h5><p>从图像中心裁剪图片</p>\n<ul>\n<li>size：所需裁剪图片尺寸</li>\n</ul>\n<h5 id=\"transforms-RandomCrop\"><a href=\"#transforms-RandomCrop\" class=\"headerlink\" title=\"transforms.RandomCrop\"></a>transforms.RandomCrop</h5><p>从图片中随机裁剪尺寸为size的图片</p>\n<p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922210309872.png\" alt=\"image-20210922210309872\"></p>\n<ul>\n<li>size：所需裁剪图片尺寸</li>\n<li>padding：设置填充大小。当为a时，上下左右均填充a个像素；当为（a，b）时，上下填充b个像素，左右填充a个。当为（a，b，c，d）时，左，上，右，下分别填充a，b，c，d个像素。</li>\n<li>pad_is_need：若图像小于设定的size，则填充</li>\n<li>padding_mode：四种填充模式<ol>\n<li>constant：像素值由fill设定</li>\n<li>edge：像素值由图像边缘像素决定</li>\n<li>reflect：镜像填充，最后一个像素不镜像[1, 2, 3, 4]——&gt; [3,2,1,2,3,4,3,2]</li>\n<li>symmetric：镜像填充，最后一个像素镜像[1, 2, 3, 4]——&gt;[2,1,1,2,3,4,4,3]</li>\n</ol>\n</li>\n<li>fill：constant时，设置填充的像素值</li>\n</ul>\n<h5 id=\"transforms-RandomResizeCrop\"><a href=\"#transforms-RandomResizeCrop\" class=\"headerlink\" title=\"transforms.RandomResizeCrop\"></a>transforms.RandomResizeCrop</h5><p>随机大小，长宽比裁剪图片</p>\n<p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922211513888.png\" alt=\"image-20210922211513888\"></p>\n<ul>\n<li><p>size：所需裁剪图片尺寸</p>\n</li>\n<li><p>scale：随即裁剪面积比例，默认（0.08，1）</p>\n</li>\n<li><p>ratio：随机长宽比，默认（3/4, 4/3）</p>\n</li>\n<li><p>interpolation：插值方法 </p>\n<p>PIL.Image.NEAREST</p>\n<p>PIL.Image.BILINEAR</p>\n<p>PIL.Image.BICUBIC</p>\n</li>\n</ul>\n<h5 id=\"FiveCrop-TenCrop\"><a href=\"#FiveCrop-TenCrop\" class=\"headerlink\" title=\"FiveCrop/TenCrop\"></a>FiveCrop/TenCrop</h5><p>在图片的上下左右以及中心裁剪出尺寸为size的5张图片，TenCrop对这5张图片进行水平或者垂直镜像获得10张图片</p>\n<p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922212510007.png\" alt=\"image-20210922212510007\"></p>\n<p>size：所需裁剪图片的尺寸</p>\n<p>vertical_flip：是否垂直翻转</p>\n<h4 id=\"2-transforms——翻转、旋转\"><a href=\"#2-transforms——翻转、旋转\" class=\"headerlink\" title=\"2. transforms——翻转、旋转\"></a>2. transforms——翻转、旋转</h4><h5 id=\"transforms-RandomHorizontalFlip-VerticalFlip\"><a href=\"#transforms-RandomHorizontalFlip-VerticalFlip\" class=\"headerlink\" title=\"transforms.RandomHorizontalFlip/VerticalFlip\"></a>transforms.RandomHorizontalFlip/VerticalFlip</h5><p>依概率水平（左右）或垂直（上下）翻转图片</p>\n<p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922213455329.png\" alt=\"image-20210922213455329\"></p>\n<ul>\n<li>p：翻转概率</li>\n</ul>\n<h5 id=\"transforms-RandomRotation\"><a href=\"#transforms-RandomRotation\" class=\"headerlink\" title=\"transforms.RandomRotation\"></a>transforms.RandomRotation</h5><p>随机旋转图片</p>\n<p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922213841911.png\" alt=\"image-20210922213841911\"></p>\n<ul>\n<li>degrees：旋转角度。当为a时，在（-a，a）之间选择旋转角度；当为（a，b）时，在（a，b）之间选择旋转角度</li>\n<li>resample：重采样方法</li>\n<li>expand：是否扩大图片，以保持原图信息</li>\n</ul>\n<h4 id=\"3-transfroms——图像变换\"><a href=\"#3-transfroms——图像变换\" class=\"headerlink\" title=\"3. transfroms——图像变换\"></a>3. transfroms——图像变换</h4><h5 id=\"transforms-Pad\"><a href=\"#transforms-Pad\" class=\"headerlink\" title=\"transforms.Pad\"></a>transforms.Pad</h5><p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922215241192.png\" alt=\"image-20210922215241192\"></p>\n<ul>\n<li><p>padding：设置填充大小。当为a时，上下左右均填充a个像素；当为（a，b）时，上下填充b个像素，左右填充a个。当为（a，b，c，d）时，左，上，右，下分别填充a，b，c，d个像素。</p>\n</li>\n<li><p>padding_mode：四种填充模式</p>\n<ol>\n<li>constant：像素值由fill设定</li>\n<li>edge：像素值由图像边缘像素决定</li>\n<li>reflect：镜像填充，最后一个像素不镜像[1, 2, 3, 4]——&gt; [3,2,1,2,3,4,3,2]</li>\n<li>symmetric：镜像填充，最后一个像素镜像[1, 2, 3, 4]——&gt;[2,1,1,2,3,4,4,3]</li>\n</ol>\n</li>\n<li><p>fill：constant时，设置填充的像素值</p>\n</li>\n</ul>\n<h5 id=\"transforms-ColorJitter\"><a href=\"#transforms-ColorJitter\" class=\"headerlink\" title=\"transforms.ColorJitter\"></a>transforms.ColorJitter</h5><p>调整亮度，对比度，饱和度和色相</p>\n<p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922215447677.png\" alt=\"image-20210922215447677\"></p>\n<ul>\n<li>brightness：亮度调整参数。当为a时，从[max(0, 1-a), 1+a]中随机选择，当为（a，b）时，从[a, b]中选择</li>\n<li>contrast：对比度参数，同上</li>\n<li>saturation：饱和度参数，同上</li>\n<li>hue：色相参数，当为a时，从[-a，a]中选择参数（0&lt;= a &lt;=0.5）当为（a，b）时，从[a, b]中选择参数，注：-0.5 &lt;= a &lt;= b &lt;= 0.5</li>\n</ul>\n<h5 id=\"transforms-Grayscale-RandomGrayscale\"><a href=\"#transforms-Grayscale-RandomGrayscale\" class=\"headerlink\" title=\"transforms.Grayscale/RandomGrayscale\"></a>transforms.Grayscale/RandomGrayscale</h5><p>依据概率将图片转换为灰度图</p>\n<p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922220031371.png\" alt=\"image-20210922220031371\"></p>\n<ul>\n<li>num_output_channels：输出通道数只能设置为1或3</li>\n<li>p：概率值，图像被转换为灰度图的概率</li>\n</ul>\n<h5 id=\"transforms-RandomAffine\"><a href=\"#transforms-RandomAffine\" class=\"headerlink\" title=\"transforms.RandomAffine\"></a>transforms.RandomAffine</h5><p>对图像进行仿射变换，仿射变换是二维的线性变换，由五种基本原子变换构成，分别是<strong>旋转</strong>、<strong>平移</strong>、<strong>缩放</strong>、<strong>错切</strong>和<strong>翻转</strong>。</p>\n<p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922220406992.png\" alt=\"image-20210922220406992\"></p>\n<ul>\n<li>degrees：旋转角度设置</li>\n<li>translate：平移区间设置，如（a，b），a设置宽（width），b设置高（height）。图像在宽维度平移的区间为-img_width* a &lt; dx &lt;img_width * a</li>\n<li>scale：缩放比例（以面积为单位）</li>\n<li>fill_color：填充颜色设置</li>\n<li>shear：错切角度设置，有水平错切和垂直错切。若为a，则仅在x轴错切，错切角度在（-a，a）之间；若为（a，b），则a设置x轴角度，b设置y的角度。若为（a，b，c，d），则a，b设置x轴角度，c，d设置y轴角度</li>\n<li>resample：重采样方式，用NEAREST，BILINEAR，BICUBIC</li>\n</ul>\n<h5 id=\"transforms-RandomErasing\"><a href=\"#transforms-RandomErasing\" class=\"headerlink\" title=\"transforms.RandomErasing\"></a>transforms.RandomErasing</h5><p>对图像进行随机遮挡</p>\n<ul>\n<li>p：概率值，执行该操作的概率</li>\n<li>scale：遮挡区域的面积</li>\n<li>ratio：遮挡区域长宽比</li>\n<li>value：设置遮挡区域的像素值（R，G，B）or（Gray）,或者任意字符串（则填入随机像素值，还挺好看）</li>\n</ul>\n<h5 id=\"transforms-Lambda\"><a href=\"#transforms-Lambda\" class=\"headerlink\" title=\"transforms.Lambda\"></a>transforms.Lambda</h5><p>用户自定义lambda方法</p>\n<p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922221913501.png\" alt=\"image-20210922221913501\"></p>\n<ul>\n<li><p>lambda：lambda匿名函数</p>\n<p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922222014416.png\" alt=\"image-20210922222014416\"></p>\n</li>\n</ul>\n<h4 id=\"4-transforms选择操作\"><a href=\"#4-transforms选择操作\" class=\"headerlink\" title=\"4. transforms选择操作\"></a>4. transforms选择操作</h4><h5 id=\"transforms-RandomChoice\"><a href=\"#transforms-RandomChoice\" class=\"headerlink\" title=\"transforms.RandomChoice\"></a>transforms.RandomChoice</h5><p>一组transforms方法中每次随机挑选一个</p>\n<p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922222201258.png\" alt=\"image-20210922222201258\"></p>\n<h5 id=\"transforms-RandomApply\"><a href=\"#transforms-RandomApply\" class=\"headerlink\" title=\"transforms.RandomApply\"></a>transforms.RandomApply</h5><p>依据概率执行一组transforms操作</p>\n<p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922222233010.png\" alt=\"image-20210922222233010\"></p>\n<h5 id=\"transforms-RandomOrder\"><a href=\"#transforms-RandomOrder\" class=\"headerlink\" title=\"transforms.RandomOrder\"></a>transforms.RandomOrder</h5><p>对一组transforms操作打乱顺序</p>\n<p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922222310180.png\" alt=\"image-20210922222310180\"></p>\n<h4 id=\"5-自定义transforms\"><a href=\"#5-自定义transforms\" class=\"headerlink\" title=\"5. 自定义transforms\"></a>5. 自定义transforms</h4><p>自定义transforms要素：</p>\n<ul>\n<li>仅接受一个参数，返回一个参数</li>\n<li>注意上游和下游的操作</li>\n</ul>\n<p><img src=\"/2021/09/26/PyTorch-DataProcess-Dataset/image-20210922222513169.png\" alt=\"image-20210922222513169\"></p>\n<h5 id=\"椒盐噪声\"><a href=\"#椒盐噪声\" class=\"headerlink\" title=\"椒盐噪声\"></a>椒盐噪声</h5><p>椒盐噪声又称脉冲噪声，是一种随机出现的白点或者黑点，白点称为盐噪声，黑点称为椒噪声</p>\n<p><strong>信噪比（Signal-Noise Rate，SNR）</strong>：衡量噪声的比例。</p>\n<h4 id=\"6-数据增强思想\"><a href=\"#6-数据增强思想\" class=\"headerlink\" title=\"6. 数据增强思想\"></a>6. 数据增强思想</h4><p>原则：让训练集与测试集更接近</p>\n<ul>\n<li><strong>空间位置</strong>：平移</li>\n<li><strong>色彩</strong>：灰度图，色彩抖动</li>\n<li><strong>形状</strong>：仿射变换</li>\n<li><strong>上下文场景</strong>：遮挡，填充</li>\n</ul>\n","categories":["PyTorch框架学习"],"tags":["PyTorch"]},{"title":"PythonLibraries","url":"/2021/09/27/PythonLabraries/","content":"<h1 id=\"一些好用且常用的库\"><a href=\"#一些好用且常用的库\" class=\"headerlink\" title=\"一些好用且常用的库\"></a>一些好用且常用的库</h1><h2 id=\"collections\"><a href=\"#collections\" class=\"headerlink\" title=\"collections\"></a>collections</h2><h3 id=\"namedtuple\"><a href=\"#namedtuple\" class=\"headerlink\" title=\"namedtuple()\"></a>namedtuple()</h3><p>collections模块的namedtuple子类不仅可以使用索引 (index) 访问item，还可以通过类似字典的方式（key-value）进行访问。在仿真引擎中对机械臂进行控制建模时，需要对同一个关节有多维度的设定，此时namedtuple用起来就很方便。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 适用于仿真中处理描述机器人运动状态和设定的各种参数</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> namedtuple</span><br><span class=\"line\"></span><br><span class=\"line\">author = namedtuple(<span class=\"string\">&#x27;author&#x27;</span>, [<span class=\"string\">&#x27;firstname&#x27;</span>, <span class=\"string\">&#x27;lastname&#x27;</span>])</span><br><span class=\"line\">me = author(Yiting, Chen)</span><br><span class=\"line\"><span class=\"built_in\">print</span> me.firstname, me.lastname</span><br><span class=\"line\"><span class=\"built_in\">print</span> me[<span class=\"number\">0</span>], me[<span class=\"number\">1</span>]</span><br><span class=\"line\">me = author._make([Eating, Chen])</span><br><span class=\"line\"><span class=\"built_in\">print</span> me.firstname, me.lastname</span><br><span class=\"line\">me = me._replace(firstname = Eating716)</span><br><span class=\"line\"><span class=\"built_in\">print</span> me.firstname, me.lastname</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># results</span></span><br><span class=\"line\"><span class=\"comment\"># Yiting Chen</span></span><br><span class=\"line\"><span class=\"comment\"># Yiting Chen</span></span><br><span class=\"line\"><span class=\"comment\"># Eating Chen</span></span><br><span class=\"line\"><span class=\"comment\"># Eating716 Chen</span></span><br><span class=\"line\"> </span><br><span class=\"line\">websites = [</span><br><span class=\"line\">    (<span class=\"string\">&#x27;Sohu&#x27;</span>, <span class=\"string\">&#x27;http://www.sohu.com/&#x27;</span>, <span class=\"string\">u&#x27;张朝阳&#x27;</span>),</span><br><span class=\"line\">    (<span class=\"string\">&#x27;Baidu&#x27;</span>, <span class=\"string\">&#x27;http://www.baidu.com.cn/&#x27;</span>, <span class=\"string\">u&#x27;李彦宏&#x27;</span>),</span><br><span class=\"line\">    (<span class=\"string\">&#x27;163&#x27;</span>, <span class=\"string\">&#x27;http://www.163.com/&#x27;</span>, <span class=\"string\">u&#x27;丁磊&#x27;</span>)</span><br><span class=\"line\">]</span><br><span class=\"line\"> </span><br><span class=\"line\">Website = namedtuple(<span class=\"string\">&#x27;Website&#x27;</span>, [<span class=\"string\">&#x27;name&#x27;</span>, <span class=\"string\">&#x27;url&#x27;</span>, <span class=\"string\">&#x27;founder&#x27;</span>])</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">for</span> website <span class=\"keyword\">in</span> websites:</span><br><span class=\"line\">    website = Website._make(website)</span><br><span class=\"line\">    <span class=\"built_in\">print</span> website</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\"># results:</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Website(name=&#x27;Sohu&#x27;, url=&#x27;http://www.sohu.com/&#x27;, founder=u&#x27;\\u5f20\\u671d\\u9633&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Website(name=&#x27;Baidu&#x27;, url=&#x27;http://www.baidu.com.cn/&#x27;, founder=u&#x27;\\u738b\\u5fd7\\u4e1c&#x27;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Website(name=&#x27;163&#x27;, url=&#x27;http://www.163.com/&#x27;, founder=u&#x27;\\u4e01\\u78ca&#x27;)</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n","categories":["PythonCoding"],"tags":["Python"]},{"title":"Prologue💬","url":"/2021/09/26/SayHi/","content":"<p><strong><font size=\"10\">Hi, this is Yiting Chen🚬. </font></strong></p>\n<p><strong><font size=\"6\">I’m an amateur in Robotics &amp; ML🤖. </font></strong></p>\n<p><strong><font size=\"4\">Wishing to make some progress in this area📈 </font></strong></p>\n<p><strong><font size=\"6\">This blog is for recording my daily study📝.</font></strong></p>\n<p><strong><font size=\"4\">Please let me 📬(<a href=\"mailto:&#99;&#104;&#x65;&#110;&#x79;&#x69;&#x74;&#x69;&#110;&#103;&#64;&#x77;&#x68;&#117;&#x2e;&#x65;&#x64;&#x75;&#46;&#99;&#110;\">&#99;&#104;&#x65;&#110;&#x79;&#x69;&#x74;&#x69;&#110;&#103;&#64;&#x77;&#x68;&#117;&#x2e;&#x65;&#x64;&#x75;&#46;&#99;&#110;</a>) know if there is any question. </font></strong></p>\n"},{"title":"PyTorch-ModelsConstruction","url":"/2021/09/26/PyTorch-ModelsConstruction/","content":"<h2 id=\"三-PyTorch模型搭建（Models）\"><a href=\"#三-PyTorch模型搭建（Models）\" class=\"headerlink\" title=\"三. PyTorch模型搭建（Models）\"></a>三. PyTorch模型搭建（Models）</h2><h3 id=\"A-模型创建步骤与nn-Module\"><a href=\"#A-模型创建步骤与nn-Module\" class=\"headerlink\" title=\"A. 模型创建步骤与nn.Module\"></a>A. 模型创建步骤与nn.Module</h3><h4 id=\"1-网路模型创建步骤\"><a href=\"#1-网路模型创建步骤\" class=\"headerlink\" title=\"1. 网路模型创建步骤\"></a>1. 网路模型创建步骤</h4><p>模型（nn.Module）：</p>\n<ul>\n<li>模型创建：<ul>\n<li>构建网络<ul>\n<li>卷积层，池化层，激活函数层等等</li>\n</ul>\n</li>\n<li>拼接网络<ul>\n<li>LeNet，ResNet，AlexNet等等</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>权值初始化<ul>\n<li>Xavier，Kaiming，正态分布，均匀分布等</li>\n</ul>\n</li>\n</ul>\n<p>例：</p>\n<p>LeNet模型如下：</p>\n<p><img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923111900159-16326570841981.png\" alt=\"image-20210923111900159\"></p>\n<p>LeNet计算图：</p>\n<p><img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923111953621-16326570841983.png\" alt=\"image-20210923111953621\"></p>\n<p>从左向右便是前向传播。</p>\n<p>构建模型的两个要素：</p>\n<ul>\n<li>构建子模块：conv，pool，fc等等：在__ init __()中实现</li>\n<li>拼接子模块：按一定的顺序，一定的拓扑结构进行组装：在forward()当中实现</li>\n</ul>\n<p>代码示例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">LeNet</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># 模型初始化，就是构建子模块</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, classes</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(LeNet, self).__init__()</span><br><span class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">16</span>*<span class=\"number\">5</span>*<span class=\"number\">5</span>, <span class=\"number\">120</span>)</span><br><span class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>)</span><br><span class=\"line\">        self.fc3 = nn.Linear(<span class=\"number\">84</span>, classes)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 拼接子模块，也就是前向传播的一个实现</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">        out = F.relu(self.conv1(x))</span><br><span class=\"line\">        out = F.max_pool2d(out, <span class=\"number\">2</span>)</span><br><span class=\"line\">        out = F.relu(self.conv2(out))</span><br><span class=\"line\">        out = out.view(out.size(<span class=\"number\">0</span>), -<span class=\"number\">1</span>)</span><br><span class=\"line\">        out = F.relu(self.fc1(out))</span><br><span class=\"line\">        out = F.relu(self.fc2(out))</span><br><span class=\"line\">        out = self.fc3(out)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br><span class=\"line\">    </span><br><span class=\"line\">        </span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"2-nn-Module\"><a href=\"#2-nn-Module\" class=\"headerlink\" title=\"2. nn.Module\"></a>2. nn.Module</h4><p><strong>torch.nn</strong>：</p>\n<ul>\n<li>nn.Parameter：张量子类，表示可学习的参数，如weight，bias</li>\n<li>nn.Module：所有网络层的基类，管理网络的属性（例如LeNet要继承它）</li>\n<li>nn.functional：函数的具体实现，如卷积，池化，激活函数等等</li>\n<li>nn.init：提供了丰富的参数初始化的方法</li>\n</ul>\n<p><strong>nn.Module</strong>：</p>\n<ul>\n<li><strong>parameters（重要）</strong>：存储管理nn.Parameter类</li>\n<li><strong>modules（重要）</strong>：存储管理nn.Module类（构建子模块）</li>\n<li>buffers：存储管理缓冲属性，如BN层中的running_mean</li>\n<li>***_hooks：存储管理钩子函数（共五个和hooks有关的字典）</li>\n</ul>\n<p><strong>在对创建的新module（例如LeNet）进行赋值时（__ init  <strong>()中的属性构建），会被一个</strong> setAttr __()函数进行拦截，判断其为parameter类还是module类。然后在存储进各自对应的字典当中。</strong></p>\n<p><strong>总结</strong>：</p>\n<ul>\n<li>一个module可以包含多个子module</li>\n<li>一个module相当于一个运算，必须实现forward()函数</li>\n<li>每个module都有8个字典管理它的属性</li>\n</ul>\n<h3 id=\"B-模型容器与AlexNet构建\"><a href=\"#B-模型容器与AlexNet构建\" class=\"headerlink\" title=\"B. 模型容器与AlexNet构建\"></a>B. 模型容器与AlexNet构建</h3><h4 id=\"1-Containers\"><a href=\"#1-Containers\" class=\"headerlink\" title=\"1. Containers\"></a>1. Containers</h4><p><strong>Containers（容器）</strong>:</p>\n<ul>\n<li><strong>nn.Sequential</strong>：按顺序的将一组网络层包裹起来</li>\n<li><strong>nn.ModuleList</strong>：像python的list一样包装多个网络层</li>\n<li><strong>nn.ModuleDict</strong>：像python的dict一样包装多个网络层</li>\n</ul>\n<h5 id=\"nn-Sequential\"><a href=\"#nn-Sequential\" class=\"headerlink\" title=\"nn.Sequential\"></a>nn.Sequential</h5><p><strong>nn.Sequential</strong>是nn.Module的容器，<strong>按顺序</strong>包装一组网络层：</p>\n<ul>\n<li>顺序性：各网络层之间严格按照顺序构建</li>\n<li>自带forward()：自带的forward里，通过for循环一次执行前向运算</li>\n</ul>\n<p>例1：用Sequential搭建网络</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">LeNetSequential</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># 构建子模块</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, classes</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(LeNetSequential, self).__init__()</span><br><span class=\"line\">        self.features = nn.Sequential(</span><br><span class=\"line\">        \tnn.Conv2d(<span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>),</span><br><span class=\"line\">            nn.ReLU(),</span><br><span class=\"line\">            nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">            nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, <span class=\"number\">5</span>),</span><br><span class=\"line\">            nn.ReLU(),</span><br><span class=\"line\">            nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>),)</span><br><span class=\"line\">       \t</span><br><span class=\"line\">        self.classifier = nn.Sequential(</span><br><span class=\"line\">        \tnn.Linear(<span class=\"number\">6</span>*<span class=\"number\">5</span>*<span class=\"number\">5</span>, <span class=\"number\">120</span>),</span><br><span class=\"line\">            nn.ReLU(),</span><br><span class=\"line\">            nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>),</span><br><span class=\"line\">            nn.ReLU(),</span><br><span class=\"line\">            nn.Linear(<span class=\"number\">84</span>, classes),)</span><br><span class=\"line\">    <span class=\"comment\"># 拼接子模块</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">        x = self.features(x)</span><br><span class=\"line\">        x = x.view(x.size()[<span class=\"number\">0</span>], -<span class=\"number\">1</span>)</span><br><span class=\"line\">        x = self.classifier(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n\n<p>通常以全连接层为界限，分为特征提取器和分类器两部分。</p>\n<p>例2：对Sequential输入OrderDict</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">LeNetSequentialOrderDict</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># 构建子模块</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, classes</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(LeNetSequentialOrderDict, self).__init__()</span><br><span class=\"line\">        self.features = nn.Sequential(OrderDict(&#123;</span><br><span class=\"line\">        \t<span class=\"string\">&#x27;conv1&#x27;</span> : nn.Conv2d(<span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>),</span><br><span class=\"line\">            <span class=\"string\">&#x27;relu1&#x27;</span> : nn.ReLU(),</span><br><span class=\"line\">            <span class=\"string\">&#x27;pool1&#x27;</span> : nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">            </span><br><span class=\"line\">            <span class=\"string\">&#x27;conv2&#x27;</span> : nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, <span class=\"number\">5</span>),</span><br><span class=\"line\">            <span class=\"string\">&#x27;relu2&#x27;</span> : nn.ReLU(),</span><br><span class=\"line\">            <span class=\"string\">&#x27;pool2&#x27;</span> : nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>,stride=<span class=\"number\">2</span>),</span><br><span class=\"line\">        &#125;))</span><br><span class=\"line\">       \t</span><br><span class=\"line\">        self.classifier = nn.Sequential(OrderDict(&#123;</span><br><span class=\"line\">        \t<span class=\"string\">&#x27;fc1&#x27;</span> : nn.Linear(<span class=\"number\">6</span>*<span class=\"number\">5</span>*<span class=\"number\">5</span>, <span class=\"number\">120</span>),</span><br><span class=\"line\">            <span class=\"string\">&#x27;relu3&#x27;</span> : nn.ReLU(),</span><br><span class=\"line\">            <span class=\"string\">&#x27;fc2&#x27;</span> : nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>),</span><br><span class=\"line\">            <span class=\"string\">&#x27;relu4&#x27;</span> : nn.ReLU(),</span><br><span class=\"line\">            <span class=\"string\">&#x27;fc3&#x27;</span> : nn.Linear(<span class=\"number\">84</span>, classes),</span><br><span class=\"line\">        &#125;))</span><br><span class=\"line\">    <span class=\"comment\"># 拼接子模块</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">        x = self.features(x)</span><br><span class=\"line\">        x = x.view(x.size()[<span class=\"number\">0</span>], -<span class=\"number\">1</span>)</span><br><span class=\"line\">        x = self.classifier(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n\n<h5 id=\"nn-ModuleList\"><a href=\"#nn-ModuleList\" class=\"headerlink\" title=\"nn.ModuleList\"></a>nn.ModuleList</h5><p>nn.ModuleList是nn.Module的容器，用于包装一组网络层，以迭代方式调用一组网络层。</p>\n<p><strong>主要方法</strong>：</p>\n<ul>\n<li>append()：在ModuleList后面添加网络层</li>\n<li>extend()：拼接两个ModuleList</li>\n<li>insert()：指定在ModuleList中位置插入网络层</li>\n</ul>\n<p>例1：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ModuleList</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(ModuleList, self).__init__()</span><br><span class=\"line\">        self.linears = nn.ModuleList([nn.Linear(<span class=\"number\">10</span>, <span class=\"number\">10</span>) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">20</span>)])</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i, linear <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(self.linears):</span><br><span class=\"line\">            x = linear(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\">    </span><br></pre></td></tr></table></figure>\n\n<h5 id=\"nn-ModuleDict\"><a href=\"#nn-ModuleDict\" class=\"headerlink\" title=\"nn.ModuleDict\"></a>nn.ModuleDict</h5><p><strong>nn.ModuleDict</strong>是 nn.module的容器， 用于包装一组网络层，以<strong>索引</strong>的方式调用网络层。</p>\n<ul>\n<li>clear()：清空ModuleDict</li>\n<li>items()：返回可迭代的键值对(key-value pairs)</li>\n<li>keys()：返回字典的键(key)</li>\n<li>values()：返回字典的值(value)</li>\n<li>pop()：返回一对键值，并从字典中删除</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ModuleDict</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(ModuleDict, self).__init__()</span><br><span class=\"line\">        self.choices = nn.ModuleDict(&#123;</span><br><span class=\"line\">            <span class=\"string\">&#x27;conv&#x27;</span> : nn.Conv2d(<span class=\"number\">10</span>, <span class=\"number\">10</span>, <span class=\"number\">3</span>),</span><br><span class=\"line\">            <span class=\"string\">&#x27;pool&#x27;</span> : nn.MaxPool2d(<span class=\"number\">3</span>)</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">        </span><br><span class=\"line\">        self.activations = nn.ModuleDict(&#123;</span><br><span class=\"line\">            <span class=\"string\">&#x27;relu&#x27;</span> : nn.ReLU(),</span><br><span class=\"line\">            <span class=\"string\">&#x27;prelu&#x27;</span> : nn.PReLU()</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x, choice, act</span>):</span></span><br><span class=\"line\">        x = self.choices[choise](x)</span><br><span class=\"line\">        x = self.activations[act](x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\">    </span><br><span class=\"line\">output = net(fake_img, <span class=\"string\">&#x27;conv&#x27;</span>, <span class=\"string\">&#x27;prelu&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<h5 id=\"容器总结：\"><a href=\"#容器总结：\" class=\"headerlink\" title=\"容器总结：\"></a>容器总结：</h5><ul>\n<li>nn.Sequential：<strong>顺序性</strong>，个网络层之间严格按顺序执行，常用于block构建</li>\n<li>nn.ModuleList：<strong>迭代性</strong>，常用于大量重复网络构建，通过for循环实现重复构建</li>\n<li>nn.ModuleDict：<strong>索引性</strong>，常用于可选择的网络层</li>\n</ul>\n<h4 id=\"2-AlexNet\"><a href=\"#2-AlexNet\" class=\"headerlink\" title=\"2.AlexNet\"></a>2.AlexNet</h4><p><img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923150239978-16326570841982.png\" alt=\"image-20210923150239978\"></p>\n<p>前面卷积池化的部分组装成<strong>features</strong>，后面的全连接部分组装成<strong>classifier</strong></p>\n<h3 id=\"C-nn网络层-卷积层\"><a href=\"#C-nn网络层-卷积层\" class=\"headerlink\" title=\"C. nn网络层-卷积层\"></a>C. nn网络层-卷积层</h3><h4 id=\"1-1d-2d-3d卷积\"><a href=\"#1-1d-2d-3d卷积\" class=\"headerlink\" title=\"1. 1d/2d/3d卷积\"></a>1. 1d/2d/3d卷积</h4><p>卷积运算：卷积核在输入信号（图像）上滑动，相应位置上进行<strong>乘加</strong>。</p>\n<p>卷积核：又称为滤波器，过滤器，可认为是某种模式，特征。</p>\n<p><img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923195219275-16326570841984.png\" alt=\"image-20210923195219275\"></p>\n<p>卷积过程类似于用一个模板去图像上寻找与他类似的区域，与卷积核模式越相似，激活值越高，从而实现特征提取。</p>\n<p>AlexNet卷积核可视化（如下图），发现卷积核学习到的是边缘，条纹，色彩着一些细节模式。<img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923195435689-16326570841985.png\" alt=\"image-20210923195435689\" style=\"zoom:50%;\"></p>\n<p><strong>卷积维度</strong>：一般情况下，卷积核<strong>在几个维度下滑动，就是几维卷积</strong>。</p>\n<p>一个卷积核在一个信号上的几维卷积。</p>\n<p>一维卷积：</p>\n<img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923201126680-16326570841987.png\" alt=\"image-20210923201126680\" style=\"zoom:33%;\">\n\n<p>二维卷积：</p>\n<img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923201301917-16326570841986.png\" alt=\"image-20210923201301917\" style=\"zoom:33%;\">\n\n<p>三维卷积：                        <img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923201323469-16326570841988.png\" alt=\"image-20210923201323469\" style=\"zoom:33%;\">、</p>\n<p>nn.Conv2d</p>\n<p>对多个二维信号进行二维卷积</p>\n<p>主要参数：</p>\n<ul>\n<li>in_channels：输入通道数</li>\n<li>out_channels：输出通道数，等价于卷积核个数</li>\n<li>kernel_size：卷积核尺寸</li>\n<li>stride：步长（每次滑动经过多少像素）</li>\n<li>padding：填充个数（保持输入和输出尺寸匹配）</li>\n<li>dilation：孔洞卷积大小（常用于图像分割任务，用于提升感受区域）</li>\n<li>groups：分组卷积设置</li>\n<li>bias：偏置</li>\n</ul>\n<p>图像尺寸变化：</p>\n<p>（简化版）：outsize = （Insize - kernelsize）/ stride + 1</p>\n<p>（完整版）：<img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923202218467-16326570841989.png\" alt=\"image-20210923202218467\"></p>\n<h4 id=\"2-卷积-nn-Conv2d\"><a href=\"#2-卷积-nn-Conv2d\" class=\"headerlink\" title=\"2. 卷积-nn.Conv2d()\"></a>2. 卷积-nn.Conv2d()</h4><p><img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923205449306-163265708419810.png\" alt=\"image-20210923205449306\"></p>\n<h4 id=\"3-转置卷积-nn-ConvTranspose\"><a href=\"#3-转置卷积-nn-ConvTranspose\" class=\"headerlink\" title=\"3.转置卷积-nn.ConvTranspose\"></a>3.转置卷积-nn.ConvTranspose</h4><p>转置卷积又称为反卷积（Deconvolution)和部分跨越卷积（Fractionally strided Convolution），用于对图像进行上采样（UpSample）。</p>\n<p><img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923210001350-163265708419811.png\" alt=\"image-20210923210001350\"></p>\n<p>nn.Transpose2d</p>\n<p>转置卷积实现上采样</p>\n<p><img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923210527210-163265708419812.png\" alt=\"image-20210923210527210\"></p>\n<p>主要参数：</p>\n<ul>\n<li>in_channels：输入通道数</li>\n<li>out_channels：输出通道数，等价于卷积核个数</li>\n<li>kernel_size：卷积核尺寸</li>\n<li>stride：步长（每次滑动经过多少像素）</li>\n<li>padding：填充个数（保持输入和输出尺寸匹配）</li>\n<li>dilation：孔洞卷积大小（常用于图像分割任务，用于提升感受区域）</li>\n<li>groups：分组卷积设置</li>\n<li>bias：偏置</li>\n</ul>\n<p>转置卷积的尺寸计算：</p>\n<p>（简化版）outsize = （insize - 1）*stride + kernelsize</p>\n<p>（完整版）：<br><img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923210734836-163265708419814.png\" alt=\"image-20210923210734836\"></p>\n<h3 id=\"D-池化、线性、激活函数层\"><a href=\"#D-池化、线性、激活函数层\" class=\"headerlink\" title=\"D. 池化、线性、激活函数层\"></a>D. 池化、线性、激活函数层</h3><h4 id=\"1-池化层——Pooling-Layer\"><a href=\"#1-池化层——Pooling-Layer\" class=\"headerlink\" title=\"1. 池化层——Pooling Layer\"></a>1. 池化层——Pooling Layer</h4><p>池化运算：对信号进行 <strong>“收集”</strong> 并 <strong>“总结”</strong>，类似水池收集水资源，因而得名池化层。</p>\n<p><strong>“收集”</strong>：多变少</p>\n<p><strong>“总结”</strong>：最大值 / 平均值</p>\n<img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923211736432-163265708419813.png\" alt=\"image-20210923211736432\" style=\"zoom:50%;\">\n\n<img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923211835083-163265708419815.png\" alt=\"image-20210923211835083\" style=\"zoom:50%;\">\n\n<h5 id=\"nn-MaxPool2d\"><a href=\"#nn-MaxPool2d\" class=\"headerlink\" title=\"nn.MaxPool2d\"></a>nn.MaxPool2d</h5><p>对二维信号（图像）进行最大值池化</p>\n<p><img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923212056580-163265708419916.png\" alt=\"image-20210923212056580\"></p>\n<p>主要参数：</p>\n<ul>\n<li>kernel_size：池化核尺寸</li>\n<li>stride：步长（步长通常与窗口大小一样）</li>\n<li>padding：填充个数</li>\n<li>dilation：池化核间隔大小</li>\n<li>ceil_mode：尺寸向上取整</li>\n<li>return_indices：记录池化像素索引（记录最大值在的位置，在反池化的时候把最大值放在对应的位置上）</li>\n</ul>\n<p><img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923212830427-163265708419917.png\" alt=\"image-20210923212830427\"></p>\n<h5 id=\"nn-AvgPool2d\"><a href=\"#nn-AvgPool2d\" class=\"headerlink\" title=\"nn.AvgPool2d\"></a>nn.AvgPool2d</h5><p>对二维信号（图像）进行平均值池化</p>\n<p><img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923213658337-163265708419918.png\" alt=\"image-20210923213658337\"></p>\n<p>主要参数：</p>\n<ul>\n<li>kernel_size：池化核尺寸</li>\n<li>stride：步长（步长通常与窗口大小一样）</li>\n<li>padding：填充个数</li>\n<li>ceil_mode：尺寸向上取整</li>\n<li>count_include_pad：填充值用于计算</li>\n<li>divisor_override：除法因子</li>\n</ul>\n<h5 id=\"nn-MaxUnpool2d\"><a href=\"#nn-MaxUnpool2d\" class=\"headerlink\" title=\"nn.MaxUnpool2d\"></a>nn.MaxUnpool2d</h5><p><img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923221525532-163265708419919.png\" alt=\"image-20210923221525532\"></p>\n<p>对二维信号（图像）进行最大值池化上采样</p>\n<p>主要参数：</p>\n<ul>\n<li>kernel_size：池化核尺寸</li>\n<li>stride：步长</li>\n<li>padding：填充个数</li>\n</ul>\n<p><img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923212830427-163265708419917.png\" alt=\"image-20210923212830427\"></p>\n<p>在forward( )的时候要传入索引层，就是indices。</p>\n<h4 id=\"2-线性层——Linear-Layer\"><a href=\"#2-线性层——Linear-Layer\" class=\"headerlink\" title=\"2. 线性层——Linear Layer\"></a>2. 线性层——Linear Layer</h4><p>线性层又称全连接层，其<strong>每个神经元与上一层所有神经元相连</strong>。</p>\n<p>实现对前一层的<strong>线性组合，线性变换</strong></p>\n<img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923222244138-163265708419920.png\" alt=\"image-20210923222244138\" style=\"zoom:25%;\">\n\n<h5 id=\"nn-Linear\"><a href=\"#nn-Linear\" class=\"headerlink\" title=\"nn.Linear\"></a>nn.Linear</h5><p>对一维信号（向量）进行线性组合</p>\n<p><img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923222813962-163265708419921.png\" alt=\"image-20210923222813962\"></p>\n<p>主要参数：</p>\n<ul>\n<li>in_features：输入结点数</li>\n<li>out_features：输出结点数</li>\n<li>bias：是否需要偏置</li>\n</ul>\n<p>计算公式：<img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923222749821-163265708419922.png\" alt=\"image-20210923222749821\" style=\"zoom:50%;\"></p>\n<h4 id=\"3-激活函数层——Activation-Layer\"><a href=\"#3-激活函数层——Activation-Layer\" class=\"headerlink\" title=\"3. 激活函数层——Activation Layer\"></a>3. 激活函数层——Activation Layer</h4><p>激活函数对特征进行<strong>非线性变换</strong>，赋予多层神经网络具有<strong>深度意义</strong>。</p>\n<p>如果<strong>没有</strong>非线性变化，<strong>无论</strong>多少层线性层叠加都没有意义，一个矩阵就都能搞定。</p>\n<h5 id=\"nn-Sigmoid\"><a href=\"#nn-Sigmoid\" class=\"headerlink\" title=\"nn.Sigmoid\"></a>nn.Sigmoid</h5><img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923223344685-163265708419923.png\" alt=\"image-20210923223344685\" style=\"zoom:33%;\">\n\n<p>特性：</p>\n<ul>\n<li>输出值在（0，1），符合概率</li>\n<li>导数范围是[0, 0.25]，容易梯度消失</li>\n<li>输出为非0均值，破坏数据分布</li>\n</ul>\n<img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923223502929-163265708419924.png\" alt=\"image-20210923223502929\" style=\"zoom: 25%;\">\n\n<h5 id=\"nn-tanh\"><a href=\"#nn-tanh\" class=\"headerlink\" title=\"nn.tanh\"></a>nn.tanh</h5><img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923223628333-163265708419925.png\" alt=\"image-20210923223628333\" style=\"zoom: 50%;\">\n\n<p>特性：</p>\n<ul>\n<li>输出值在（-1，1），数据符合0均值</li>\n<li>导数范围是（0，1），易导致梯度消失</li>\n</ul>\n<img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923223736195-163265708419926.png\" alt=\"image-20210923223736195\" style=\"zoom: 25%;\">\n\n<h5 id=\"nn-ReLU\"><a href=\"#nn-ReLU\" class=\"headerlink\" title=\"nn.ReLU\"></a>nn.ReLU</h5><img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923223932829-163265708419927.png\" alt=\"image-20210923223932829\" style=\"zoom: 33%;\">\n\n<p>特性：</p>\n<ul>\n<li>输出值均为正数，负半轴导致死神经元（有一部分神经元就死掉了）</li>\n<li>导数是1（不改变梯度的尺度），缓解梯度消失，但易引发梯度爆炸</li>\n</ul>\n<img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923224046062-163265708419928.png\" alt=\"image-20210923224046062\" style=\"zoom:25%;\">\n\n<h5 id=\"nn-ReLU变式\"><a href=\"#nn-ReLU变式\" class=\"headerlink\" title=\"nn.ReLU变式\"></a>nn.ReLU变式</h5><p>nn.LeakyReLU</p>\n<ul>\n<li>negative_slope：负半轴斜率</li>\n</ul>\n<p>nn.PReLU</p>\n<ul>\n<li>init：可学习斜率</li>\n</ul>\n<p>nn.RReLU</p>\n<p>R代表random</p>\n<ul>\n<li>lower：均匀分布下限</li>\n<li>upper：均匀分布上限</li>\n</ul>\n<img src=\"/2021/09/26/PyTorch-ModelsConstruction/image-20210923224438706-163265708419929.png\" alt=\"image-20210923224438706\" style=\"zoom:25%;\">\n\n<p><strong>还有很多其他改进</strong>，日后补充</p>\n","categories":["PyTorch框架学习"],"tags":["PyTorch"]},{"title":"用随机梯度下降来优化人生","url":"/2021/09/27/limu-SGD/","content":"<p>作者：李沐 | 来源：知乎</p>\n<p><strong>要有目标</strong>。你需要有目标。短的也好，长的也好。认真定下的也好，别人那里捡的也好。就跟随机梯度下降需要有个目标函数一样。</p>\n<p><strong>目标要大</strong>。不管是人生目标还是目标函数，你最好不要知道最后可以走到哪里。如果你知道，那么你的目标就太简单了，可能是个凸函数。你可以在一开始的时候给自己一些小目标，例如期末考个80分，训练一个线性模型。但接下来得有更大的目标，财富自由也好，100亿参数的变形金刚也好，得足够一颗赛艇。</p>\n<p><strong>坚持走</strong>。不管你的目标多复杂，随机梯度下降都是最简单的。每一次你找一个大概还行的方向（梯度），然后迈一步（下降）。两个核心要素是方向和步子的长短。但最重要的是你得一直走下去，能多走几步就多走几步。</p>\n<p><strong>痛苦的卷</strong>。每一步里你都在试图改变你自己或者你的模型参数。改变带来痛苦。但没有改变就没有进步。你过得很痛苦不代表在朝着目标走，因为你可能走反了。但过得很舒服那一定在原地踏步。需要时刻跟自己作对。</p>\n<p><strong>可以躺平</strong>。你用你内心的激情来迈步子。步子太小走不动，步子太长容易过早消耗掉了激情。周期性的调大调小步长效果挺好。所以你可以时不时休息休息。</p>\n<p><strong>四处看看</strong>。每一步走的方向是你对世界的认识。如果你探索的世界不怎么变化，那么要么你的目标太简单，要么你困在你的舒适区了。随机梯度下降的第一个词是随机，就是你需要四处走走，看过很多地方，做些错误的决定，这样你可以在前期迈过一些不是很好的舒适区。</p>\n<p><strong>快也是慢</strong>。你没有必要特意去追求找到最好的方向和最合适的步子。你身边当然会有幸运之子，他们每一步都在别人前面。但经验告诉我们，随机梯度下降前期进度太快，后期可能乏力。就是说你过早的找到一个舒适区，忘了世界有多大。所以你不要急，前面徘徊一段时间不是坏事。成名无需太早。</p>\n<p><strong>赢在起点</strong>。起点当然重要。如果你在终点附近起步，可以少走很多路。而且终点附近的路都比较平，走着舒服。当你发现别人不如你的时候，看看自己站在哪里。可能你就是运气很好，赢在了起跑线。如果你跟别人在同一起跑线，不见得你能做更好。</p>\n<p><strong>很远也能到达</strong>。如果你是在随机起点，那么做好准备前面的路会非常不平坦。越远离终点，越人迹罕见。四处都是悬崖。但随机梯度下降告诉我们，不管起点在哪里，最后得到的解都差不多。当然这个前提是你得一直按照梯度的方向走下去。如果中间梯度炸掉了，那么你随机一个起点，调整步子节奏，重新来。</p>\n<p><strong>独一无二</strong>。也许大家有着差不多的目标，在差不多的时间毕业买房结婚生娃。但每一步里，每个人内心中看到的世界都不一样，导致走的路不一样。你如果跑多次随机梯度下降，在各个时间点的目标函数值可能都差不多，但每次的参数千差万别。不会有人关心你每次训练出来的模型里面参数具体是什么值，除了你自己。</p>\n<p><strong>简单最好</strong> 。当然有比随机梯度下降更复杂的算法。他们想每一步看想更远更准，想步子迈最大。但如果你的目标很复杂，简单的随机梯度下降反而效果最好。深度学习里大家都用它。关注当前，每次抬头瞄一眼世界，快速做个决定，然后迈一小步。小步快跑。只要你有目标，不要停，就能到达。</p>\n","categories":["杂文"],"tags":["杂文"]},{"title":"面向对象编程","url":"/2021/09/27/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B/","content":"<h2 id=\"面向对象编程\"><a href=\"#面向对象编程\" class=\"headerlink\" title=\"面向对象编程\"></a>面向对象编程</h2><h3 id=\"面向对象的思想\"><a href=\"#面向对象的思想\" class=\"headerlink\" title=\"面向对象的思想\"></a>面向对象的思想</h3><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 我们现在先整个类</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">human</span>(<span class=\"params\"><span class=\"built_in\">object</span></span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, nation</span>):</span></span><br><span class=\"line\">        self.planet = earth</span><br><span class=\"line\">        self.nation = nation</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">whereufrom</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;I come from %s&quot;</span> % self.nation)</span><br><span class=\"line\">        </span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Yiting</span>(<span class=\"params\">human</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, age, surname, nation</span>):</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">super</span>(<span class=\"params\">Yiting, self</span>).<span class=\"title\">__init__</span>(<span class=\"params\">nation</span>)</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">self</span>.<span class=\"title\">age</span> = <span class=\"title\">age</span></span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">self</span>.<span class=\"title\">surname</span> = <span class=\"title\">surname</span></span></span><br><span class=\"line\"><span class=\"function\">    \t<span class=\"title\">self</span>.<span class=\"title\">firstname</span> = &#x27;<span class=\"title\">Yiting</span>&#x27;</span></span><br><span class=\"line\"><span class=\"function\">        </span></span><br><span class=\"line\"><span class=\"function\">    <span class=\"title\">def</span> <span class=\"title\">eating</span>(<span class=\"params\">self, food</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Yiting is eating&quot;</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">whoamI</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;My name is Yiting &#123;&#125;&quot;</span>.<span class=\"built_in\">format</span>(self.surname))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.firstname + self.surname</span><br><span class=\"line\">    </span><br><span class=\"line\">me = Yiting(<span class=\"number\">21</span>, Chen, China)</span><br></pre></td></tr></table></figure>\n\n\n\n<ul>\n<li><p><strong>类(Class):</strong> 用来描述具有相同的属性和方法的对象的集合。它定义了该集合中每个对象所共有的属性和方法。对象是类的实例。<strong>实例化</strong>创建一个类的实例，类的具体对象。例如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">me = Yiting(<span class=\"number\">21</span>, Chen)</span><br></pre></td></tr></table></figure></li>\n<li><p><strong>类变量：</strong>类变量在整个实例化的对象中是公用的。类变量定义在类中且在函数体之外。类变量通常不作为实例变量使用。可以使用点号 <strong>.</strong> 来访问对象的属性。使用如下类的名称访问类变量:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">Yiting.age, Yiting.surname</span><br><span class=\"line\"><span class=\"comment\"># 21, Chen</span></span><br></pre></td></tr></table></figure></li>\n<li><p><strong>方法重写：</strong>如果<strong>从父类继承的方法不能满足子类的需求</strong>，可以对其进行改写，这个过程叫方法的覆盖（override），也称为方法的重写。</p>\n</li>\n<li><p><strong>局部变量：</strong>定义在方法中的变量，<strong>只作用于当前实例</strong>的类。</p>\n</li>\n</ul>\n<ul>\n<li><strong>实例变量：</strong>在类的声明中，属性是用变量来表示的。这种变量就称为实例变量，是在类声明的内部但是在类的其他成员方法之外声明的。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">Yiting.age, Yiting.surname, etc.</span><br></pre></td></tr></table></figure>\n\n\n\n<ul>\n<li><strong>继承：</strong>即一个派生类（derived class）继承基类（base class）的字段和方法。继承也允许把一个派生类的对象作为一个基类对象对待。例如，有这样一个设计：一个Dog类型的对象派生自Animal类，这是模拟”是一个（is-a）”关系（例如Dog是一个Animal）。Python 总是<strong>首先查找对应类型的方法，如果它不能在派生类中找到对应的方法，它才开始到基类中逐个查找。</strong>（先在本类中查找调用的方法，找不到才去基类中找）。</li>\n</ul>\n<ul>\n<li><p><strong>方法：</strong>类中定义的函数。__ init __()方法是一种特殊的方法，被称为类的构造函数或初始化方法，当创建了这个类的实例时就会调用该方法</p>\n</li>\n<li><p><strong>对象：</strong>通过类定义的数据结构实例。对象包括两个数据成员（类变量和实例变量）和方法。例如me就是对象。</p>\n</li>\n</ul>\n<h3 id=\"类的私有（保护）属性和方法\"><a href=\"#类的私有（保护）属性和方法\" class=\"headerlink\" title=\"类的私有（保护）属性和方法\"></a>类的私有（保护）属性和方法</h3><p>__ private_attrs：两个下划线开头，声明该属性为私有，<strong>不能在类的外部被使用或直接访问。</strong>在类内部的方法中使用时：self.__private_attrs。</p>\n<p>__ private_method：两个下划线开头，声明该方法为私有方法，<strong>不能在类的外部调用。</strong>在类的内部调用 self. __private_methods</p>\n<p>_foo: 以单下划线开头的表示的是 protected 类型的变量，<strong>即保护类型只能允许其本身与子类进行访问</strong>，不能用于 <strong>from module import *</strong></p>\n<h3 id=\"super-的使用方法\"><a href=\"#super-的使用方法\" class=\"headerlink\" title=\"super()的使用方法\"></a>super()的使用方法</h3><p><strong>super()</strong> 函数是用于调用父类(超类)的一个方法。</p>\n<p>注意，python2.x和3.x有所不同：</p>\n<ul>\n<li>Python 3 可以使用直接使用 <strong>super().xxx</strong> 代替 <strong>super(Class, self).xxx</strong> </li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FooParent</span>(<span class=\"params\"><span class=\"built_in\">object</span></span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># 这个为父类</span></span><br><span class=\"line\">    <span class=\"comment\"># 初始化</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        self.parent = <span class=\"string\">&#x27;I\\&#x27;m the parent.&#x27;</span></span><br><span class=\"line\">        <span class=\"built_in\">print</span> (<span class=\"string\">&#x27;Parent&#x27;</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 定义函数</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bar</span>(<span class=\"params\">self,message</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">print</span> (<span class=\"string\">&quot;%s from Parent&quot;</span> % message)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FooChild</span>(<span class=\"params\">FooParent</span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># 这个为子类</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"comment\"># super(FooChild,self) 首先找到 FooChild 的父类（就是类 FooParent），然后把类 FooChild 的对象转换为类 FooParent 的对象</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(FooChild,self).__init__()    </span><br><span class=\"line\">        <span class=\"built_in\">print</span> (<span class=\"string\">&#x27;Child&#x27;</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bar</span>(<span class=\"params\">self,message</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(FooChild, self).bar(message)</span><br><span class=\"line\">        <span class=\"built_in\">print</span> (<span class=\"string\">&#x27;Child bar fuction&#x27;</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span> (self.parent)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    fooChild = FooChild()</span><br><span class=\"line\">    fooChild.bar(<span class=\"string\">&#x27;HelloWorld&#x27;</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\"># Parent</span></span><br><span class=\"line\"><span class=\"comment\"># Child</span></span><br><span class=\"line\"><span class=\"comment\"># HelloWorld from Parent</span></span><br><span class=\"line\"><span class=\"comment\"># Child bar fuction</span></span><br><span class=\"line\"><span class=\"comment\"># I&#x27;m the parent.</span></span><br></pre></td></tr></table></figure>\n\n<p>三种情况：</p>\n<p>情况一：<strong>子类需要自动调用父类的方法：</strong>子类不重写__ init __ ()方法，实例化子类后，会自动调用父类的 __ init __()的方法。</p>\n<p>情况二：<strong>子类不需要自动调用父类的方法：</strong>子类重写 __ init __ ()方法，实例化子类后，将不会自动调用父类的__ init __()的方法。</p>\n<p>情况三：<strong>子类重写__init__()方法又需要调用父类的方法：</strong>使用super关键词：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">super</span>(子类，self).__init__(参数<span class=\"number\">1</span>，参数<span class=\"number\">2</span>，....)</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Son</span>(<span class=\"params\">Father</span>):</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, name</span>):</span>   </span><br><span class=\"line\">    <span class=\"built_in\">super</span>(Son, self).__init__(name)</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id><a href=\"#\" class=\"headerlink\" title></a></h2>","categories":["PythonCoding"],"tags":["Python"]},{"title":"PyTorch-EnvSetup&BasicConcept","url":"/2021/09/26/PyTorch-EnvSetup-BasicConcept/","content":"<h2 id=\"一-PyTorch基础概念\"><a href=\"#一-PyTorch基础概念\" class=\"headerlink\" title=\"一. PyTorch基础概念\"></a>一. PyTorch基础概念</h2><h3 id=\"A-环境搭建\"><a href=\"#A-环境搭建\" class=\"headerlink\" title=\"A. 环境搭建\"></a>A. 环境搭建</h3><h4 id=\"1-系统显卡驱动，CUDA和cudnn配置\"><a href=\"#1-系统显卡驱动，CUDA和cudnn配置\" class=\"headerlink\" title=\"1. 系统显卡驱动，CUDA和cudnn配置\"></a>1. 系统显卡驱动，CUDA和cudnn配置</h4><p>本机配置：Win10，Geforce 3070 140w 8GB显存，Intel 11th i7 八核十六线程</p>\n<p>当前Nvidia显卡驱动版本462.62，最高支持cuda版本11.2（安装cuda版本11.1）</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921151203907.png\" alt=\"image-20210921151203907\"></p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921151508628.png\" alt=\"image-20210921151508628\"></p>\n<p>本机目前安装CUDA版本11.1，可通过nvcc -V查看</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921152447031.png\" alt=\"image-20210921152447031\"></p>\n<p>对应cudnn版本为8.1.1</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921155417638.png\" alt=\"image-20210921155417638\"></p>\n<p>通过运行CUDA自带的deviceQuery.exe可知，</p>\n<p>当前cuda driver version为11.2，当前cuda runtime version为11.1。两者允许不一致，但是runtime version必须 &lt;= driver version</p>\n<h4 id=\"2-PyTorch安装\"><a href=\"#2-PyTorch安装\" class=\"headerlink\" title=\"2. PyTorch安装\"></a>2. PyTorch安装</h4><p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921155822943.png\" alt=\"image-20210921155822943\"></p>\n<p>CUDA版本应该与runtime version相对应。</p>\n<p>可以下载对应的torch，torchvision和torchaudio手动安装：</p>\n<p><a href=\"https://download.pytorch.org/whl/torch_stable.html\">https://download.pytorch.org/whl/torch_stable.html</a></p>\n<p>直接到conda 环境下运行辣个command/或者pip install 各个.whl文件。</p>\n<h3 id=\"B-张量简介与创建\"><a href=\"#B-张量简介与创建\" class=\"headerlink\" title=\"B. 张量简介与创建\"></a>B. 张量简介与创建</h3><h4 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h4><p>张量<strong>tensor</strong>就是<strong>多维数组</strong>，是标量、向量、矩阵的高维拓展。具备以下属性。</p>\n<ul>\n<li>卷积之类的处理之后，数据类型默认为32-bit floating point, torch.float/float32 ；</li>\n<li>图像的标签则默认为64-bit integer (signed) torch.int/torch.int64</li>\n</ul>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921163051863.png\" alt=\"image-20210921163051863\"></p>\n<h4 id=\"1-直接创建\"><a href=\"#1-直接创建\" class=\"headerlink\" title=\"1. 直接创建\"></a>1. 直接创建</h4><h5 id=\"torch-tensor\"><a href=\"#torch-tensor\" class=\"headerlink\" title=\"torch.tensor()\"></a>torch.tensor()</h5><p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921163600998.png\" alt=\"image-20210921163600998\"></p>\n<ul>\n<li>data：数据，可以是list，numpy</li>\n<li>dtype：数据类型，默认与data的一致</li>\n<li>device：所在设备，cuda/cpu</li>\n<li>requires_grad：是否需要梯度</li>\n<li>pin_memory：是否存于锁页内存（目前不懂作用，默认为false即可）</li>\n</ul>\n<h5 id=\"torch-from-numpy\"><a href=\"#torch-from-numpy\" class=\"headerlink\" title=\"torch.from_numpy()\"></a>torch.from_numpy()</h5><p>从ndarray创建</p>\n<p><em><strong><u><code>torch.from_numpy(ndarray)</code></u></strong></em></p>\n<p>从numpy创建tensor，<strong>注意：两者之间共享内存，改动其中一个另一个也会被更改。</strong></p>\n<h4 id=\"2-依据数值创建\"><a href=\"#2-依据数值创建\" class=\"headerlink\" title=\"2. 依据数值创建\"></a>2. 依据数值创建</h4><h5 id=\"torch-xxx-like-xxx\"><a href=\"#torch-xxx-like-xxx\" class=\"headerlink\" title=\"torch.xxx_like()/xxx()\"></a>torch.xxx_like()/xxx()</h5><p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921164613808.png\" alt=\"image-20210921164613808\"></p>\n<ul>\n<li>out: 输出的张量（给生成的数据打上新标签）</li>\n<li>layout: 内存中的布局形式，有strided, sparse_coo等（目前不懂作用，不过默认strided就好）</li>\n</ul>\n<p>根据input形状创建全零或全一张量</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921164936568.png\" alt=\"image-20210921164936568\"></p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921165012519.png\" alt=\"image-20210921165012519\"></p>\n<p><em><strong><u>torch.full( )</u><em><strong>和</strong></em><u>torch.full_like( )</u></strong></em></p>\n<p>依据input形状创建全（fill_value）值的向量</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921165151658.png\" alt=\"image-20210921165151658\"></p>\n<ul>\n<li>size：张量的形状，如（3，3）</li>\n<li>fill_value：张量的值</li>\n</ul>\n<h5 id=\"torch-arange\"><a href=\"#torch-arange\" class=\"headerlink\" title=\"torch.arange( )\"></a>torch.arange( )</h5><p>创建等差的1维张量，数值区间为[start, end)</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921165611232.png\" alt=\"image-20210921165611232\"></p>\n<ul>\n<li>start: 初始数值</li>\n<li>end: 结束数值</li>\n<li>step：数列公差</li>\n</ul>\n<h5 id=\"torch-linspace\"><a href=\"#torch-linspace\" class=\"headerlink\" title=\"torch.linspace( )\"></a>torch.linspace( )</h5><p>创建均分的1维张量，数值区间[start, end]</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921165758777.png\" alt=\"image-20210921165758777\"></p>\n<ul>\n<li>steps: 数列长度</li>\n</ul>\n<h5 id=\"torch-logspace\"><a href=\"#torch-logspace\" class=\"headerlink\" title=\"torch.logspace( )\"></a>torch.logspace( )</h5><p>创建对数均分的一维张量，长度为<em>steps</em>，底为<em>base</em>（默认为10）</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921170039498.png\" alt=\"image-20210921170039498\"></p>\n<h5 id=\"torch-eyes\"><a href=\"#torch-eyes\" class=\"headerlink\" title=\"torch.eyes( )\"></a>torch.eyes( )</h5><p>创建单位对角矩阵（2维），默认为方阵，n行数（方阵只需设置行数），m列数</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921170217035.png\" alt=\"image-20210921170217035\"></p>\n<h4 id=\"3-依概率分布创建\"><a href=\"#3-依概率分布创建\" class=\"headerlink\" title=\"3.依概率分布创建\"></a>3.依概率分布创建</h4><h5 id=\"torch-normal\"><a href=\"#torch-normal\" class=\"headerlink\" title=\"torch.normal( )\"></a>torch.normal( )</h5><p>生成正态分布（Gaussian）</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921170751238.png\" alt=\"image-20210921170751238\"></p>\n<ul>\n<li><p>mean：均值</p>\n</li>\n<li><p>std：标准差</p>\n</li>\n<li><p>四种模式：</p>\n<ul>\n<li><p>mean为<strong>标</strong>量，std为<strong>标</strong>量，例：torch.normal(0., 1., size=(4, ))；</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921171403037.png\" alt=\"image-20210921171403037\"></p>\n</li>\n<li><p>mean为<strong>标</strong>量，std为张量, 同下，</p>\n</li>\n<li><p>mean为张量，std为<strong>标</strong>量, 例如mean = torch.arrange(1, 5, dtype=torch.float), std = 1</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921171600585.png\" alt=\"image-20210921171600585\"></p>\n</li>\n<li><p>mean为张量，std为张量,例: mean = torch.arrange(1, 5, dypte=torch.float) , std = torch.arrange(1, 5, dypte=torch.float) <img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921171237469.png\" alt=\"image-20210921171237469\"></p>\n</li>\n</ul>\n</li>\n</ul>\n<h5 id=\"torch-randn-randn-like\"><a href=\"#torch-randn-randn-like\" class=\"headerlink\" title=\"torch.randn()/randn_like()\"></a>torch.randn()/randn_like()</h5><p>生成<strong>标准正态分布</strong></p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921171733876.png\" alt=\"image-20210921171733876\"></p>\n<ul>\n<li>size：张量的形状</li>\n</ul>\n<h5 id=\"torch-rand-rand-like\"><a href=\"#torch-rand-rand-like\" class=\"headerlink\" title=\"torch.rand()/rand_like()\"></a>torch.rand()/rand_like()</h5><p>在区间**[0,1)**上，生成**均匀分布**<img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921171938510.png\" alt=\"image-20210921171938510\"></p>\n<h5 id=\"torch-randint-randint-like\"><a href=\"#torch-randint-randint-like\" class=\"headerlink\" title=\"torch.randint()/randint_like()\"></a>torch.randint()/randint_like()</h5><p>在区间[low, high)生成<strong>整数均匀分布</strong></p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921172147889.png\" alt=\"image-20210921172147889\"></p>\n<h5 id=\"torch-randperm\"><a href=\"#torch-randperm\" class=\"headerlink\" title=\"torch.randperm()\"></a>torch.randperm()</h5><p>生成从0到n-1的随机排列</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921172235710.png\" alt=\"image-20210921172235710\"></p>\n<h5 id=\"torch-bernoulli\"><a href=\"#torch-bernoulli\" class=\"headerlink\" title=\"torch.bernoulli()\"></a>torch.bernoulli()</h5><p>以input为概率，生成伯努利分布（0-1分布，两点分布），input为概率值</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921172334929.png\" alt=\"image-20210921172334929\"></p>\n<h3 id=\"C-张量的操作\"><a href=\"#C-张量的操作\" class=\"headerlink\" title=\"C. 张量的操作\"></a>C. 张量的操作</h3><h4 id=\"1-张量拼接与切分\"><a href=\"#1-张量拼接与切分\" class=\"headerlink\" title=\"1.张量拼接与切分\"></a>1.张量拼接与切分</h4><h5 id=\"torch-cat\"><a href=\"#torch-cat\" class=\"headerlink\" title=\"torch.cat( )\"></a>torch.cat( )</h5><p>将张量按维度dim进行拼接，**<u>cat()操作并不会拓展张量的维度</u>**</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921193450329.png\" alt=\"image-20210921193450329\"></p>\n<ul>\n<li>tensor：张量序列</li>\n<li>dim：要拼接的维度</li>\n</ul>\n<h5 id=\"torch-stack\"><a href=\"#torch-stack\" class=\"headerlink\" title=\"torch.stack()\"></a>torch.stack()</h5><p>在**<u>新创建</u>**的维度dim上进行拼接</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921193619709.png\" alt=\"image-20210921193619709\"></p>\n<ul>\n<li>tensor：张量序列</li>\n<li>dim：要拼接的维度</li>\n</ul>\n<h5 id=\"torch-chunk\"><a href=\"#torch-chunk\" class=\"headerlink\" title=\"torch.chunk()\"></a>torch.chunk()</h5><p>将张量按维度dim进行平均切分，返回张量列表。若不能整除，最后一份张量最小。</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921194449301.png\" alt=\"image-20210921194449301\"></p>\n<ul>\n<li>input：要切分的张量</li>\n<li>chunks：要切分的份数</li>\n<li>dim：要切分的维度</li>\n</ul>\n<p>例：</p>\n<p><code>a = torch.ones((2, 7))</code> </p>\n<p><code>list_of_tensors = torch.chunk(a, dim=1, chunk = 3)</code></p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921195940743.png\" alt=\"image-20210921195940743\"></p>\n<h5 id=\"torch-split\"><a href=\"#torch-split\" class=\"headerlink\" title=\"torch.split()\"></a>torch.split()</h5><p>将张量按维度dim进行切分，返回值是张量列表</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921200047221.png\" alt=\"image-20210921200047221\"></p>\n<ul>\n<li>tensor：要切分的张量</li>\n<li>split_size_or_sections：为int时，表示每一份的长度；为list时，按list元素切分</li>\n<li>dim：要切分的维度</li>\n</ul>\n<h4 id=\"2-张量索引\"><a href=\"#2-张量索引\" class=\"headerlink\" title=\"2. 张量索引\"></a>2. 张量索引</h4><h5 id=\"torch-index-select\"><a href=\"#torch-index-select\" class=\"headerlink\" title=\"torch.index_select()\"></a>torch.index_select()</h5><p>在维度dim上，按index索引数据，返回值为依index索引数据拼接而成的张量。注意，此函数中，index类型必须为torch.long。</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921200540590.png\" alt=\"image-20210921200540590\"></p>\n<ul>\n<li>input：要索引的张量</li>\n<li>dim：要索引的维度</li>\n<li>index：要索引数据的序号</li>\n</ul>\n<p>例：</p>\n<p><code>t = torch.randint(0, 9, size=(3,3))</code></p>\n<p><code>idx = torch.tensor([0, 2], dtype=torch.long)</code></p>\n<p><code>t_select = torch.index_select(t, dim=0, index=idx)</code></p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921200914186.png\" alt=\"image-20210921200914186\"></p>\n<h5 id=\"torch-masked-select\"><a href=\"#torch-masked-select\" class=\"headerlink\" title=\"torch.masked_select()\"></a>torch.masked_select()</h5><p>按mask中的True进行索引，返回值为一维张量。</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921201241414.png\" alt=\"image-20210921201241414\"></p>\n<ul>\n<li>input：要索引的张量</li>\n<li>mask：与input同形状的布尔类型张量</li>\n</ul>\n<p><code>热知识：t = torch.randint(0, 9, size=(3,3))</code></p>\n<p>​                <code>mask = t.ge(5) # ge means greater or equal</code>    </p>\n<p>​        <code>#        mask = t.gt(5) # ge means greater</code>    </p>\n<p>​        <code>#        mask = t.le(5) # ge means less or equal</code>    </p>\n<p>​        <code>#        mask = t.lt(5) # ge means less</code></p>\n<p>​        <code>t_select = torch.masked_select(t, mask)</code></p>\n<p>Output:</p>\n<p>​        <img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921201821510.png\" alt=\"image-20210921201821510\"></p>\n<h4 id=\"3-张量的变换\"><a href=\"#3-张量的变换\" class=\"headerlink\" title=\"3. 张量的变换\"></a>3. 张量的变换</h4><h5 id=\"torch-reshape\"><a href=\"#torch-reshape\" class=\"headerlink\" title=\"torch.reshape()\"></a>torch.reshape()</h5><p>变换张量的形状，<strong>注意</strong>：当张量在内存中是连续时，新张量与input<strong>共享数据内存</strong>。</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921202011330.png\" alt=\"image-20210921202011330\"></p>\n<p>新张量形状大小应该与input相匹配。若为某一维度长度由其他维度计算得知，<strong>可设置为-1</strong>。</p>\n<h5 id=\"torch-transpose-torch-t\"><a href=\"#torch-transpose-torch-t\" class=\"headerlink\" title=\"torch.transpose()/torch.t()\"></a>torch.transpose()/torch.t()</h5><p>交换张量的两个维度。**(torch.t()适用于二维张量，等于torch.transpose(input, 0, 1))**</p>\n<p><strong>图像处理时</strong>经常会用到，将channels * h * w变成h * w * channels。</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921202318278.png\" alt=\"image-20210921202318278\"></p>\n<ul>\n<li>input：要变换的张量</li>\n<li>dim0：要交换的维度</li>\n<li>dim1：要交换的维度</li>\n</ul>\n<h5 id=\"torch-squeeze-unsqueeze\"><a href=\"#torch-squeeze-unsqueeze\" class=\"headerlink\" title=\"torch.squeeze()/unsqueeze()\"></a>torch.squeeze()/unsqueeze()</h5><p><strong>squeeze()</strong>: 压缩长度为1的维度（轴）</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921202819633.png\" alt=\"image-20210921202819633\"></p>\n<p>dim：若为None，移除所有长度为1的轴；若指定维度，当且仅当该轴长度为1时，可以被移除。</p>\n<p>**unsqueeze()**：依据dim扩展维度</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921202948318.png\" alt=\"image-20210921202948318\"></p>\n<p>dim: 扩展的维度</p>\n<h3 id=\"D-张量数学运算\"><a href=\"#D-张量数学运算\" class=\"headerlink\" title=\"D. 张量数学运算\"></a>D. 张量数学运算</h3><p>分为三类：</p>\n<ol>\n<li>加减乘除</li>\n<li>对数，指数，幂函数</li>\n<li>三角函数</li>\n</ol>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921203343226.png\" alt=\"image-20210921203343226\"></p>\n<h4 id=\"两个pythonic的方法\"><a href=\"#两个pythonic的方法\" class=\"headerlink\" title=\"两个pythonic的方法\"></a>两个pythonic的方法</h4><p>两个特别pythonic的方法：</p>\n<p>torch.addcdiv()</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921203603682.png\" alt=\"image-20210921203603682\"></p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921203550310.png\" alt=\"image-20210921203550310\"></p>\n<p>torch.addcmul()</p>\n<p>理解方式同上</p>\n<h3 id=\"E-计算图与动态图机制\"><a href=\"#E-计算图与动态图机制\" class=\"headerlink\" title=\"E. 计算图与动态图机制\"></a>E. 计算图与动态图机制</h3><h4 id=\"1-线性回归（Linear-Regression）\"><a href=\"#1-线性回归（Linear-Regression）\" class=\"headerlink\" title=\"1. 线性回归（Linear Regression）\"></a>1. 线性回归（Linear Regression）</h4><p><strong>线性回归</strong>是分析一个<strong>变量</strong>与另一个（多个）<strong>变量</strong>之间<strong>关系</strong>的方法。</p>\n<p>因变量：y</p>\n<p>自变量：x</p>\n<p>关系：y = w*x + b</p>\n<p>分析：求解w，b</p>\n<h5 id=\"模型\"><a href=\"#模型\" class=\"headerlink\" title=\"模型\"></a>模型</h5><p>Model：y = w*x + b</p>\n<h5 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h5><p>MSE： <img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921204404418.png\" alt=\"image-20210921204404418\"></p>\n<h5 id=\"求解梯度并更新参数\"><a href=\"#求解梯度并更新参数\" class=\"headerlink\" title=\"求解梯度并更新参数\"></a>求解梯度并更新参数</h5><p>w = w - LR * w.grad</p>\n<p>b = w - LR * b.grad</p>\n<h4 id=\"2-计算图（Computational-Graph）\"><a href=\"#2-计算图（Computational-Graph）\" class=\"headerlink\" title=\"2. 计算图（Computational Graph）\"></a>2. 计算图（Computational Graph）</h4><h5 id=\"简介-1\"><a href=\"#简介-1\" class=\"headerlink\" title=\"简介\"></a>简介</h5><p>计算图是用来<strong>描述运算</strong>的<strong>有向无环图</strong>。</p>\n<p>计算图的两个主要元素：<strong>结点</strong>（Node）和<strong>边</strong>（Edge）</p>\n<p>结点表示<strong>数据</strong>，如向量，矩阵，张量</p>\n<p>边表示<strong>运算</strong>，如加减乘除卷积等<br>$$<br>y = (x + w) * (w + 1)<br>$$<br>a = x + w<br>b = w + 1<br>y = a * b</p>\n<img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921212833057.png\" alt=\"image-20210921212833057\" style=\"zoom: 25%;\">\n\n<h5 id=\"计算图与求导机制\"><a href=\"#计算图与求导机制\" class=\"headerlink\" title=\"计算图与求导机制\"></a>计算图与求导机制</h5><p>当我们对w进行求导</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921213436761.png\" alt=\"image-20210921213436761\"></p>\n<img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210921213452525.png\" alt=\"image-20210921213452525\" style=\"zoom:25%;\">\n\n<h5 id=\"is-leaf-叶子结点\"><a href=\"#is-leaf-叶子结点\" class=\"headerlink\" title=\"is_leaf(叶子结点)\"></a>is_leaf(叶子结点)</h5><p>torch.Tensor的is_leaf属性。用户创建的结点为叶子结点。</p>\n<p>在反向传播（BP）中，所有的计算都要依赖于叶子结点，反向传播之后，所有的非叶子结点的梯度都会被释放掉。</p>\n<p>is_leaf：张量是否为叶子结点（结点代表数据，边代表运算）。</p>\n<p><strong>如果想使用非叶子结点的梯度，则需要用torch.Tensor.retain_grad()</strong></p>\n<h5 id=\"grad-fn-方法\"><a href=\"#grad-fn-方法\" class=\"headerlink\" title=\"grad_fn(方法)\"></a>grad_fn(方法)</h5><p>记录创建该张量时所用的方法（函数）。</p>\n<p><strong>叶子结点的gradient_function 为None。</strong></p>\n<p>例：</p>\n<p>y.grad_fn = &lt; MulBackward0 &gt;</p>\n<p>a.grad_fn = &lt; AddBackward0 &gt;</p>\n<p>b.grad_fn = &lt; AddBackward0 &gt;</p>\n<h4 id=\"3-动态图vs静态图\"><a href=\"#3-动态图vs静态图\" class=\"headerlink\" title=\"3. 动态图vs静态图\"></a>3. 动态图vs静态图</h4><p>(PyTorch) 动态图：运算与搭建<strong>同时</strong>进行（自驾游出行）。</p>\n<p>(Tensorflow1.0) 静态图：<strong>先</strong>搭建图，<strong>后</strong>运算（跟旅游团出行）。</p>\n<h3 id=\"F-autograd与逻辑回归\"><a href=\"#F-autograd与逻辑回归\" class=\"headerlink\" title=\"F. autograd与逻辑回归\"></a>F. autograd与逻辑回归</h3><p>深度学习模型训练就是不断的更新权值。</p>\n<p>我们不需要手动计算梯度，只需要搭好前向计算图。</p>\n<h4 id=\"1-torch-autograd\"><a href=\"#1-torch-autograd\" class=\"headerlink\" title=\"1. torch.autograd\"></a>1. torch.autograd</h4><h5 id=\"torch-autograd-backward\"><a href=\"#torch-autograd-backward\" class=\"headerlink\" title=\"torch.autograd.backward\"></a>torch.autograd.backward</h5><p>用于自动求取梯度 </p>\n<p>一般常见的loss.backward()就是调用的这个方法。</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210922005408519.png\" alt=\"image-20210922005408519\"></p>\n<ul>\n<li>tensors: 用于求导的张量，如loss</li>\n<li>retain_graph：保存计算图</li>\n<li>create_graph：创建导数计算图，用于高阶求导</li>\n<li>grad_tensors：多梯度权重（用于多个梯度之间权重的设置）</li>\n</ul>\n<p><strong>注意</strong>：在retain_graph为false的情况，无法连续执行两次反向传播，会提示”the buffers have already been freed“。</p>\n<h5 id=\"torch-autograd-grad\"><a href=\"#torch-autograd-grad\" class=\"headerlink\" title=\"torch.autograd.grad\"></a>torch.autograd.grad</h5><p>求取梯度，返回值是我们想要的那一个张量的梯度。</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210922010704023.png\" alt=\"image-20210922010704023\"></p>\n<ul>\n<li>outputs：用于求导的张量，如loss</li>\n<li>inputs：需要梯度的张量</li>\n<li>create_graph：创建导数计算图（用于高阶求导时要为True，例如对导数再次求导）</li>\n<li>retain_graph：保存计算图</li>\n<li>grad_outputs：多梯度权重</li>\n</ul>\n<h5 id=\"注意\"><a href=\"#注意\" class=\"headerlink\" title=\"注意\"></a><strong><u><em>注意</em></u></strong></h5><ol>\n<li><p><strong>梯度不自动清零</strong></p>\n<p>所以一般在执行backward( )操作之后要进行grad.zero_( )</p>\n<p>_下划线表示原地，原位操作（in-place）</p>\n</li>\n<li><p><strong>依赖于叶子结点的结点，required_grad默认为True</strong></p>\n</li>\n<li><p><strong>叶子结点不可执行in-place操作</strong></p>\n<p>反向传播时根据地址寻找数据，并且用到了叶子结点该地址当中的数据。如果在反向传播前改变了该地址的数据，就会引起反向传播出错。</p>\n</li>\n</ol>\n<h4 id=\"2-逻辑回归（Logisti-Regress）\"><a href=\"#2-逻辑回归（Logisti-Regress）\" class=\"headerlink\" title=\"2. 逻辑回归（Logisti Regress）\"></a>2. 逻辑回归（Logisti Regress）</h4><p>逻辑回归是线性的二分类模型</p>\n<p>模型表达式:</p>\n<img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210922012147317.png\" alt=\"image-20210922012147317\" style=\"zoom:25%;\">\n\n<img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210922012334999.png\" alt=\"image-20210922012334999\" style=\"zoom: 50%;\">\n\n<p>线性回归是分析自变量x与因变量y（标量）之间关系的方法</p>\n<p>逻辑回归是分析自变量x与因变量y（概率）之间关系的方法</p>\n<p><img src=\"/2021/09/26/PyTorch-EnvSetup-BasicConcept/image-20210922012729014.png\" alt=\"image-20210922012729014\"></p>\n","categories":["PyTorch框架学习"],"tags":["PyTorch"]},{"title":"PyTorch-TrainingProcess","url":"/2021/09/28/PyTorch-TrainingProcess/","content":"<h2 id=\"五-PyTorch训练过程\"><a href=\"#五-PyTorch训练过程\" class=\"headerlink\" title=\"五. PyTorch训练过程\"></a>五. PyTorch训练过程</h2><h4 id=\"A-学习率调整策略\"><a href=\"#A-学习率调整策略\" class=\"headerlink\" title=\"A. 学习率调整策略\"></a>A. 学习率调整策略</h4><h5 id=\"1-为什么要调整学习率\"><a href=\"#1-为什么要调整学习率\" class=\"headerlink\" title=\"1. 为什么要调整学习率\"></a>1. 为什么要调整学习率</h5><p>学习率（learning rate）控制了参数更新的步伐，一般前期会比较大，后期会下降。</p>\n<p><strong>就像打高尔夫。</strong></p>\n<p>PyTorch提供了很好的学习率调整方法。</p>\n<h5 id=\"2-调整方法\"><a href=\"#2-调整方法\" class=\"headerlink\" title=\"2. 调整方法\"></a>2. 调整方法</h5><h6 id=\"class-LRScheduler\"><a href=\"#class-LRScheduler\" class=\"headerlink\" title=\"class _LRScheduler\"></a>class _LRScheduler</h6><p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210926212016387.png\" alt=\"image-20210926212016387\"></p>\n<p>主要属性：</p>\n<ul>\n<li>optimizer：关联的优化器（学习率放在优化器当中，所以必须关联）</li>\n<li>last_epoch：记录epoch数，学习率的调整以epoch为周期</li>\n<li>base_lrs：记录初始学习率</li>\n</ul>\n<p><strong>主要方法：</strong></p>\n<ul>\n<li>step()：更新下一个epoch的学习率</li>\n<li>get_lr()：虚函数，计算下一个epoch的学习率。用来给子类override。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 例如：</span></span><br><span class=\"line\">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=<span class=\"number\">10</span>, gamma=<span class=\"number\">0.1</span> )</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">StepLR</span>(<span class=\"params\">_LRScheduler</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, optimizer, step_size, gamma=<span class=\"number\">0.1</span>, last_epoch=-<span class=\"number\">1</span></span>):</span></span><br><span class=\"line\">        self.step_size = gamma</span><br><span class=\"line\">        self.gamma = gamma</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(StepLR, self).__init__(optimizer, last_epoch)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_lr</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> [base_lr * self.gamma ** (self.last_epoch//self.step_size) <span class=\"keyword\">for</span> base_lr <span class=\"keyword\">in</span> self.base_lrs]</span><br></pre></td></tr></table></figure>\n\n<p>整个scheduler只出现在两行，第一次是构建，第二次是执行step()。执行step()时注意一定要<strong>放在epoch当中</strong>，而不是iteration。</p>\n<h6 id=\"StepLR\"><a href=\"#StepLR\" class=\"headerlink\" title=\"StepLR\"></a>StepLR</h6><p>等间隔调整学习率</p>\n<p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210926212157451.png\" alt=\"image-20210926212157451\"></p>\n<p>主要参数：</p>\n<ul>\n<li>step_size：等间隔数</li>\n<li>gamma：调整系数</li>\n</ul>\n<p><strong>调整方式：lr = lr * gamma</strong></p>\n<h6 id=\"MultiStepLR\"><a href=\"#MultiStepLR\" class=\"headerlink\" title=\"MultiStepLR\"></a>MultiStepLR</h6><p>按给定时间调整学习率</p>\n<p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210926220926788.png\" alt=\"image-20210926220926788\"></p>\n<p>主要参数：</p>\n<ul>\n<li>milestones：设定调整时刻数（构建一个list放入，例如[10, 120, 200], 根据list数值调整）</li>\n<li>gamma：调整系数</li>\n</ul>\n<h6 id=\"ExponentialLR\"><a href=\"#ExponentialLR\" class=\"headerlink\" title=\"ExponentialLR\"></a>ExponentialLR</h6><p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210926221144356.png\" alt=\"image-20210926221144356\"></p>\n<p>按指数衰减调整学习率</p>\n<p>主要参数：</p>\n<ul>\n<li>gamma：指数的底（通常设置为接近1的一个数）</li>\n</ul>\n<p>调整方式：lr = lr * gamma ** epoch</p>\n<h6 id=\"CosineAnnealingLR\"><a href=\"#CosineAnnealingLR\" class=\"headerlink\" title=\"CosineAnnealingLR\"></a>CosineAnnealingLR</h6><p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210926221836558.png\" alt=\"image-20210926221836558\"></p>\n<p>余弦周期调整学习率</p>\n<p>主要参数：</p>\n<ul>\n<li>T_max：下降周期</li>\n<li>eta_min：学习率的下限</li>\n<li><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210926222018170.png\" alt=\"image-20210926222018170\" style=\"zoom: 25%;\"></li>\n</ul>\n<p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210926221851244.png\" alt=\"image-20210926221851244\"></p>\n<h6 id=\"ReduceLRonPlateau\"><a href=\"#ReduceLRonPlateau\" class=\"headerlink\" title=\"ReduceLRonPlateau\"></a>ReduceLRonPlateau</h6><p>监控指标，当指标不再变化则调整</p>\n<p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210926222248004.png\" alt=\"image-20210926222248004\"></p>\n<p>主要参数：</p>\n<ul>\n<li>mode：min/max 两种模式</li>\n<li>factor：调整系数</li>\n<li>patience：”耐心“，接受几次不变化</li>\n<li>cooldown：”冷却时间“，停止监控一段时间</li>\n<li>verbose：是否打印日志</li>\n<li>min_lr：学习率的下限</li>\n<li>eps：学习率衰减最小值</li>\n</ul>\n<h6 id=\"LambaLR\"><a href=\"#LambaLR\" class=\"headerlink\" title=\"LambaLR\"></a>LambaLR</h6><p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210926224426382.png\" alt=\"image-20210926224426382\"></p>\n<p>自定义调整策略</p>\n<p>主要参数：</p>\n<ul>\n<li>lr_lambda：function or list</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">optimizer = optim.SGD([&#123;<span class=\"string\">&#x27;params&#x27;</span> : [weights_1]&#125;, &#123;<span class=\"string\">&#x27;params&#x27;</span>: [weights_2]&#125;], lr = lr_init)</span><br><span class=\"line\"></span><br><span class=\"line\">lambda1 = <span class=\"keyword\">lambda</span> epoch: <span class=\"number\">0.1</span> ** (epoch // <span class=\"number\">20</span>)</span><br><span class=\"line\">lambda2 = <span class=\"keyword\">lambda</span> epoch: <span class=\"number\">0.95</span> ** epoch</span><br><span class=\"line\"></span><br><span class=\"line\">scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])</span><br></pre></td></tr></table></figure>\n\n<p>不同的参数组，有不同的调整策略。在finetune当中很常见。</p>\n<h5 id=\"3-学习率调整小结\"><a href=\"#3-学习率调整小结\" class=\"headerlink\" title=\"3. 学习率调整小结\"></a>3. 学习率调整小结</h5><ol>\n<li>有序调整：Step，MultiStep，Exponential 和 CosineAnnealing</li>\n<li>自适应调整：ReduceLROnPleateau</li>\n<li>自定义调整：Lambda</li>\n</ol>\n<p>学习率初始化：</p>\n<ol>\n<li>设置较小数：0.01、0.001、0.0001</li>\n<li>搜索最大学习率：《Cyclical Learning Rates for Training Neural Networks》<ol>\n<li><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210926225813725.png\" alt=\"image-20210926225813725\" style=\"zoom:33%;\"></li>\n<li><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210926225855652.png\" alt=\"image-20210926225855652\" style=\"zoom:33%;\"></li>\n</ol>\n</li>\n</ol>\n<h4 id=\"B-可视化工具-——-Tensorboard\"><a href=\"#B-可视化工具-——-Tensorboard\" class=\"headerlink\" title=\"B. 可视化工具 —— Tensorboard\"></a>B. 可视化工具 —— Tensorboard</h4><h5 id=\"1-TensorBoard简介\"><a href=\"#1-TensorBoard简介\" class=\"headerlink\" title=\"1. TensorBoard简介\"></a>1. TensorBoard简介</h5><p>TensorBoard：Tensorflow中强大的可视化工具</p>\n<p>支持标量、图像、文本、音频、视频和Embedding等多种数据的可视化。</p>\n<p>用来监控当前训练是不是一个良好的训练状态。</p>\n<p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210926230630563.png\" alt=\"image-20210926230630563\"></p>\n<h5 id=\"2-TensorBoard安装\"><a href=\"#2-TensorBoard安装\" class=\"headerlink\" title=\"2. TensorBoard安装\"></a>2. TensorBoard安装</h5><p>“报错安装法”</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">pip install future</span><br><span class=\"line\">pip install tensorboard</span><br></pre></td></tr></table></figure>\n\n\n\n<h5 id=\"3-TensorBoard运行可视化\"><a href=\"#3-TensorBoard运行可视化\" class=\"headerlink\" title=\"3. TensorBoard运行可视化\"></a>3. TensorBoard运行可视化</h5><p>运行机制：</p>\n<p>python脚本：记录可视化的数据 ——&gt; 硬盘：event file ——&gt; 终端：tensorboard ——&gt; Web端：<img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210926230827260.png\" alt=\"image-20210926230827260\" style=\"zoom:25%;\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.tensorboard <span class=\"keyword\">import</span> summaryWriter</span><br><span class=\"line\"></span><br><span class=\"line\">writer = SummaryWriter(comment=<span class=\"string\">&#x27;test tensorboard&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">100</span>):</span><br><span class=\"line\">    writer.add_scalar(<span class=\"string\">&#x27;y=2x&#x27;</span>, x*<span class=\"number\">2</span>,x)</span><br><span class=\"line\">    writer.add_scalar(<span class=\"string\">&#x27;y=pow(2,x)&#x27;</span>, <span class=\"number\">2</span>**x, x)</span><br><span class=\"line\">    </span><br><span class=\"line\">    writer.add_scalar(<span class=\"string\">&#x27;data/scalar_group&#x27;</span>, &#123;<span class=\"string\">&quot;xsinx&quot;</span>: x*np.sin(x), <span class=\"string\">&quot;xcosx&quot;</span>: x * np.cos(x), <span class=\"string\">&quot;arctanx&quot;</span>: np.arctan(x)&#125;, x)</span><br><span class=\"line\">    </span><br><span class=\"line\">writer.close()</span><br></pre></td></tr></table></figure>\n\n<p>runs/文件夹下会有event file拿来可视化。cd到runs所在文件夹下，输入：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">tensorboard --logdir=./runs</span><br></pre></td></tr></table></figure>\n\n<p>之后点击终端里的网址，便在在Web中打开可视化界面。</p>\n<h4 id=\"C-TensorBoard使用（一）\"><a href=\"#C-TensorBoard使用（一）\" class=\"headerlink\" title=\"C. TensorBoard使用（一）\"></a>C. TensorBoard使用（一）</h4><p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210926233724340.png\" alt=\"image-20210926233724340\"></p>\n<h5 id=\"1-SummaryWriter\"><a href=\"#1-SummaryWriter\" class=\"headerlink\" title=\"1. SummaryWriter\"></a>1. SummaryWriter</h5><h6 id=\"SummaryWriter\"><a href=\"#SummaryWriter\" class=\"headerlink\" title=\"SummaryWriter\"></a>SummaryWriter</h6><p>提供创建event file的高级接口：</p>\n<p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210926234221840.png\" alt=\"image-20210926234221840\"></p>\n<p>主要属性：</p>\n<ul>\n<li>log_dir：event file输出文件夹</li>\n<li>comment：不指定log_dir时，文件夹后缀</li>\n<li>filename_suffix：event file文件名后缀</li>\n</ul>\n<p>（不设置log_dir的话，就会是/runs/comment/event_file）</p>\n<p>设置log_dir之后，comment就不会起作用。</p>\n<h5 id=\"2-SummaryWriter-add-scalar-amp-add-histogram\"><a href=\"#2-SummaryWriter-add-scalar-amp-add-histogram\" class=\"headerlink\" title=\"2.SummaryWriter.add_scalar &amp; add_histogram\"></a>2.SummaryWriter.add_scalar &amp; add_histogram</h5><h6 id=\"add-scalar\"><a href=\"#add-scalar\" class=\"headerlink\" title=\"add_scalar()\"></a>add_scalar()</h6><p>功能：记录标量</p>\n<p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210926235143739.png\" alt=\"image-20210926235143739\"></p>\n<ul>\n<li>tag：图像的标签名，图的唯一标识</li>\n<li>scalar_value：要记录的标量</li>\n<li>global_step：x轴</li>\n</ul>\n<h6 id=\"add-scalars\"><a href=\"#add-scalars\" class=\"headerlink\" title=\"add_scalars()\"></a>add_scalars()</h6><p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210926235304794.png\" alt=\"image-20210926235304794\"></p>\n<ul>\n<li>main_tag：该图的标签（意义同上的tag）</li>\n<li>tag_scalar_dict：key是变量的sub-tag，value是变量的值</li>\n</ul>\n<h6 id=\"add-histogram\"><a href=\"#add-histogram\" class=\"headerlink\" title=\"add_histogram()\"></a>add_histogram()</h6><p>功能：统计直方图与多分位数折线图</p>\n<p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210927000249812.png\" alt=\"image-20210927000249812\"></p>\n<ul>\n<li>tag：图像的标签名，图的唯一标识</li>\n<li>values：要统计的参数</li>\n<li>global_step：y轴</li>\n<li>bins：取直方图的bins</li>\n</ul>\n<h5 id=\"3-模型指标监控\"><a href=\"#3-模型指标监控\" class=\"headerlink\" title=\"3. 模型指标监控\"></a>3. 模型指标监控</h5><ol>\n<li><p>在迭代训练之前，先构建一个SummaryWriter。</p>\n</li>\n<li><p>先设置iteration记录参数，iter_count += 1，然后</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">writer.add_scalars(<span class=\"string\">&quot;Loss&quot;</span>, &#123;<span class=\"string\">&quot;Train&quot;</span>：loss.item()&#125;, iter_count)</span><br><span class=\"line\"></span><br><span class=\"line\">writer.add_scalars(<span class=\"string\">&quot;Accuracy&quot;</span>,&#123;<span class=\"string\">&quot;Train&quot;</span>: correct / total&#125;, iter_count)</span><br></pre></td></tr></table></figure></li>\n<li><p>```python<br>writer.add_scalars(“Loss”, {“Train”:np.mean(valid_curve)}, iter_count)</p>\n<p>writer.add_scalars(“Accuracy”, {“Train”: correct / total}, iter_count)</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">   </span><br><span class=\"line\"></span><br><span class=\"line\">4. 每个epoch，记录梯度，权值：</span><br><span class=\"line\"></span><br><span class=\"line\">   ```python</span><br><span class=\"line\">   for name, param in net.named_parameter():</span><br><span class=\"line\">       writer.add_histogram(name + &#x27;_grad&#x27;, param.grad, epoch)</span><br><span class=\"line\">       writer.add_histogram(name + &#x27;_data&#x27;, param, epoch)</span><br><span class=\"line\">   </span><br></pre></td></tr></table></figure>\n\n<p><strong>可以在训练过程中可视化，训练过程中即时生成可视化。</strong></p>\n</li>\n</ol>\n<p>在观察每一层的数值和梯度分布时：</p>\n<ol>\n<li>当靠后的epoch梯度很小时，并不一定是梯度消失。loss的稳定也会导致梯度变小。</li>\n<li>如果模型的参数发散（非正态），并且模型表现不佳，则训练很明显出了问题。</li>\n<li>如果说最后一层梯度大，前面梯度小，则说明反向传播时，梯度的尺度在不断减小，出现了梯度消失，无法合理更新模型。</li>\n</ol>\n<h4 id=\"D-Tensorboard的使用（二）\"><a href=\"#D-Tensorboard的使用（二）\" class=\"headerlink\" title=\"D. Tensorboard的使用（二）\"></a>D. Tensorboard的使用（二）</h4><h5 id=\"1-add-image-and-torchvision-utils-make-grid\"><a href=\"#1-add-image-and-torchvision-utils-make-grid\" class=\"headerlink\" title=\"1. add_image and torchvision.utils.make_grid\"></a>1. add_image and torchvision.utils.make_grid</h5><h6 id=\"add-image\"><a href=\"#add-image\" class=\"headerlink\" title=\"add_image()\"></a>add_image()</h6><p>记录图像</p>\n<p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210927110457174.png\" alt=\"image-20210927110457174\"></p>\n<ul>\n<li>tag：图像的标签名，图的唯一标识</li>\n<li>img_tensor：图像数据，注意尺度（最终需要缩放到0-255之间，如果都是0-1区间，会默认放大到0-255区间）</li>\n<li>global_step：x轴</li>\n<li>dataformats：数据形式，CHW，HWC，HW（二维灰度图，不带颜色）</li>\n</ul>\n<h6 id=\"torchvision-utils-make-grid\"><a href=\"#torchvision-utils-make-grid\" class=\"headerlink\" title=\"torchvision.utils.make_grid\"></a>torchvision.utils.make_grid</h6><p>制作网格图像</p>\n<p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210927145242252.png\" alt=\"image-20210927145242252\"></p>\n<ul>\n<li>tensor：图像数据，B（几张图） * C * H * W格式</li>\n<li>nrow：行数（列数自动计算）</li>\n<li>padding：图像间距（像素单位）</li>\n<li>range：标准化范围</li>\n<li>scale_each：是否单张图维度标准化</li>\n<li>pad_value：padding的像素值</li>\n</ul>\n<p>如何可视化卷积特征图，部分代码示例如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> instance(sub_module, nn.Conv2d):</span><br><span class=\"line\">    kernel_num += <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> kernel_num &gt; vis_max:</span><br><span class=\"line\">        <span class=\"keyword\">break</span></span><br><span class=\"line\">    kernel = sub_module.weight</span><br><span class=\"line\">    c_out, c_int, k_w, k_h = <span class=\"built_in\">tuple</span>(kernels.shape)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">for</span> o_icx <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(c_out):</span><br><span class=\"line\">        kernel_idx = kernels[o_idx, :, :, :].unsqueeze(<span class=\"number\">1</span>) <span class=\"comment\"># make_grid需要BCHW，需要拓展C维数据</span></span><br><span class=\"line\">        kernel_grid = vutils.make_grid(kernel_idx, normalize=<span class=\"literal\">True</span>, scale_each=<span class=\"literal\">True</span>, nrow=c_int)</span><br><span class=\"line\">        writer.add_image(<span class=\"string\">&#x27;&#123;&#125; Convlayer split_in_channel&#x27;</span>.<span class=\"built_in\">format</span>(kernel_num), kernel_grid, global_step=o_idx)</span><br><span class=\"line\">        kernel_all = kernels.view(-<span class=\"number\">1</span>, <span class=\"number\">3</span>, k_h, k_w)</span><br><span class=\"line\">        kernel_grid = vutils.make_grid(kernel_all, normalize=<span class=\"literal\">True</span>, scale_each=<span class=\"literal\">True</span>, nrow=<span class=\"number\">8</span>)</span><br><span class=\"line\">        writer.add_image(<span class=\"string\">&#x27;&#123;&#125;_all&#x27;</span>.<span class=\"built_in\">format</span>(kernel_num), kernel_grid, global_step=<span class=\"number\">322</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\"><span class=\"comment\"># 最后带个writer.close()</span></span><br></pre></td></tr></table></figure>\n\n\n\n<h5 id=\"2-AlexNet卷积核与特征图可视化\"><a href=\"#2-AlexNet卷积核与特征图可视化\" class=\"headerlink\" title=\"2. AlexNet卷积核与特征图可视化\"></a>2. AlexNet卷积核与特征图可视化</h5><p>先对数据进行预处理，例如resize，正则化，ToTensor等操作。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 模型</span></span><br><span class=\"line\">alexnet = models.alexnet(pretrained=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># forward</span></span><br><span class=\"line\">convlayer1 = alexnet.features[<span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"comment\"># 如果不在此手动获取，特征图便会被释放掉。高阶用法可以使用hook函数勾取。</span></span><br><span class=\"line\">fmap_1 = convlayer1(img_tensor)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#预处理</span></span><br><span class=\"line\">fmap_1.transpose_(<span class=\"number\">0</span>, <span class=\"number\">1</span>) <span class=\"comment\"># bchw=(1, 64, 55, 55) --&gt; (64, 1, 55, 55)</span></span><br><span class=\"line\">fmap_1_grid = vutils.make_grid(fmap_1, normalize=<span class=\"literal\">True</span>, scale_each=<span class=\"literal\">True</span>, nrow=<span class=\"number\">8</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">writer.add_image(<span class=\"string\">&#x27;feature map in conv1&#x27;</span>, fmap_1_grid, global_step=<span class=\"number\">322</span>)</span><br><span class=\"line\">writer.close()</span><br></pre></td></tr></table></figure>\n\n<p>部分特征图：</p>\n<img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210927153107085.png\" alt=\"image-20210927153107085\" style=\"zoom:33%;\">\n\n<h5 id=\"3-add-graph-and-torchsummary\"><a href=\"#3-add-graph-and-torchsummary\" class=\"headerlink\" title=\"3. add_graph and torchsummary\"></a>3. add_graph and torchsummary</h5><h6 id=\"add-graph\"><a href=\"#add-graph\" class=\"headerlink\" title=\"add_graph()\"></a>add_graph()</h6><p>可视化模型计算图（看起来非常的宏伟）</p>\n<p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210927153238642.png\" alt=\"image-20210927153238642\"></p>\n<ul>\n<li>model：模型，必须是nn.Module</li>\n<li>input_to_model：输出给模型的数据</li>\n<li>verbose：是否打印计算图结构信息</li>\n</ul>\n<h6 id=\"torchsummary\"><a href=\"#torchsummary\" class=\"headerlink\" title=\"torchsummary\"></a>torchsummary</h6><p>查看模型信息，便于调试</p>\n<p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210927153850524.png\" alt=\"image-20210927153850524\"></p>\n<ul>\n<li>model：pytorch模型</li>\n<li>input_size：模型输入size</li>\n<li>batch_size：batch size</li>\n<li>device：”cuda” or “cpu”</li>\n</ul>\n<p><strong>github</strong>：<a href=\"https://github.com/sksq96/pytorch-summary\">https://github.com/sksq96/pytorch-summary</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 安装 pip install torchsummary</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> torchsummary <span class=\"keyword\">import</span> summary</span><br><span class=\"line\"><span class=\"built_in\">print</span>(summary(lenet, (<span class=\"number\">3</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>), device=<span class=\"string\">&quot;gpu&quot;</span>))</span><br></pre></td></tr></table></figure>\n\n<p>打印信息如下：含有可训练参数，不可训练参数，模型所占内存。</p>\n<img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210927154217792.png\" alt=\"image-20210927154217792\" style=\"zoom: 33%;\">\n\n<h4 id=\"E-Hook函数与CAM算法\"><a href=\"#E-Hook函数与CAM算法\" class=\"headerlink\" title=\"E. Hook函数与CAM算法\"></a>E. Hook函数与CAM算法</h4><h5 id=\"1-Hook函数概念\"><a href=\"#1-Hook函数概念\" class=\"headerlink\" title=\"1. Hook函数概念\"></a>1. Hook函数概念</h5><p>Hook函数机制：不改变主体，实现额外的功能，就像hook（挂件，挂钩）</p>\n<p>在前向传播或者反向传播的主体上，通过hook获取中间的特征图，梯度。</p>\n<ol>\n<li>torch.Tensor.register_hook(hook)</li>\n<li>torch.nn.Module.register_forward_hook</li>\n<li>torch.nn.Module.register_forward_pre_hook</li>\n<li>torch.nn.Module.register_backward_hook</li>\n</ol>\n<h6 id=\"Tensor-register-hook\"><a href=\"#Tensor-register-hook\" class=\"headerlink\" title=\"Tensor.register_hook\"></a>Tensor.register_hook</h6><p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210927161602936.png\" alt=\"image-20210927161602936\"></p>\n<p>功能：注册一个<strong>反向传播</strong>hook函数</p>\n<p>Hook函数仅一个输入参数，为张量的<strong>梯度</strong>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">x = torch.tensor([<span class=\"number\">2.</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">a = torch.add(w, x)</span><br><span class=\"line\">b = torch.add(w, <span class=\"number\">1</span>)</span><br><span class=\"line\">y = torch.mul(a, b)</span><br><span class=\"line\"></span><br><span class=\"line\">a_grad = <span class=\"built_in\">list</span>()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">grad_hook</span>(<span class=\"params\">grad</span>):</span></span><br><span class=\"line\">    a_grad.append(grad)</span><br><span class=\"line\">    <span class=\"comment\"># return grad*3</span></span><br><span class=\"line\">    </span><br><span class=\"line\">handle = a.register_hook(grad_hook)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 当backward结束之后，a的梯度将会被存储在list a_grad当中。我们如果带return的话，会覆盖掉原始张量的梯度。</span></span><br></pre></td></tr></table></figure>\n\n<h6 id=\"Module-register-forward-hook\"><a href=\"#Module-register-forward-hook\" class=\"headerlink\" title=\"Module.register_forward_hook\"></a>Module.register_forward_hook</h6><p>注册module的前向传播hook函数</p>\n<p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210927234212309.png\" alt=\"image-20210927234212309\"></p>\n<ul>\n<li>module：当前网络层</li>\n<li>input：当前网络层输入数据</li>\n<li>output：当前网络层输出数据</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward_hook</span>(<span class=\"params\">module, data_input, data_output</span>):</span></span><br><span class=\"line\">    fmap_block.append(data_output) <span class=\"comment\"># feature map</span></span><br><span class=\"line\">    input_block.append(data_input) <span class=\"comment\"># 网络层的输入数据</span></span><br><span class=\"line\">    </span><br><span class=\"line\">fmap_block = <span class=\"built_in\">list</span>()</span><br><span class=\"line\">input_block = <span class=\"built_in\">list</span>()</span><br><span class=\"line\"><span class=\"comment\"># 将函数注册上去</span></span><br><span class=\"line\">net.conv1.register_forward_hook(forward_hook)</span><br></pre></td></tr></table></figure>\n\n<h6 id=\"Module-register-forward-pre-hook\"><a href=\"#Module-register-forward-pre-hook\" class=\"headerlink\" title=\"Module.register_forward_pre_hook\"></a>Module.register_forward_pre_hook</h6><p>注册module前向传播前的hook函数</p>\n<p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210928005054070.png\" alt=\"image-20210928005054070\"></p>\n<p>参数：</p>\n<ul>\n<li>module：当前网络层</li>\n<li>input：当前网络层输入数据</li>\n</ul>\n<p>前向传播<strong>前</strong>，网络层还没有对数据进行运算。</p>\n<h6 id=\"Module-register-backward-hook\"><a href=\"#Module-register-backward-hook\" class=\"headerlink\" title=\"Module.register_backward_hook\"></a>Module.register_backward_hook</h6><p>注册module反向传播的hook函数</p>\n<p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210928010019514.png\" alt=\"image-20210928010019514\"></p>\n<p>参数：</p>\n<ul>\n<li>module：当前网络层</li>\n<li>grad_input：当前网络层输入梯度数据</li>\n<li>grad_output：当前网络层输出梯度数据</li>\n</ul>\n<p>对hook函数设置好功能之后，直接挂在（register）网络上就行。</p>\n<h5 id=\"2-Hook函数与特征图提取\"><a href=\"#2-Hook函数与特征图提取\" class=\"headerlink\" title=\"2. Hook函数与特征图提取\"></a>2. Hook函数与特征图提取</h5><p>代码示例如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">flag = <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> flag:</span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        writer = SummaryWriter(comment=<span class=\"string\">&#x27;test_your_comment&#x27;</span>, filename_suffix=<span class=\"string\">&quot;_test_your_filename_suffix&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 数据</span></span><br><span class=\"line\">        path_img = <span class=\"string\">&quot;./lena.png&quot;</span>     <span class=\"comment\"># your path to image</span></span><br><span class=\"line\">        normMean = [<span class=\"number\">0.49139968</span>, <span class=\"number\">0.48215827</span>, <span class=\"number\">0.44653124</span>]</span><br><span class=\"line\">        normStd = [<span class=\"number\">0.24703233</span>, <span class=\"number\">0.24348505</span>, <span class=\"number\">0.26158768</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        norm_transform = transforms.Normalize(normMean, normStd)</span><br><span class=\"line\">        img_transforms = transforms.Compose([</span><br><span class=\"line\">            transforms.Resize((<span class=\"number\">224</span>, <span class=\"number\">224</span>)),</span><br><span class=\"line\">            transforms.ToTensor(),</span><br><span class=\"line\">            norm_transform</span><br><span class=\"line\">        ])</span><br><span class=\"line\"></span><br><span class=\"line\">        img_pil = Image.<span class=\"built_in\">open</span>(path_img).convert(<span class=\"string\">&#x27;RGB&#x27;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> img_transforms <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            img_tensor = img_transforms(img_pil)</span><br><span class=\"line\">        img_tensor.unsqueeze_(<span class=\"number\">0</span>)    <span class=\"comment\"># chw --&gt; bchw</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 模型</span></span><br><span class=\"line\">        alexnet = models.alexnet(pretrained=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 注册hook</span></span><br><span class=\"line\">        fmap_dict = <span class=\"built_in\">dict</span>()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> name, sub_module <span class=\"keyword\">in</span> alexnet.named_modules():</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(sub_module, nn.Conv2d):</span><br><span class=\"line\">                key_name = <span class=\"built_in\">str</span>(sub_module.weight.shape)</span><br><span class=\"line\">                fmap_dict.setdefault(key_name, <span class=\"built_in\">list</span>())</span><br><span class=\"line\"></span><br><span class=\"line\">                n1, n2 = name.split(<span class=\"string\">&quot;.&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hook_func</span>(<span class=\"params\">m, i, o</span>):</span></span><br><span class=\"line\">                    key_name = <span class=\"built_in\">str</span>(m.weight.shape)</span><br><span class=\"line\">                    fmap_dict[key_name].append(o)</span><br><span class=\"line\"></span><br><span class=\"line\">                alexnet._modules[n1]._modules[n2].register_forward_hook(hook_func)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># forward</span></span><br><span class=\"line\">        output = alexnet(img_tensor)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># add image</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> layer_name, fmap_list <span class=\"keyword\">in</span> fmap_dict.items():</span><br><span class=\"line\">            fmap = fmap_list[<span class=\"number\">0</span>]</span><br><span class=\"line\">            fmap.transpose_(<span class=\"number\">0</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">            nrow = <span class=\"built_in\">int</span>(np.sqrt(fmap.shape[<span class=\"number\">0</span>]))</span><br><span class=\"line\">            fmap_grid = vutils.make_grid(fmap, normalize=<span class=\"literal\">True</span>, scale_each=<span class=\"literal\">True</span>, nrow=nrow)</span><br><span class=\"line\">            writer.add_image(<span class=\"string\">&#x27;feature map in &#123;&#125;&#x27;</span>.<span class=\"built_in\">format</span>(layer_name), fmap_grid, global_step=<span class=\"number\">322</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<h5 id=\"3-CAM-class-activation-map，类激活图\"><a href=\"#3-CAM-class-activation-map，类激活图\" class=\"headerlink\" title=\"3. CAM (class activation map，类激活图)\"></a>3. CAM (class activation map，类激活图)</h5><p>CAM：类激活图，class activation map</p>\n<p>对网络的最后一个特征图进行加权求和，得到网络的注意力放在哪里。</p>\n<p>如何获取Wn的权值才是关键。必须要有GAP（global average pooling）。</p>\n<p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210928012310456.png\" alt=\"image-20210928012310456\"></p>\n<p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210928012408043.png\" alt=\"image-20210928012408043\"></p>\n<h5 id=\"4-Grad-CAM\"><a href=\"#4-Grad-CAM\" class=\"headerlink\" title=\"4. Grad-CAM\"></a>4. Grad-CAM</h5><p>Grad-CAM：CAM改进版，利用梯度作为特征图权重</p>\n<p><img src=\"/2021/09/28/PyTorch-TrainingProcess/image-20210928013456482.png\" alt=\"image-20210928013456482\"></p>\n","categories":["PyTorch框架学习"],"tags":["PyTorch"]}]